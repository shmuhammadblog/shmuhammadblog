[
  {
    "objectID": "dailypython/py0001/py0001.html",
    "href": "dailypython/py0001/py0001.html",
    "title": "Getting and Knowing your Data",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\n\nurl = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv'\n    \ndf = pd.read_csv(url, sep = '\\t')\n\nSee the first 10 entries\n\ndf.head(10)\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n0\n1\n1\nChips and Fresh Tomato Salsa\nNaN\n$2.39\n\n\n1\n1\n1\nIzze\n[Clementine]\n$3.39\n\n\n2\n1\n1\nNantucket Nectar\n[Apple]\n$3.39\n\n\n3\n1\n1\nChips and Tomatillo-Green Chili Salsa\nNaN\n$2.39\n\n\n4\n2\n2\nChicken Bowl\n[Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n$16.98\n\n\n5\n3\n1\nChicken Bowl\n[Fresh Tomato Salsa (Mild), [Rice, Cheese, Sou...\n$10.98\n\n\n6\n3\n1\nSide of Chips\nNaN\n$1.69\n\n\n7\n4\n1\nSteak Burrito\n[Tomatillo Red Chili Salsa, [Fajita Vegetables...\n$11.75\n\n\n8\n4\n1\nSteak Soft Tacos\n[Tomatillo Green Chili Salsa, [Pinto Beans, Ch...\n$9.25\n\n\n9\n5\n1\nSteak Burrito\n[Fresh Tomato Salsa, [Rice, Black Beans, Pinto...\n$9.25\n\n\n\n\n\n\n\n\nWhat is the number of observations(rows) dataset?\n\n# Solution 1\n\ncount_row = df.shape[0]  # Gives number of rows\ncount_col = df.shape[1]  # Gives number of columns\n\nprint(count_row, count_col)\n\n4622 5\n\n\n\nr, c = df.shape \n\nprint(r,c) # r = row and c = column\n\n4622 5\n\n\n\nlen() function is used to compute the length of each element in the Series/Index.\n\n\nlen(df) #   \n\n4622\n\n\n\nThe info() method prints information about the DataFrame. The information contains the number of columns, column labels, column data types, memory usage, range index, and the number of cells in each column (non-null values)\n\n\ndf.info() # inf \n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4622 entries, 0 to 4621\nData columns (total 5 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   order_id            4622 non-null   int64 \n 1   quantity            4622 non-null   int64 \n 2   item_name           4622 non-null   object\n 3   choice_description  3376 non-null   object\n 4   item_price          4622 non-null   object\ndtypes: int64(2), object(3)\nmemory usage: 180.7+ KB\n\n\n\nPrint the name of all the columns.\n\n\ndf.columns # list available columns in dataframe\n\nIndex(['order_id', 'quantity', 'item_name', 'choice_description',\n       'item_price'],\n      dtype='object')\n\n\n\n\nWhich was the most-ordered item?\n\nmost_ordered = df.groupby('item_name').sum().sort_values(['quantity'], ascending=False)\n\nmost_ordered.head()\n\n\n\n\n\n\n\n\norder_id\nquantity\n\n\nitem_name\n\n\n\n\n\n\nChicken Bowl\n713926\n761\n\n\nChicken Burrito\n497303\n591\n\n\nChips and Guacamole\n449959\n506\n\n\nSteak Burrito\n328437\n386\n\n\nCanned Soft Drink\n304753\n351\n\n\n\n\n\n\n\n\n\nFor the most-ordered item, how many items were ordered?\n\ndf.groupby('item_name').sum().sort_values(['quantity'], ascending=False).head(1)\n\n\n\n\n\n\n\n\norder_id\nquantity\n\n\nitem_name\n\n\n\n\n\n\nChicken Bowl\n713926\n761\n\n\n\n\n\n\n\n\n\nWhat was the most ordered item in the choice_description column?\n\ndf.groupby('choice_description').sum().sort_values(['quantity'], ascending=False).head(1)\n\n\n\n\n\n\n\n\norder_id\nquantity\n\n\nchoice_description\n\n\n\n\n\n\n[Diet Coke]\n123455\n159\n\n\n\n\n\n\n\n\n\nHow many items were orderd in total?\n\ndf.quantity.sum()\n\n4972\n\n\n\n\nTurn the item price into a float\n\ndf\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n0\n1\n1\nChips and Fresh Tomato Salsa\nNaN\n$2.39\n\n\n1\n1\n1\nIzze\n[Clementine]\n$3.39\n\n\n2\n1\n1\nNantucket Nectar\n[Apple]\n$3.39\n\n\n3\n1\n1\nChips and Tomatillo-Green Chili Salsa\nNaN\n$2.39\n\n\n4\n2\n2\nChicken Bowl\n[Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n$16.98\n\n\n...\n...\n...\n...\n...\n...\n\n\n4617\n1833\n1\nSteak Burrito\n[Fresh Tomato Salsa, [Rice, Black Beans, Sour ...\n$11.75\n\n\n4618\n1833\n1\nSteak Burrito\n[Fresh Tomato Salsa, [Rice, Sour Cream, Cheese...\n$11.75\n\n\n4619\n1834\n1\nChicken Salad Bowl\n[Fresh Tomato Salsa, [Fajita Vegetables, Pinto...\n$11.25\n\n\n4620\n1834\n1\nChicken Salad Bowl\n[Fresh Tomato Salsa, [Fajita Vegetables, Lettu...\n$8.75\n\n\n4621\n1834\n1\nChicken Salad Bowl\n[Fresh Tomato Salsa, [Fajita Vegetables, Pinto...\n$8.75\n\n\n\n\n4622 rows × 5 columns"
  },
  {
    "objectID": "dailypython/py0003/py0002.html",
    "href": "dailypython/py0003/py0002.html",
    "title": "Tips on Python Numbers",
    "section": "",
    "text": "Today our tips is using Numbers in Python"
  },
  {
    "objectID": "dailypython/py0003/py0002.html#formatting-numbers-using-underscore",
    "href": "dailypython/py0003/py0002.html#formatting-numbers-using-underscore",
    "title": "Tips on Python Numbers",
    "section": "Formatting Numbers using underscore",
    "text": "Formatting Numbers using underscore\nWe sometime used to have large numbers\n\na = 1000000000\nprint(a)\n\n1000000000\n\n\nSometimes, for humans it is difficult to count how many digits in the number. Using underscore solves that problem\n\nb = 1_000_000_000\nprint(b)\n\n1000000000\n\n\nHowever, if you print the number, the underscore disappears. You cam still show the numbers with underscore using the following:\n\nb = 1_000_000_000\nprint(f\"{b:_}\")\n\n1_000_000_000"
  },
  {
    "objectID": "dailypython/py0003/py0002.html#confirm-if-a-variable-is-a-number",
    "href": "dailypython/py0003/py0002.html#confirm-if-a-variable-is-a-number",
    "title": "Tips on Python Numbers",
    "section": "Confirm if a variable is a number",
    "text": "Confirm if a variable is a number\nIf you want to confirm whether a variable is a number without caring whether it is a float or an integer, numbers, use numbers.Number.\n\nfrom numbers import Number\n\na = 3\nb = 0.3\n\n# Check if a is a number\nprint(isinstance(a, Number))\nprint(isinstance(b, Number))\n\nTrue\nTrue\n\n\n\nnum = 5.45656\n\nprint(f'{num:.2f}') # Limit to 2 decimal\nprint(f'{num:.3f}') # Limit to 3 decimals\n\n5.46\n5.457"
  },
  {
    "objectID": "dailypython/py0003/py0002.html#floor-division-with-double-forward-slash",
    "href": "dailypython/py0003/py0002.html#floor-division-with-double-forward-slash",
    "title": "Tips on Python Numbers",
    "section": "Floor Division with Double Forward Slash",
    "text": "Floor Division with Double Forward Slash\nNormal division gives decimals:\n\n3/2\n\n1.5\n\n\nIf you want to do floor division using //\n\n3//2\n\n1"
  },
  {
    "objectID": "dailypython/py0003/py0002.html#displaying-floating-point-numbers",
    "href": "dailypython/py0003/py0002.html#displaying-floating-point-numbers",
    "title": "Tips on Python Numbers",
    "section": "Displaying Floating-point numbers",
    "text": "Displaying Floating-point numbers\nPython also uses E notation to display large floating-point numbers:\n\n2e4\n\n20000.0\n\n\n\n# for negative powers, we must use minus(-)\n1e-4\n\n0.0001\n\n\nWhen you reach the maximum floating-point number, Python returns a special float value, inf:\n\n2e400 # large numbers show infinite \n\ninf\n\n\n\nInf is of type float\n\n\nn = 2e400 \ntype(n) \n\nfloat"
  },
  {
    "objectID": "dailypython/py0003/py0002.html#rounding-ties-to-even.",
    "href": "dailypython/py0003/py0002.html#rounding-ties-to-even.",
    "title": "Tips on Python Numbers",
    "section": "Rounding ties to even.",
    "text": "Rounding ties to even.\nWe use round function to round numbers to the nearest integer value\n\nround(2.3)\n\n2\n\n\nWe can also pass the number of digits we want round\n\nround(3.14159, 3)\n\n3.142\n\n\nBut, why the two outputs are different:\n\nprint(round(3.5)) # round to what?\nprint(round(2.5)) # round to what?\n\n4\n2\n\n\n2.5 is rounded down to 2, and 3.5 is rounded up to 4. Most people expect a number that ends in .5 to be rounded up, so let’s take a closer look at what’s going on here.\n\nPython 3 rounds numbers according to a strategy called rounding ties to even. A tie is any number whose last digit is five. 2.5 and 3.1415 are ties, but 1.37 is not. When you round ties to even, you first look at the digit one decimal place to the left of the last digit in the tie. If that digit is even, then you round down. If the digit is odd, then you round up. That’s why 2.5 rounds down to 2 and 3.5 rounds up to 4."
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "© Copyright Shamsuddeen Muhammad\nThis is my personal website. Content on this site is provided under a Creative Commons (CC-BY) 4.0 license. You may reuse this content as long as you indicate my authorship and provide a link back to the original material. Source code of the site is provided under the MIT license and may be reused without restriction."
  },
  {
    "objectID": "til/dailyml0006/0003.html",
    "href": "til/dailyml0006/0003.html",
    "title": "A decision tree for that?",
    "section": "",
    "text": "Gianna had the opportunity to meet some of her technical heroes at a conference, and of course, she peppered them with questions!\nAt some point, the conversation centered around a machine learning solution they implemented. Gianna was delighted to hear that they used a decision tree for their solution.\nThe next day, she was still thinking about the conversation. She didn’t have much machine learning experience but knew enough about decision trees to feel validated. It was fantastic to find out that decision trees are useful!\nWhich of the following machine learning problems can you solve with a decision tree?\n\nBinary classification problems where you need to decide the correct category for a sample among two possible choices.\nMulti-class classification problems where you need to decide the correct category for a sample among multiple choices.\nMulti-label classification problems where you need to decide the correct categories for a sample among multiple choices.\nRegression problems where you need to predict a continuous output."
  },
  {
    "objectID": "til/dailyml0006/0003.html#a-decision-tree-for-that",
    "href": "til/dailyml0006/0003.html#a-decision-tree-for-that",
    "title": "A decision tree for that?",
    "section": "",
    "text": "Gianna had the opportunity to meet some of her technical heroes at a conference, and of course, she peppered them with questions!\nAt some point, the conversation centered around a machine learning solution they implemented. Gianna was delighted to hear that they used a decision tree for their solution.\nThe next day, she was still thinking about the conversation. She didn’t have much machine learning experience but knew enough about decision trees to feel validated. It was fantastic to find out that decision trees are useful!\nWhich of the following machine learning problems can you solve with a decision tree?\n\nBinary classification problems where you need to decide the correct category for a sample among two possible choices.\nMulti-class classification problems where you need to decide the correct category for a sample among multiple choices.\nMulti-label classification problems where you need to decide the correct categories for a sample among multiple choices.\nRegression problems where you need to predict a continuous output."
  },
  {
    "objectID": "til/dailyml0006/0003.html#attempt",
    "href": "til/dailyml0006/0003.html#attempt",
    "title": "A decision tree for that?",
    "section": "Attempt",
    "text": "Attempt\nEvery one of the choices is a correct answer: decision trees are really powerful!\nWe usually discuss two types of decision trees in machine learning: classification and regression trees. The former covers the first three choices, while the latter covers the fourth choice.\nBinary classification problems aim to classify one sample into two different categories. Multi-class classification problems are similar, but they classify samples into more than two categories. Decision trees are a perfect fit for these problems.\nMulti-label classification is somewhat different. Here we want to classify a sample into one or more categories. Decision trees can also solve these problems. Check out Scikit-Learn’s implementation to see how they tackle multi-label classification.\nFinally, decision trees can also solve regression problems where we want to predict a continuous target variable. “How can Regression Trees be used for Solving Regression Problems?” is an excellent introduction."
  },
  {
    "objectID": "til/dailyml0006/0003.html#recommended-reading",
    "href": "til/dailyml0006/0003.html#recommended-reading",
    "title": "A decision tree for that?",
    "section": "Recommended reading",
    "text": "Recommended reading\n\nScikit-Learn’s “Multiclass and multioutput algorithms” will introduce you to solve these problems using Scikit-Learn’s decision tree implementation.\n“How can Regression Trees be used for Solving Regression Problems?” is an excellent article about using decision trees for regression tasks."
  },
  {
    "objectID": "til/dailyml0008/0003.html",
    "href": "til/dailyml0008/0003.html",
    "title": "Salary leak",
    "section": "",
    "text": "There was a massive data leak, and for some mysterious reason, you came across a dataset full of compensation data from top tech companies in Silicon Valley.\nYou thought it wouldn’t hurt to play around with the data for a little bit. You could finally build a model to predict future compensation based on the different attributes of each employee.\nBut one thing becomes apparent from the start: You need to cut down useless features to build something useful.\nDimensionality reduction to the rescue. You haven’t done it before and want to ensure you are doing it correctly.\nHow should you apply dimensionality reduction to your data?\n\nReduce the dimensions of the training dataset. It’s unnecessary to use dimensionality reduction on the test dataset.\nReduce the dimensions of the entire dataset. Split your data into training and test right after.\nReduce the dimensions of the training dataset, then reduce the dimensions of the test dataset. We can use different dimensionality reduction techniques as long as both splits end up with the same features.\nReduce the dimensions of the training dataset, then apply the same transformations to the test dataset."
  },
  {
    "objectID": "til/dailyml0008/0003.html#salary-leak",
    "href": "til/dailyml0008/0003.html#salary-leak",
    "title": "Salary leak",
    "section": "",
    "text": "There was a massive data leak, and for some mysterious reason, you came across a dataset full of compensation data from top tech companies in Silicon Valley.\nYou thought it wouldn’t hurt to play around with the data for a little bit. You could finally build a model to predict future compensation based on the different attributes of each employee.\nBut one thing becomes apparent from the start: You need to cut down useless features to build something useful.\nDimensionality reduction to the rescue. You haven’t done it before and want to ensure you are doing it correctly.\nHow should you apply dimensionality reduction to your data?\n\nReduce the dimensions of the training dataset. It’s unnecessary to use dimensionality reduction on the test dataset.\nReduce the dimensions of the entire dataset. Split your data into training and test right after.\nReduce the dimensions of the training dataset, then reduce the dimensions of the test dataset. We can use different dimensionality reduction techniques as long as both splits end up with the same features.\nReduce the dimensions of the training dataset, then apply the same transformations to the test dataset."
  },
  {
    "objectID": "til/dailyml0008/0003.html#attempt",
    "href": "til/dailyml0008/0003.html#attempt",
    "title": "Salary leak",
    "section": "Attempt",
    "text": "Attempt\nDimensionality reduction is a technique that reduces the number of features in a dataset. It is used to reduce the complexity of a model and to reduce the training time.\nYou found there are too many features to create a useful model. The Curse of Dimensionality states that, as the dimensionality of the data increases, the amount of data needed to train a learning algorithm grows exponentially.\nThere are different techniques to reduce the dimensionality of a dataset. They all follow the same principle: you start with a dataset, reduce its dimensionality, and obtain a new dataset with fewer features. Depending on the technique, the final dataset may contain a subset of the initial features or even have entirely different columns not present in the initial dataset.\nThe first choice argues that you only need to worry about reducing the dimension of the training dataset. That’s incorrect. How can you test a model trained with a dataset containing different features?\nThe second choice argues for reducing the dimensionality of the entire dataset and splitting the data right after that. Dimensionality reduction algorithms like PCA will use information about the whole dataset to produce new features. If we apply this algorithm to all of our data—including the test data, which we aren’t supposed to know about— we’ll leak details from the test data into the training set.\nThe third choice is also incorrect. You need to apply dimensionality reduction separately to the training and test datasets and make sure you use the same transformations from the training data on the test data.\nFor example, imagine your dimensionality reduction technique creates a new feature based on the mean of another two columns. If you compute this mean separately on the train and test data, the resulting columns will come from different mean values. You need to avoid this problem by using what the fourth choice suggests: apply the same transformations and use the same information from the training and testing sets."
  },
  {
    "objectID": "til/dailyml0008/0003.html#recommended-reading",
    "href": "til/dailyml0008/0003.html#recommended-reading",
    "title": "Salary leak",
    "section": "Recommended reading",
    "text": "Recommended reading\n\nCheck “What is the Curse of Dimensionality?” for an introduction to the Curse of Dimensionality.\nFor an explanation of data leakages, check “Data Leakage in Machine Learning”.\n“Introduction to Dimensionality Reduction for Machine Learning” covers different examples of dimensionality reduction."
  },
  {
    "objectID": "til/dailyml0007/0003.html",
    "href": "til/dailyml0007/0003.html",
    "title": "Weather predictions",
    "section": "",
    "text": "For the first project, Cam wants to work with the weather dataset he found online.\nCam’s Machine Learning class was a ton of fun. Their professor let them choose a problem to solve to allow them to showcase what they’ve learned so far.\nCam wants to predict the probability of snowing based on four factors: the date, the air temperature, the location, and the air pressure.\nWhich of the following is the most appropriate algorithm that Cam should use to solve this problem?\n\nCam should use linear regression.\nCam should use logistic regression.\nCam should use K-means.\nCam should use DBSCAN."
  },
  {
    "objectID": "til/dailyml0007/0003.html#weather-predictions",
    "href": "til/dailyml0007/0003.html#weather-predictions",
    "title": "Weather predictions",
    "section": "",
    "text": "For the first project, Cam wants to work with the weather dataset he found online.\nCam’s Machine Learning class was a ton of fun. Their professor let them choose a problem to solve to allow them to showcase what they’ve learned so far.\nCam wants to predict the probability of snowing based on four factors: the date, the air temperature, the location, and the air pressure.\nWhich of the following is the most appropriate algorithm that Cam should use to solve this problem?\n\nCam should use linear regression.\nCam should use logistic regression.\nCam should use K-means.\nCam should use DBSCAN."
  },
  {
    "objectID": "til/dailyml0007/0003.html#attempt",
    "href": "til/dailyml0007/0003.html#attempt",
    "title": "Weather predictions",
    "section": "Attempt",
    "text": "Attempt\nLogistic Regression is used when the dependent variable(target) is categorical.For example,\n\nTo predict whether an email is spam (1) or (0)\nWhether the tumor is malignant (1) or not (0)\n\nConsider a scenario where we need to classify whether an email is spam or not. If we use linear regression for this problem, there is a need for setting up a threshold based on which classification can be done. Say if the actual class is malignant, predicted continuous value 0.4 and the threshold value is 0.5, the data point will be classified as not malignant which can lead to serious consequence in real time.\nFrom this example, it can be inferred that linear regression is not suitable for classification problem. Linear regression is unbounded, and this brings logistic regression into picture.\nWhen to use logistic regression?\nLogistic regression is applied to predict the categorical dependent variable. In other words, it’s used when the prediction is categorical, for example, yes or no, true or false, 0 or 1. The predicted probability or output of logistic regression can be either one of them, and there’s no middle ground.\nIn essence, logistic regression helps solve probability and classification problems. In other words, you can expect only classification and probability outcomes from logistic regression.\nWhile logistic regression predicts the categorical variable for one or more independent variables, linear regression predicts the continuous variable. In other words, logistic regression provides a constant output, whereas linear regression offers a continuous output.\nSince the outcome is continuous in linear regression, there are infinite possible values for the outcome. But for logistic regression, the number of possible outcome values is limited.\nCam’s problem has two possible outcomes: it will snow or it won’t, and Cam wants his model to return the probability of it. Logistic regression is an excellent fit for any problem with a binary outcome.\nLogistic regression estimates the probability of an event occurring based on a dataset of independent variables. Linear regression, on the other hand, predicts the continuous dependent variable using a dataset of independent variables.\nK-means and DBSCAN are clustering algorithms and are not a good approach for this problem."
  },
  {
    "objectID": "til/dailyml0007/0003.html#recommended-reading",
    "href": "til/dailyml0007/0003.html#recommended-reading",
    "title": "Weather predictions",
    "section": "Recommended reading",
    "text": "Recommended reading\n\n“Linear Regression vs Logistic Regression” does a full comparison between linear and logistic regression.\nCheck “8 Clustering Algorithms in Machine Learning that All Data Scientists Should Know” for an explanation of K-Means and DBSCAN."
  },
  {
    "objectID": "til/stringprinting/daily0001.html",
    "href": "til/stringprinting/daily0001.html",
    "title": "Formatting number with an underscore in Python",
    "section": "",
    "text": "Today I learn printing numbers with an underscore in Python.\nIn Python, you can format a large number with an underscore to make it more readable. For example, the following example shows the format of a number with an underscore:\n\ntemp = 1_000_00\nprint(temp)\n\n100000\n\n\nBut, printing a number with an underscore gives the number without the underscore. If you want to print a number with an underscore, you can use the following syntax:\n\nprint(f'{temp:_}')\nprint(f'{temp:,}')\n\n100_000\n100,000\n\n\nSee you tomorrow!"
  },
  {
    "objectID": "til/labeled/daily0001.html",
    "href": "til/labeled/daily0001.html",
    "title": "labelled vs labeled",
    "section": "",
    "text": "I often forget which one to use between labeled and labelled.\nLabeled is an American spelling while labelled is British spelling.\nI use labelled since I write in British English."
  },
  {
    "objectID": "til/labeled/daily0001.html#labelled-vs-labelled",
    "href": "til/labeled/daily0001.html#labelled-vs-labelled",
    "title": "labelled vs labeled",
    "section": "",
    "text": "I often forget which one to use between labeled and labelled.\nLabeled is an American spelling while labelled is British spelling.\nI use labelled since I write in British English."
  },
  {
    "objectID": "til/labeled/daily0001.html#focus-vs-focused",
    "href": "til/labeled/daily0001.html#focus-vs-focused",
    "title": "labelled vs labeled",
    "section": "Focus vs focused",
    "text": "Focus vs focused\nThere is a rule in the English language called the doubling up rule. Basically, if you add a vowel suffix to the end of a word, you typically want to double up the final letter to allow room for it.\nThe official requirements are that we ‘double a single consonant letter at the end of any base where the preceding vowel is spelled with a single letter and stressed’. For example: (bar, barring, barred), (beg, begging, begged), (occur, occurring, occurred).\nHowever, no doubling when the preceding vowel is unstressed. For example: panic becomes panicking, traffic becomes trafficking,\nNow is focus or focussing?\n\nThis word can take either double or single s, with the single option being highly preferred.\n\nfocusing/focussing and focused/focussed are all correct. !"
  },
  {
    "objectID": "til/Python_module/py0002.html",
    "href": "til/Python_module/py0002.html",
    "title": "Looping in Python Series",
    "section": "",
    "text": "For loops are used to iterate over a sequence (that is either a list, a tuple, a dictionary, a set, or a string).\nThis is less like the for keyword in other programming languages, and works more like an iterator method as found in other object-orientated programming languages.\nWith the for loop we can execute a set of statements, once for each item in a list, tuple, set etc.\nfor loops in Python don’t have indexes. Instead, we can use for loops to iterate over the items of any sequence, such as a list or a string.\nIn this tutorial, we will learn how to use for loops in Python with the help of examples. We will also learn the following:\n\nWhat is iterable in Python?\nHow to use for loops in Python?\nLooping with index\nLooping over multiple iterable at once"
  },
  {
    "objectID": "til/Python_module/py0002.html#manual-iteration-using-for-loop",
    "href": "til/Python_module/py0002.html#manual-iteration-using-for-loop",
    "title": "Looping in Python Series",
    "section": "Manual iteration using for loop",
    "text": "Manual iteration using for loop\n\nExample 1: Iterating over a list\nLet’s say we have a list of numbers and we want to print each number in the list. We can do this using a for loop.\n\n# Example 1: Iterating over a list\n\n# List of numbers\nnumbers = [6, 5, 3, 8, 4, 2, 5, 4, 11]\n\n# variable to store the sum\n\nsum = 0\n\n# iterate over the list\n\nfor val in numbers:\n\n    sum = sum+val\n\nprint(\"The sum is\", sum)\nOutput:\n\nThe sum is 48\n\n\nExample 2: Iterating over a string\nWe can also iterate over a string. Here is an example to count the number of ’l’s in a string.\n\n# Example 2: Iterating over a string\n\n# String\n\nname = \"Python\"\n\n# variable to store the count\n\ncount = 0\n\nfor letter in name:\n\n    if(letter == 't'):\n\n        count = count + 1\n\nprint('Count of t in Python is:', count)\nOutput:\n\nCount of t in Python is: 1\n\n\nExample 3: Iterating over a tuple (immutable)\nWe can also iterate over a tuple. Here is an example to multiply all the elements of a tuple.\n\n# Example 3: Iterating over a tuple (immutable)\n\n# Tuple of numbers\n\nnumbers = (1, 2, 3, 4)\n\n# variable to store the product\n\nresult = 1\n\nfor x in numbers:\n\n    result = result * x\n\nprint(result)\nOutput:\n\n24"
  },
  {
    "objectID": "til/Python_module/py0002.html#all-iteration-utilities-do-the-same-kind-of-looping",
    "href": "til/Python_module/py0002.html#all-iteration-utilities-do-the-same-kind-of-looping",
    "title": "Looping in Python Series",
    "section": "All iteration utilities do the same kind of looping",
    "text": "All iteration utilities do the same kind of looping\nThe for loop is the most common way to iterate over a sequence. However, we can also use the while loop to iterate over a sequence.\nThe while loop is more general, since we can write any arbitrary code in its block. The for loop is more concise and Pythonic.\n\nExample 4: Iterating over a list using while loop\n\n# Example 4: Iterating over a list using while loop\n\n# List of numbers\n\nnumbers = [6, 5, 3, 8, 4, 2, 5, 4, 11]\n\n# variable to store the sum\n\nsum = 0\n\n# iterate over the list\n\ni = 0\n\nwhile i &lt; len(numbers):\n\n    sum = sum + numbers[i]\n\n    i = i+1\n\nprint(\"The sum is\", sum)\nOutput:\n\nThe sum is 48"
  },
  {
    "objectID": "til/Python_module/py0002.html#iterating-using-constructors",
    "href": "til/Python_module/py0002.html#iterating-using-constructors",
    "title": "Looping in Python Series",
    "section": "Iterating using Constructors",
    "text": "Iterating using Constructors\nWe can also use the iter() function to get an iterator object and then use the next() function to iterate through it.\n\nExample 5: Iterating using iter() and next()\n\n# Example 5: Iterating using iter() and next()\n\n# List of numbers\n\nnumbers = [6, 5, 3, 8, 4, 2, 5, 4, 11]\n\n# variable to store the sum\n\nsum = 0\n\n# get an iterator object from that iterable\n\nit = iter(numbers)\n\n# iterate through it using next() function\n\nwhile True:\n\n    try:\n\n        # get the next item\n\n        x = next(it)\n\n        # do something with the item\n\n        sum = sum + x\n\n    except StopIteration:\n\n        # if StopIteration is raised, break from loop\n\n        break\n\nprint(\"The sum is\", sum)\nOutput:\n\nThe sum is 48"
  },
  {
    "objectID": "til/Python_module/py0002.html#looping-with-index",
    "href": "til/Python_module/py0002.html#looping-with-index",
    "title": "Looping in Python Series",
    "section": "Looping with index",
    "text": "Looping with index\nWe can also loop over a sequence using the index of each item within the body of a loop using the built-in function range() and len() functions.\n\nExample 6: Looping with index\n\n# Example 6: Looping with index\n\n# List of numbers\n\nnumbers = [6, 5, 3, 8, 4, 2, 5, 4, 11]\n\n# variable to store the sum\n\n\nsum = 0\n\n# iterate over the list using index\n\nfor i in range(len(numbers)):\n\n    sum = sum + numbers[i]\n\nprint(\"The sum is\", sum)\nOutput:\n\nThe sum is 48\n\n\nLooping over multiple iterable at once\nWe can also loop over multiple iterable at once using the zip() function.\n\n\nExample 7: Looping over multiple iterable at once\n\n# Example 7: Looping over multiple iterable at once\n\n# List of numbers\n\nnumbers = [6, 5, 3, 8, 4, 2, 5, 4, 11]\n\n# List of names\n\nnames = ['John', 'Paul', 'George', 'Ringo']\n\n# iterate over two lists simultaneously\n\nfor i, j in zip(numbers, names):\n\n    print(i, j)\nOutput:\n\n6 John\n\n5 Paul\n\n3 George\n\n8 Ringo"
  },
  {
    "objectID": "til/Python_module/py0002.html#looping-in-reverse",
    "href": "til/Python_module/py0002.html#looping-in-reverse",
    "title": "Looping in Python Series",
    "section": "Looping in reverse",
    "text": "Looping in reverse\nWe can loop over a sequence in reverse using the reversed() function.\n\nExample 8: Looping in reverse\n\n# Example 8: Looping in reverse\n\n# List of numbers\n\nnumbers = [6, 5, 3, 8, 4, 2, 5, 4, 11]\n\n# iterate in reverse\n\nfor num in reversed(numbers):\n\n    print(num)\nOutput:\n\n11\n\n4\n\n5\n\n2\n\n4\n\n8\n\n\n3\n\n5\n\n6"
  },
  {
    "objectID": "til/Python_module/py0002.html#looping-over-a-dictionary",
    "href": "til/Python_module/py0002.html#looping-over-a-dictionary",
    "title": "Looping in Python Series",
    "section": "Looping over a dictionary",
    "text": "Looping over a dictionary\nWe can loop over a dictionary using a for loop. By default, iterating over a dictionary will loop over its keys. However, we can use the values() or items() methods to access the values or both keys and values.\n\nExample 9: Looping over a dictionary\n\n# Example 9: Looping over a dictionary\n\n\n# Dictionary\n\nd = {'x': 1, 'y': 2, 'z': 3}\n\n# Iterate over dictionary keys\n\nfor key in d:\n\n    print(key, 'corresponds to', d[key])\nOutput:\n\nx corresponds to 1\n\ny corresponds to 2\n\nz corresponds to 3\n\n\nExample 10: Looping over dictionary keys and values\n\n\n# Example 10: Looping over dictionary keys and values\n\n# Dictionary\n\nd = {'x': 1, 'y': 2, 'z': 3}\n\n# Iterate over dictionary keys and values\n\nfor key, value in d.items():\n\n    print(key, 'corresponds to', d[key])\nOutput:\n\nx corresponds to 1\n\ny corresponds to 2\n\nz corresponds to 3\n\n\nExample 11: Looping over dictionary keys and values using iteritems()\n\n# Example 11: Looping over dictionary keys and values using iteritems()\n\n# Dictionary\n\nd = {'x': 1, 'y': 2, 'z': 3}\n\n# Iterate over dictionary keys and values\n\nfor key, value in d.iteritems():\n\n    print(key, 'corresponds to', d[key])\nOutput:\n\n\nx corresponds to 1\n\n\ny corresponds to 2\n\n\nz corresponds to 3"
  },
  {
    "objectID": "til/Python_module/py0002.html#looping-over-a-string",
    "href": "til/Python_module/py0002.html#looping-over-a-string",
    "title": "Looping in Python Series",
    "section": "Looping over a string",
    "text": "Looping over a string\nWe can loop over a string using a for loop. By default, iterating over a string will loop over its characters.\n\nExample 12: Looping over a string\n\n\n# Example 12: Looping over a string\n\n# String\n\n\nname = \"John\"\n\n\n# Iterate over string characters\n\n\nfor char in name:\n\n    print(char)\nOutput:\n\n\nJohn"
  },
  {
    "objectID": "til/Python_module/py0002.html#looping-over-a-file",
    "href": "til/Python_module/py0002.html#looping-over-a-file",
    "title": "Looping in Python Series",
    "section": "Looping over a file",
    "text": "Looping over a file\nWe can loop over a file using a for loop. By default, iterating over a file will loop over its lines.\n\nExample 13: Looping over a file\n\n# Example 13: Looping over a file\n\n# Open file for reading\n\nwith open('test.txt', 'r') as f:\n\n    # Loop over each line of the file\n\n    for line in f:\n\n        # process the line\n\n        print(line)\nOutput:\n\nHello World!\n\nThis is our new text file\n\nand this is another line."
  },
  {
    "objectID": "til/Python_module/py0002.html#looping-with-else",
    "href": "til/Python_module/py0002.html#looping-with-else",
    "title": "Looping in Python Series",
    "section": "Looping with else",
    "text": "Looping with else\nWe can use the else clause in a for loop to execute a block of code once when the loop has exhausted iterating the list.\n\nExample 14: Looping with else\n\n# Example 14: Looping with else\n\n# List of numbers\n\nnumbers = [6, 5, 3, 8, 4, 2, 5, 4, 11]\n\n# variable to store the sum\n\nsum = 0\n\n# iterate over the list\n\nfor num in numbers:\n\n    sum = sum + num\n\nelse:\n\n    print(\"Inside else\")\n\nprint(\"The sum is\", sum)\nOutput:\n\nInside else\n\nThe sum is 48"
  },
  {
    "objectID": "til/Python_module/py0002.html#nested-loops",
    "href": "til/Python_module/py0002.html#nested-loops",
    "title": "Looping in Python Series",
    "section": "Nested loops",
    "text": "Nested loops\nWe can use one or more for loops inside another for loop. These are called nested loops.\n\nExample 15: Nested loops\n\n\n# Example 15: Nested loops\n\n# List of numbers\n\nnumbers = [6, 5, 3, 8, 4, 2, 5, 4, 11]\n\n# variable to store the sum\n\n\nsum = 0\n\n\n# iterate over the list\n\n\nfor num in numbers:\n\n    sum = sum + num\n\n    for i in range(2):\n\n        sum = sum + i\n\nprint(\"The sum is\", sum)\nOutput:\n\nThe sum is 66"
  },
  {
    "objectID": "til/Python_module/py0002.html#loop-using",
    "href": "til/Python_module/py0002.html#loop-using",
    "title": "Looping in Python Series",
    "section": "loop using",
    "text": "loop using"
  },
  {
    "objectID": "til/Python_module/py0002.html#looping-using-constructor",
    "href": "til/Python_module/py0002.html#looping-using-constructor",
    "title": "Looping in Python Series",
    "section": "looping using constructor",
    "text": "looping using constructor\nA for loop isn’t the only way to iterate over an iterable.\nA list constructor does iteration as well. “,\n“Here’s an example of using a list constructor to iterate over a string:”,\n“```python”,\n“for char in list(‘hello’):”,\n” print(char)“,\n“```”,\n“Output:”,\n“```python”,\n“h”,\n“e”,\n“l”,\n“l”,\n“o”,\n“```”,\n“## loop using enumerate”,\n“Another way to iterate over an iterable is to use the enumerate() function. ”,\n“enumerate() returns a tuple containing a count (from start which defaults to 0) ”,\n“and the values obtained from iterating over the iterable.”,\n“Here’s an example of using enumerate() to iterate over a string:”,\n“```python”,\n“for i, char in enumerate(‘hello’):”,\n” print(i, char)“,\n“```”,\n“Output:”,\n“```python”,\n“0 h”,\n“1 e”,\n“2 l”,\n“3 l”,\n“4 o”,\n“```”,\n“## loop using zip”,\n“Another way to iterate over iterables is to use the zip() function. ”,\n“zip() returns an iterator of tuples, where the i-th tuple contains the i-th element ”,\n“from each of the argument sequences or iterables.”,\n“Here’s an example of using zip() to iterate over two lists:”,\n“```python”,\n“for i, j in zip([1, 2, 3], [4, 5, 6]):”,\n” print(i, j)“,\n“```”,\n“Output:”,\n“```python”,\n“1 4”,\n“2 5”,\n“3 6”,\n“```”,\n“## loop using reversed”,\n“Another way to iterate over an iterable is to use the reversed() function. ”,\n“reversed() returns a reverse iterator. ”,\n“Here’s an example of using reversed() to iterate over a string:”,\n“```python”,\n“for char in reversed(‘hello’):”,\n” print(char)“,\n“```”,\n“Output:”,\n“```python”,\n“o”,\n“l”,\n“l”,\n“e”,\n“h”,\n“```”,\n“## loop using sorted”,\n“Another way to iterate over an iterable is to use the sorted() function. ”,\n“sorted() returns a new sorted list from the items in iterable.”,\n“Here’s an example of using sorted() to iterate over a string:”,\n“```python”,\n“for char in sorted(‘hello’):”,\n” print(char)“,\n“```”,\n“Output:”,\n“```python”,\n“e”,\n“h”,\n“l”,\n“l”,\n“o”,\n“```”,\n“## loop using map”,\n“Another way to iterate over an iterable is to use the map() function. ”,\n“map() applies a function to all the items in an input_list.”,\n“Here’s an example of using map() to iterate over a list:”,\n“```python”,\n“def add_one(x):”,\n” return x + 1“,\n“for i in map(add_one, [1, 2, 3, 4, 5]):”,\n” print(i)“,\n“```”,\n“Output:”,\n“```python”,\n“2”,\n“3”,\n“4”,\n“5”,\n“6”,\n“```”,\n“## loop using filter”,\n“Another way to iterate over an iterable is to use the filter() function. ”,\n“filter() constructs an iterator from those elements of iterable for which function returns true.”,\n“Here’s an example of using filter() to iterate over a list:”,\n“```python”,\n“def is_even(x):”,\n” return x % 2 == 0“,\n“for i in filter(is_even, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]):”,\n” print(i)“,\n“```”,\n“Output:”,\n“```python”,\n“2”,\n“4”,\n“6”,\n“8”,\n“10”,\n“```”,\n“## loop using reduce”,\n“Another way to iterate over an iterable is to use the reduce() function. ”,\n“reduce() applies a rolling computation to sequential pairs of values in a list.”,\n“Here’s an example of using reduce() to iterate over a list:”,\n“```python”,\n“from functools import reduce”,\n“def add(x, y):”,\n” return x + y“,\n“print(reduce(add, [1, 2, 3, 4, 5]))”,\n“```”,\n“Output:”,\n“```python”,\n“15”,\n“```”,\n“## loop using itertools”,\n“Another way to iterate over an iterable is to use the itertools module. ”,\n“itertools is a standard library that contains several functions that are useful in functional programming.”,\n“Here’s an example of using itertools to iterate over a list:”,\n“```python”,\n“import itertools”,\n“for i in itertools.chain([1, 2, 3], [4, 5, 6]):”,\n” print(i)“,\n“```”,\n“Output:”,\n“```python”,\n“1”,\n“2”,\n“3”,\n“4”,\n“5”,\n“6”,\n“```”,\n“## loop using operator”,\n“Another way to iterate over an iterable is to use the operator module. ”,\n“operator is a standard library that contains the implementation of intrinsic operators in Python.”,\n“Here’s an example of using operator to iterate over a list:”,\n“```python”,\n“import operator”,\n“for i in map(operator.add, [1, 2, 3], [4, 5, 6]):”,\n” print(i)“,\n“```”,\n“Output:”,\n“```python”,\n“5”,\n“7”,\n“9”,\n“```”,\n“## loop using functools”,\n“Another way to iterate over an iterable is to use the functools module. ”,\n“functools is a standard library that contains several higher order functions and operations on callable objects.”,\n“Here’s an example of using functools to iterate over a list:”,\n“```python”,\n“import functools”,\n“for i in map(functools.partial(operator.add, 1), [1, 2, 3, 4, 5]):”,\n” print(i)“,\n“```”,\n“Output:”,\n“```python”,\n“2”,\n“3”,\n“4”,\n“5”,\n“6”,\n“```”,\n“## loop using zip”,\n“Another way to iterate over an iterable is to use the zip() function. ”,\n“zip() returns an iterator of tuples, where the i-th tuple contains the i-th element from each of the argument sequences or iterables.”,\n“Here’s an example of using zip() to iterate over two lists:”,\n“```python”,\n“for i, j in zip([1, 2, 3], [4, 5, 6]):”,\n” print(i, j)“,\n“```”,\n“Output:”,\n“```python”,\n“1 4”,\n“2 5”,\n“3 6”,\n“```”,\n“## loop using enumerate”,"
  },
  {
    "objectID": "til/Python_module/py0002.html#looping-with-index-1",
    "href": "til/Python_module/py0002.html#looping-with-index-1",
    "title": "Looping in Python Series",
    "section": "Looping with index",
    "text": "Looping with index\n“Another way to iterate over an iterable is to use the enumerate() function. ”,\n“enumerate() returns an enumerate object. It contains the index and value of all the items in the list as pairs.”,\n“Here’s an example of using enumerate() to iterate over a list:”,\n“```python”,\n“for i, j in enumerate([1, 2, 3, 4, 5]):”,\n” print(i, j)“,\n“```”,\n“Output:”,\n“```python”,\n\nDictionaries, generators and files are also iterables. There are lots of other iterables in the Python, both built-in and included in third-party libraries."
  },
  {
    "objectID": "til/latex_comments_todo/daily0001.html",
    "href": "til/latex_comments_todo/daily0001.html",
    "title": "Bias and Varience in Machine Learning Algorithms",
    "section": "",
    "text": "Machine learning algorithm prediction error can be classified into three types: irreducible error, bias error, and variable error."
  },
  {
    "objectID": "til/latex_comments_todo/daily0001.html#bias-error",
    "href": "til/latex_comments_todo/daily0001.html#bias-error",
    "title": "Bias and Varience in Machine Learning Algorithms",
    "section": "Bias error",
    "text": "Bias error\nA model bias is a prior assumption made by a model to make the target function easier to learn. For example, a linear algorithm assumes that the target function is linear. Hence, if the target function is non-linear, then the model will have a bias error. Though they are easy to learn and understand, they are not flexible and have lower predictive performance compared to complex or flexible algorithms. Therefore, less-flexible algorithms (e.g., linear regression, linear discriminant analysis, and logistic regression) have higher bias than complex or flexible algorithms (e.g., neural networks, decision trees, k-Nearest Neighbors, and support vector machines)."
  },
  {
    "objectID": "til/latex_comments_todo/daily0001.html#variance-error",
    "href": "til/latex_comments_todo/daily0001.html#variance-error",
    "title": "Bias and Varience in Machine Learning Algorithms",
    "section": "Variance error",
    "text": "Variance error\nAssume we train the same model with two different training datasets: N and M. Variance refers to the changes in the model when different training data sets are used. Certainly, the model will have a variance error when the training dataset is different. But, the error should not be too high between different training datasets.Therefore, low-variance algorithms have small changes when a different training dataset is used, and high-variance algorithms have large changes when a different training dataset is used. Flexible or complex algorithms have a high variance (e.g. SVM, Decision Trees, Neural Networks, k-Nearest Neighbors). However, non-flexible algorithms have a low variance (e.g., linear regression, linear discriminant analysis, and logistic regression).\nFinally, the goal of any supervised learning algorithm is to achieve low-bias and low-variance errors. Achieving these goals is the key to the success of any supervised learning algorithm. But, how can we achieve these goals? That is where the idea of Bias-Variance Tradeoff comes into play.\nSummary:\n\nNon-complex machine learning algorithms have high bias and low variance.\nNn-complex/flexible machine learning algorithms have a low bias but a high variance."
  },
  {
    "objectID": "til/nlp0002/nlp0002.html",
    "href": "til/nlp0002/nlp0002.html",
    "title": "Deep Learning",
    "section": "",
    "text": "Deep Lea"
  },
  {
    "objectID": "til/dailyML01/daily0001.html",
    "href": "til/dailyML01/daily0001.html",
    "title": "Bias and Varience in Machine Learning Algorithms",
    "section": "",
    "text": "Machine learning algorithm prediction error can be classified into three types: irreducible error, bias error, and variable error."
  },
  {
    "objectID": "til/dailyML01/daily0001.html#bias-error",
    "href": "til/dailyML01/daily0001.html#bias-error",
    "title": "Bias and Varience in Machine Learning Algorithms",
    "section": "Bias error",
    "text": "Bias error\nA model bias is a prior assumption made by a model to make the target function easier to learn. For example, a linear algorithm assumes that the target function is linear. Hence, if the target function is non-linear, then the model will have a bias error. Though they are easy to learn and understand, they are not flexible and have lower predictive performance compared to complex or flexible algorithms. Therefore, less-flexible algorithms (e.g., linear regression, linear discriminant analysis, and logistic regression) have higher bias than complex or flexible algorithms (e.g., neural networks, decision trees, k-Nearest Neighbors, and support vector machines)."
  },
  {
    "objectID": "til/dailyML01/daily0001.html#variance-error",
    "href": "til/dailyML01/daily0001.html#variance-error",
    "title": "Bias and Varience in Machine Learning Algorithms",
    "section": "Variance error",
    "text": "Variance error\nAssume we train the same model with two different training datasets: N and M. Variance refers to the changes in the model when different training data sets are used. Certainly, the model will have a variance error when the training dataset is different. But, the error should not be too high between different training datasets.Therefore, low-variance algorithms have small changes when a different training dataset is used, and high-variance algorithms have large changes when a different training dataset is used. Flexible or complex algorithms have a high variance (e.g. SVM, Decision Trees, Neural Networks, k-Nearest Neighbors). However, non-flexible algorithms have a low variance (e.g., linear regression, linear discriminant analysis, and logistic regression).\nFinally, the goal of any supervised learning algorithm is to achieve low-bias and low-variance errors. Achieving these goals is the key to the success of any supervised learning algorithm. But, how can we achieve these goals? That is where the idea of Bias-Variance Tradeoff comes into play.\nSummary:\n\nNon-complex machine learning algorithms have high bias and low variance.\nNn-complex/flexible machine learning algorithms have a low bias but a high variance."
  },
  {
    "objectID": "til/dailyml0003/0003.html",
    "href": "til/dailyml0003/0003.html",
    "title": "Logistic regression",
    "section": "",
    "text": "There were four different exercises, but Jade only knew how to use logistic regression.\nLuckily, Jade only needed to choose one exercise and solve it. But she needs to ensure she can find a solution using logistic regression; she doesn’t have time to learn anything else.\nSelect every exercise that Jade could solve using logistic regression?\n\nThe first exercise is a regression problem.\nThe second exercise is a binary classification problem.\nThe third exercise is a 2-class classification problem.\nThe fourth exercise is a 3-class classification problem."
  },
  {
    "objectID": "til/dailyml0003/0003.html#logistic-regression-exercises",
    "href": "til/dailyml0003/0003.html#logistic-regression-exercises",
    "title": "Logistic regression",
    "section": "",
    "text": "There were four different exercises, but Jade only knew how to use logistic regression.\nLuckily, Jade only needed to choose one exercise and solve it. But she needs to ensure she can find a solution using logistic regression; she doesn’t have time to learn anything else.\nSelect every exercise that Jade could solve using logistic regression?\n\nThe first exercise is a regression problem.\nThe second exercise is a binary classification problem.\nThe third exercise is a 2-class classification problem.\nThe fourth exercise is a 3-class classification problem."
  },
  {
    "objectID": "til/dailyml0003/0003.html#answer",
    "href": "til/dailyml0003/0003.html#answer",
    "title": "Logistic regression",
    "section": "Answer",
    "text": "Answer\nFortunately for Jade, she can tackle the second, third, and fourth exercises.\nLogistic regression estimates the probability of an event occurring by outputting a single value bounded between 0 and 1. The closer the result is to zero, the less likely the event will occur, while the closer is to 1, the more likely it is.\nThis structure makes logistic regression ideal for tackling binary or 2-class classification problems. Notice that we can formulate a binary classification task as a 2-class classification task and vice versa.\nJade can also work on the 3-class classification exercise using the One-vs-All method (also called “One-vs-Rest.”) She can train three different logistic regression models:\nModel 1: Identifying Class 1 vs. [Class 2, Class 3] Model 2: Identifying Class 2 vs. [Class 1, Class 3] Model 3: Identifying Class 3 vs. [Class 1, Class 2]\nThe final result would be the prediction from the model with the highest confidence.\nJade should stay away from the first exercise. Logistic regression will not help her solve a regression problem."
  },
  {
    "objectID": "til/dailyml0003/0003.html#recommended-reading",
    "href": "til/dailyml0003/0003.html#recommended-reading",
    "title": "Logistic regression",
    "section": "Recommended reading",
    "text": "Recommended reading\n\nCheck out “Logistic Regression for Machine Learning” for an introduction to Logistic regression.\n“Essential Data Science Tips: How to Use One-Vs-Rest and One-Vs-One for Multi-Class Classification” is a great reference to understand how to use the One-vs-All method for multi-class classification."
  },
  {
    "objectID": "til/py0002/py0002.html",
    "href": "til/py0002/py0002.html",
    "title": "TF_ID IF",
    "section": "",
    "text": "from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom visual import show_tfidf  \n\n\ndocs = [\n    \"it is a good day, I like to stay here\",\n    \"I am happy to be here\",\n    \"I am bob\",\n    \"it is sunny today\",\n    \"I have a party today\",\n    \"it is a dog and that is a cat\",\n    \"there are dog and cat on the tree\",\n    \"I study hard this morning\",\n    \"today is a good day\",\n    \"tomorrow will be a good day\",\n    \"I like coffee, I like book and I like apple\",\n    \"I do not like it\",\n    \"I am kitty, I like bob\",\n    \"I do not care who like bob, but I like kitty\",\n    \"It is coffee time, bring your cup\",\n]\n\nvectorizer = TfidfVectorizer()\ntf_idf = vectorizer.fit_transform(docs)\nprint(\"idf: \", [(n, idf) for idf, n in zip(vectorizer.idf_, vectorizer.get_feature_names())])\nprint(\"v2i: \", vectorizer.vocabulary_)\n\n\nq = \"I get a coffee cup\"\nqtf_idf = vectorizer.transform([q])\nres = cosine_similarity(tf_idf, qtf_idf)\nres = res.ravel().argsort()[-3:]\nprint(\"\\ntop 3 docs for '{}':\\n{}\".format(q, [docs[i] for i in res[::-1]]))\n\n\ni2v = {i: v for v, i in vectorizer.vocabulary_.items()}\ndense_tfidf = tf_idf.todense()\nshow_tfidf(dense_tfidf, [i2v[i] for i in range(dense_tfidf.shape[1])], \"tfidf_sklearn_matrix\")\n\nidf:  [('am', 2.386294361119891), ('and', 2.386294361119891), ('apple', 3.0794415416798357), ('are', 3.0794415416798357), ('be', 2.6739764335716716), ('bob', 2.386294361119891), ('book', 3.0794415416798357), ('bring', 3.0794415416798357), ('but', 3.0794415416798357), ('care', 3.0794415416798357), ('cat', 2.6739764335716716), ('coffee', 2.6739764335716716), ('cup', 3.0794415416798357), ('day', 2.386294361119891), ('do', 2.6739764335716716), ('dog', 2.6739764335716716), ('good', 2.386294361119891), ('happy', 3.0794415416798357), ('hard', 3.0794415416798357), ('have', 3.0794415416798357), ('here', 2.6739764335716716), ('is', 1.9808292530117262), ('it', 1.9808292530117262), ('kitty', 2.6739764335716716), ('like', 1.9808292530117262), ('morning', 3.0794415416798357), ('not', 2.6739764335716716), ('on', 3.0794415416798357), ('party', 3.0794415416798357), ('stay', 3.0794415416798357), ('study', 3.0794415416798357), ('sunny', 3.0794415416798357), ('that', 3.0794415416798357), ('the', 3.0794415416798357), ('there', 3.0794415416798357), ('this', 3.0794415416798357), ('time', 3.0794415416798357), ('to', 2.6739764335716716), ('today', 2.386294361119891), ('tomorrow', 3.0794415416798357), ('tree', 3.0794415416798357), ('who', 3.0794415416798357), ('will', 3.0794415416798357), ('your', 3.0794415416798357)]\nv2i:  {'it': 22, 'is': 21, 'good': 16, 'day': 13, 'like': 24, 'to': 37, 'stay': 29, 'here': 20, 'am': 0, 'happy': 17, 'be': 4, 'bob': 5, 'sunny': 31, 'today': 38, 'have': 19, 'party': 28, 'dog': 15, 'and': 1, 'that': 32, 'cat': 10, 'there': 34, 'are': 3, 'on': 27, 'the': 33, 'tree': 40, 'study': 30, 'hard': 18, 'this': 35, 'morning': 25, 'tomorrow': 39, 'will': 42, 'coffee': 11, 'book': 6, 'apple': 2, 'do': 14, 'not': 26, 'kitty': 23, 'care': 9, 'who': 41, 'but': 8, 'time': 36, 'bring': 7, 'your': 43, 'cup': 12}\n\ntop 3 docs for 'I get a coffee cup':\n['It is coffee time, bring your cup', 'I like coffee, I like book and I like apple', 'I do not care who like bob, but I like kitty']\n\n\n\n\n\nSee you tomorrow !"
  },
  {
    "objectID": "til/py0003/py0002.html",
    "href": "til/py0003/py0002.html",
    "title": "Skimpy: A light weight tool for creating summary statistics from dataframes",
    "section": "",
    "text": "from skimpy import skim\nfrom skimpy import skim, generate_test_data\n\ndf = generate_test_data()\n\ndf.head()\n\n\n\n\n\n\n\n\nlength\nwidth\ndepth\nrnd\nclass\nlocation\nbooly_col\ntext\ndate\ndate_no_freq\n\n\n\n\n0\n0.762796\n1.468082\n9\n-0.423534\nvirtginica\nUK\nFalse\nWhat weather!\n2018-01-31\nNaT\n\n\n1\n0.031203\n0.267769\n10\n2.102890\nvirtginica\nUK\nFalse\nHow are you?\n2018-02-28\n1992-01-05\n\n\n2\n0.044075\n3.571043\n12\n0.147606\nsetosa\nUK\nTrue\nHow are you?\n2018-03-31\n2022-01-01\n\n\n3\n0.914088\n2.838664\n15\n-0.997567\nvirtginica\nNaN\nTrue\n&lt;NA&gt;\n2018-04-30\nNaT\n\n\n4\n0.555878\n2.214629\n5\n0.329828\nsetosa\nUK\nFalse\nHow are you?\n2018-05-31\n2022-01-01\ndf.columns\n\nIndex(['length', 'width', 'depth', 'rnd', 'class', 'location', 'booly_col',\n       'text', 'date', 'date_no_freq'],\n      dtype='object')\ndf.shape \n\n(1000, 10)\nGenerate summary statistics with skim\nskim(df) \n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types               Categories                                        │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓ ┏━━━━━━━━━━━━━━━━━━━━━━━┓                                │\n│ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃ ┃ Categorical Variables ┃                                │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩ ┡━━━━━━━━━━━━━━━━━━━━━━━┩                                │\n│ │ Number of rows    │ 1000   │ │ float64     │ 3     │ │ class                 │                                │\n│ │ Number of columns │ 10     │ │ category    │ 2     │ │ location              │                                │\n│ └───────────────────┴────────┘ │ datetime64  │ 2     │ └───────────────────────┘                                │\n│                                │ int64       │ 1     │                                                          │\n│                                │ bool        │ 1     │                                                          │\n│                                │ string      │ 1     │                                                          │\n│                                └─────────────┴───────┘                                                          │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┓  │\n│ ┃ column_name      ┃ NA    ┃ NA %    ┃ mean     ┃ sd     ┃ p0         ┃ p25      ┃ p75    ┃ p100   ┃ hist    ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━━┩  │\n│ │ length           │     0 │       0 │      0.5 │   0.36 │    1.6e-06 │     0.13 │   0.86 │      1 │ █▃▃▃▄█  │  │\n│ │ width            │     0 │       0 │        2 │    1.9 │     0.0021 │      0.6 │      3 │     14 │   █▃▁   │  │\n│ │ depth            │     0 │       0 │       10 │    3.2 │          2 │        8 │     12 │     20 │ ▁▄█▆▃▁  │  │\n│ │ rnd              │   120 │      12 │    -0.02 │      1 │       -2.8 │    -0.74 │   0.66 │    3.7 │  ▁▄█▅▁  │  │\n│ └──────────────────┴───────┴─────────┴──────────┴────────┴────────────┴──────────┴────────┴────────┴─────────┘  │\n│                                                    category                                                     │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓  │\n│ ┃ column_name                      ┃ NA        ┃ NA %           ┃ ordered               ┃ unique             ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩  │\n│ │ class                            │         0 │              0 │ False                 │                  2 │  │\n│ │ location                         │         1 │            0.1 │ False                 │                  5 │  │\n│ └──────────────────────────────────┴───────────┴────────────────┴───────────────────────┴────────────────────┘  │\n│                                                    datetime                                                     │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓  │\n│ ┃ column_name             ┃ NA    ┃ NA %      ┃ first               ┃ last                ┃ frequency        ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩  │\n│ │ date                    │     0 │         0 │     2018-01-31      │     2101-04-30      │ M                │  │\n│ │ date_no_freq            │     3 │       0.3 │     1992-01-05      │     2023-03-04      │ None             │  │\n│ └─────────────────────────┴───────┴───────────┴─────────────────────┴─────────────────────┴──────────────────┘  │\n│                                                     string                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓  │\n│ ┃ column_name               ┃ NA      ┃ NA %       ┃ words per row                ┃ total words              ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩  │\n│ │ text                      │       6 │        0.6 │                          5.8 │                     5800 │  │\n│ └───────────────────────────┴─────────┴────────────┴──────────────────────────────┴──────────────────────────┘  │\n│                                                      bool                                                       │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓  │\n│ ┃ column_name                        ┃ true            ┃ true rate                     ┃ hist                ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩  │\n│ │ booly_col                          │             520 │                          0.52 │       █    █        │  │\n│ └────────────────────────────────────┴─────────────────┴───────────────────────────────┴─────────────────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯"
  },
  {
    "objectID": "til/py0003/py0002.html#clean_name-function",
    "href": "til/py0003/py0002.html#clean_name-function",
    "title": "Skimpy: A light weight tool for creating summary statistics from dataframes",
    "section": "Clean_name function",
    "text": "Clean_name function\nskimpy also comes with a clean_columns function as a convenience. This slugifies column names. For example,\n\nimport pandas as pd\nfrom rich import print\nfrom skimpy import clean_columns\n\ncolumns = [\n    \"bs lncs;n edbn \",\n    \"Nín hǎo. Wǒ shì zhōng guó rén\",\n    \"___This is a test___\",\n    \"ÜBER Über German Umlaut\",\n]\nmessy_df = pd.DataFrame(columns=columns, index=[0], data=[range(len(columns))])\nprint(\"Column names:\")\nprint(list(messy_df.columns))\n\nColumn names:\n\n\n\n['bs lncs;n edbn ', 'Nín hǎo. Wǒ shì zhōng guó rén', '___This is a test___', 'ÜBER Über German Umlaut']\n\n\n\n\nmessy_df\n\n\n\n\n\n\n\n\nbs lncs;n edbn\nNín hǎo. Wǒ shì zhōng guó rén\n___This is a test___\nÜBER Über German Umlaut\n\n\n\n\n0\n0\n1\n2\n3\n\n\n\n\n\n\n\n\nclean_df = clean_columns(messy_df)\nprint(list(clean_df.columns))\n\n4 column names have been cleaned\n\n\n\n['bs_lncs_n_edbn', 'nin_hao_wo_shi_zhong_guo_ren', 'this_is_a_test', 'uber_uber_german_umlaut']"
  },
  {
    "objectID": "dailyR/exercises_chap1/daily0001.html",
    "href": "dailyR/exercises_chap1/daily0001.html",
    "title": "R4DS Chapter 2 Exercises",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\nRun ggplot(data = mpg). What do you see?\n\n\nggplot(data = mpg)\n\n\n\n\n\nThis creates the background of the plot, but since no layers were specified with geom function, nothing is drawn.\n\nI see empty.\n\nHow many rows are in mpg? How many columns?\n\n\nnrow(mpg)\n\n[1] 234\n\n\nThere are 234 rows.\nWe can also use glimpes() to find number of rows:\n\nglimpse(mpg)\n\nRows: 234\nColumns: 11\n$ manufacturer &lt;chr&gt; \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"…\n$ model        &lt;chr&gt; \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 quattro\", \"…\n$ displ        &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0, 2.…\n$ year         &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, 200…\n$ cyl          &lt;int&gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8, …\n$ trans        &lt;chr&gt; \"auto(l5)\", \"manual(m5)\", \"manual(m6)\", \"auto(av)\", \"auto…\n$ drv          &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"4\", \"4\", \"4\", \"4\", \"4…\n$ cty          &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 1…\n$ hwy          &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25, 2…\n$ fl           &lt;chr&gt; \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p…\n$ class        &lt;chr&gt; \"compact\", \"compact\", \"compact\", \"compact\", \"compact\", \"c…\n\n\n\nWhat does the drv variable describe? Read the help for ?mpg to find out.\n\n\n?mpg\n\n\ndv means: “the type of drive train, where f = front-wheel drive, r = rear wheel drive, 4 = 4wd”\n\n\nMake a scatterplot of hwy vs cyl.\n\n\nggplot(data = mpg)+\ngeom_point(aes(hwy,cyl))\n\n\n\n\n\nWhat happens if you make a scatterplot of class vs drv? Why is the plot not useful?\n\n\nggplot(data = mpg)+\ngeom_point(aes(class,drv))\n\n\n\n\n\nA scatter plot is not a useful display of these variables since both drv and class are categorical variables. A scatterplots work best for plotting a continuous x and a continuous y variable, and when all (x, y) values are unique.\n\nHowever, there are other methods for plotting categorical variables: geom_count() and geom_tile()\n\nggplot(mpg, aes(x = class, y = drv)) +\n  geom_count()\n\n\n\n\n\nmpg %&gt;%\n  count(class, drv) %&gt;%\n  ggplot(aes(x = class, y = drv)) +\n    geom_tile(mapping = aes(fill = n))\n\n\n\n\n\nIn the previous plot, there are many missing tiles. These missing tiles represent unobserved combinations of class and drv values. These missing values are not unknown, but represent values of (class, drv) where n = 0. The complete() function in the tidyr package adds new rows to a data frame for missing combinations of columns.\n\n\nmpg %&gt;%\n  count(class, drv) %&gt;%\n  complete(class, drv, fill = list(n = 0)) %&gt;%\n  ggplot(aes(x = class, y = drv)) +\n    geom_tile(mapping = aes(fill = n))"
  },
  {
    "objectID": "dailyR/exercises_chap1/daily0001.html#exercises",
    "href": "dailyR/exercises_chap1/daily0001.html#exercises",
    "title": "R4DS Chapter 2 Exercises",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\nRun ggplot(data = mpg). What do you see?\n\n\nggplot(data = mpg)\n\n\n\n\n\nThis creates the background of the plot, but since no layers were specified with geom function, nothing is drawn.\n\nI see empty.\n\nHow many rows are in mpg? How many columns?\n\n\nnrow(mpg)\n\n[1] 234\n\n\nThere are 234 rows.\nWe can also use glimpes() to find number of rows:\n\nglimpse(mpg)\n\nRows: 234\nColumns: 11\n$ manufacturer &lt;chr&gt; \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"…\n$ model        &lt;chr&gt; \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 quattro\", \"…\n$ displ        &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0, 2.…\n$ year         &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, 200…\n$ cyl          &lt;int&gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8, …\n$ trans        &lt;chr&gt; \"auto(l5)\", \"manual(m5)\", \"manual(m6)\", \"auto(av)\", \"auto…\n$ drv          &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"4\", \"4\", \"4\", \"4\", \"4…\n$ cty          &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 1…\n$ hwy          &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25, 2…\n$ fl           &lt;chr&gt; \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p…\n$ class        &lt;chr&gt; \"compact\", \"compact\", \"compact\", \"compact\", \"compact\", \"c…\n\n\n\nWhat does the drv variable describe? Read the help for ?mpg to find out.\n\n\n?mpg\n\n\ndv means: “the type of drive train, where f = front-wheel drive, r = rear wheel drive, 4 = 4wd”\n\n\nMake a scatterplot of hwy vs cyl.\n\n\nggplot(data = mpg)+\ngeom_point(aes(hwy,cyl))\n\n\n\n\n\nWhat happens if you make a scatterplot of class vs drv? Why is the plot not useful?\n\n\nggplot(data = mpg)+\ngeom_point(aes(class,drv))\n\n\n\n\n\nA scatter plot is not a useful display of these variables since both drv and class are categorical variables. A scatterplots work best for plotting a continuous x and a continuous y variable, and when all (x, y) values are unique.\n\nHowever, there are other methods for plotting categorical variables: geom_count() and geom_tile()\n\nggplot(mpg, aes(x = class, y = drv)) +\n  geom_count()\n\n\n\n\n\nmpg %&gt;%\n  count(class, drv) %&gt;%\n  ggplot(aes(x = class, y = drv)) +\n    geom_tile(mapping = aes(fill = n))\n\n\n\n\n\nIn the previous plot, there are many missing tiles. These missing tiles represent unobserved combinations of class and drv values. These missing values are not unknown, but represent values of (class, drv) where n = 0. The complete() function in the tidyr package adds new rows to a data frame for missing combinations of columns.\n\n\nmpg %&gt;%\n  count(class, drv) %&gt;%\n  complete(class, drv, fill = list(n = 0)) %&gt;%\n  ggplot(aes(x = class, y = drv)) +\n    geom_tile(mapping = aes(fill = n))"
  },
  {
    "objectID": "dailyR/exercises_chap1/daily0001.html#exercises-1",
    "href": "dailyR/exercises_chap1/daily0001.html#exercises-1",
    "title": "R4DS Chapter 2 Exercises",
    "section": "2.3.1 Exercises",
    "text": "2.3.1 Exercises\n\nWhat’s gone wrong with this code? Why are the points not blue?\n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, color = \"blue\"))\n\n\n\n\n\nThe argument colour = “blue” is included within the mapping argument, and as such, it is treated as an aesthetic, which is a mapping between a variable and a value\n\nBelow is the correct format:\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy), color = \"blue\")\n\n\n\n\nand it is blow."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a PhD student at the University of Porto, Portugal. I am interested in low-resource natural language processing (NLP). I write stuff related to NLP and data science. You can reach out to me via shamsuddeen2004@gmail.com."
  },
  {
    "objectID": "project copy/package/wizehiver/index.html",
    "href": "project copy/package/wizehiver/index.html",
    "title": "wizehiver",
    "section": "",
    "text": "Wizehiver Hex"
  },
  {
    "objectID": "bookreading/Deep Work/daily0001.html",
    "href": "bookreading/Deep Work/daily0001.html",
    "title": "Deep Work",
    "section": "",
    "text": "I am currently reading Deep Work book by Cal NewPort"
  },
  {
    "objectID": "bookreading/Deep Work/daily0001.html#deep-work",
    "href": "bookreading/Deep Work/daily0001.html#deep-work",
    "title": "Deep Work",
    "section": "",
    "text": "I am currently reading Deep Work book by Cal NewPort"
  },
  {
    "objectID": "blog/python_pathlib/pathlib.html",
    "href": "blog/python_pathlib/pathlib.html",
    "title": "PathLib: Yet Another Python file and directory managament",
    "section": "",
    "text": "Why You Should Start Using Pathlib as an Alternative to the OS Module\nPathlib is a module in the Python standard library that provides classes for working with file and directory paths.\nIt is designed to be more intuitive and easier to use than the traditional os.path module, which is based on string manipulation and can be error-prone.\nThe Path class in pathlib represents a file or directory path and offers a convenient way to perform various operations on paths, such as reading and writing files, traversing directories, and checking the existence of files and directories.\nBelow is an example of using pathlib to read a file and print its contents:\n\n\nfrom pathlib import Path\n\n# Open the file in read-only mode\npath_name  =  'file.txt' # i am in the same directory as the file\nfile_path = Path(path_name)\nwith file_path.open(mode='r') as f:\n    # Read the contents of the file\n    contents = f.read()\n\n# Print the contents of the file\nprint(contents)\n\nhey i love this car\nhey i love this bike\nhey i love this bicycle"
  },
  {
    "objectID": "blog/python_pathlib/pathlib.html#pathlib-a-path-management-library-called-pathlib.",
    "href": "blog/python_pathlib/pathlib.html#pathlib-a-path-management-library-called-pathlib.",
    "title": "PathLib: Yet Another Python file and directory managament",
    "section": "",
    "text": "Why You Should Start Using Pathlib as an Alternative to the OS Module\nPathlib is a module in the Python standard library that provides classes for working with file and directory paths.\nIt is designed to be more intuitive and easier to use than the traditional os.path module, which is based on string manipulation and can be error-prone.\nThe Path class in pathlib represents a file or directory path and offers a convenient way to perform various operations on paths, such as reading and writing files, traversing directories, and checking the existence of files and directories.\nBelow is an example of using pathlib to read a file and print its contents:\n\n\nfrom pathlib import Path\n\n# Open the file in read-only mode\npath_name  =  'file.txt' # i am in the same directory as the file\nfile_path = Path(path_name)\nwith file_path.open(mode='r') as f:\n    # Read the contents of the file\n    contents = f.read()\n\n# Print the contents of the file\nprint(contents)\n\nhey i love this car\nhey i love this bike\nhey i love this bicycle"
  },
  {
    "objectID": "blog/python_pathlib/pathlib.html#join-path-with-pathlib",
    "href": "blog/python_pathlib/pathlib.html#join-path-with-pathlib",
    "title": "PathLib: Yet Another Python file and directory managament",
    "section": "Join path with pathlib",
    "text": "Join path with pathlib\n\nThe joinpath() method is used to join one or more path components to the end of the path.\nThe joinpath() method returns a new path object that is the concatenation of the path and the path components passed as arguments.`\nThe joinpath() method is similar to the os.path.join() function.\nAn example of using the joinpath() method to join two path components to the end of the path:\n\n\nfrom pathlib import Path\n\npath = Path('/home')\n\npath = path.joinpath('user', 'file.txt')\n\nprint(path)\n\n```\n\n- The output of the above program is:\n\n```python\n\n/home/user/file.txt\n\n```\n\n- The `joinpath()` method can also be used to join a string to the end of the path:\n\n```python \n\nfrom pathlib import Path\n\npath = Path('/home')\n\npath = path.joinpath('user/file.txt')\n\nprint(path)\n\n```\n\nPathlib is part of the standard Python library and has been introduced since Python 3.4 (see PEP 428) with the goal of representing paths not as simple strings but as supercharged Python objects with many useful methods and attributes under the hood."
  },
  {
    "objectID": "blog/movingfilestodocker/movinfiles.html",
    "href": "blog/movingfilestodocker/movinfiles.html",
    "title": "Copying files to and from a docker containers",
    "section": "",
    "text": "You can use the docker cp command to copy files from the host to the container or vice versa.\n\n\nThe syntax for copying file from host to container is\ndocker cp [source] [container:]destination`.\nThe example below show how to copy a file (dataset_on_host.csv) from the host to a container folder called data. We saved the file with a new name (dataset_on_container.csv) in the container folder.\n\n\n\n\nThe syntax for copying file from container to host is:\ndocker cp [container:]source [destination]\n    \nThe example below also show how a file from a container is copied to the host.\n\nThank you for reading."
  },
  {
    "objectID": "blog/movingfilestodocker/movinfiles.html#copying-files-to-and-from-docker-container-to-a-host",
    "href": "blog/movingfilestodocker/movinfiles.html#copying-files-to-and-from-docker-container-to-a-host",
    "title": "Copying files to and from a docker containers",
    "section": "",
    "text": "You can use the docker cp command to copy files from the host to the container or vice versa.\n\n\nThe syntax for copying file from host to container is\ndocker cp [source] [container:]destination`.\nThe example below show how to copy a file (dataset_on_host.csv) from the host to a container folder called data. We saved the file with a new name (dataset_on_container.csv) in the container folder.\n\n\n\n\nThe syntax for copying file from container to host is:\ndocker cp [container:]source [destination]\n    \nThe example below also show how a file from a container is copied to the host.\n\nThank you for reading."
  },
  {
    "objectID": "blog/obtaining data/obtaininda.html",
    "href": "blog/obtaining data/obtaininda.html",
    "title": "Using command line tool to obtain data",
    "section": "",
    "text": "Data can be obtained in several ways—for example by downloading it from a server, querying a database, or connecting to a Web API. Sometimes, the data comes in a compressed form or in a binary format such as a Microsoft Excel Spreadsheet. Tools such as curl, tar, wget, csvtool and others can be used to download data. —Data Science at command Line"
  },
  {
    "objectID": "blog/obtaining data/obtaininda.html#downloading-from-the-internet-using-curl",
    "href": "blog/obtaining data/obtaininda.html#downloading-from-the-internet-using-curl",
    "title": "Using command line tool to obtain data",
    "section": "Downloading from the Internet using Curl",
    "text": "Downloading from the Internet using Curl\nCurl and wget are command line tools that allows you to download files from the Internet. curl and wget are installed by default on most Linux systems. if you do not have either tool installed, you can use the commands below to install curl and wget on Ubuntu.\n   sudo apt install curl\n   sudo apt install wget\nIf you are using a Mac, you can install curl and wget using the following commands:\n    brew install curl\n    brew install wget\n\nHow to Use Curl\nThe curl syntax is :\ncurl [options] [URL...] \nIn its simpform, curl is used to download a file from the Internet without any options.\n    curl example.com\nThe command above will download and print the source code of the example.com homepage in your terminal window.\n\n\nSave the Output to a File\nRather than displaying to the standard output, we can use the -o option to save the output to a file.\nThe above example will save the output to a file named example.html instead of printing it to the terminal.\ncurl -o example.html example.com # \nOR\ncurl http://example.com --output filename\nOR\n\ncurl example.com -o example.html\nYou can also redirect the output to a file using the &gt; operator.\n    curl -o example.com &gt; example.html\nSometimes, it’s a good idea to set the verbose mode on using -V option. This may provide useful information about the progress of the download.\n\ncurl -v example.com\ncurl outputs a progress meter that shows the download rate and the expected time of completion. You can also use the -s option to hide the progress meter and the progress.\ncurl -s example.com\n\n\nFollowing Redirects\nWhen accessing shortened URL with curl, such as: http://bit.ly/, we need to set the -L option to automatically follow redirects\n    curl -L http://bit.ly/\n\n\nUsing Curl to Download from FTP server\nCurl has over 20 built-in FTP commands. You can use curl to download files from FTP servers as follows:\ncurl -s \"ftp://ftp.gnu.org/welcome.msg\" | trim\n\n\nHow to use Wget Command\nThe wget command is similar to curl, but it does not print the output to the terminal. Instead, it saves the output to a file. Its syntax is shown below:\nwget [options] [URL...]\nYou can download a file from the Internet using wget and saved to a file with same name.\nwget example.com\nYou can also save the output to a diffrent file name using the -O option.\nwget -O example.html example.com\nYou can save in a specify folder using the -P option.\n    wget -P /home/example.com example.com"
  },
  {
    "objectID": "blog/obtaining data/obtaininda.html#difference-between-wget-and-curl",
    "href": "blog/obtaining data/obtaininda.html#difference-between-wget-and-curl",
    "title": "Using command line tool to obtain data",
    "section": "Difference between Wget and Curl",
    "text": "Difference between Wget and Curl\nWget is similar to Curl, but curl is more powerful. wget is a tool to download files from servers while curl is a tool that let’s you exchange requests/responses with a server.\nwget’s major advantage is its ability to download recursively. Wget is command line only. There’s no library.\nCurl is powered by libcurl - a cross-platform library with a stable API that can be used by each and everyone.\nCurl is generally preferred since it supports more features than wget. For comparison here is the list of features of available with different tools:"
  },
  {
    "objectID": "blog/obtaining data/obtaininda.html#other-cool-stuff-you-can-do-with-curl",
    "href": "blog/obtaining data/obtaininda.html#other-cool-stuff-you-can-do-with-curl",
    "title": "Using command line tool to obtain data",
    "section": "Other cool stuff you can do with curl",
    "text": "Other cool stuff you can do with curl\n\n\n\n\n\n\nGet weather information with : wttr.in\n\n\n\nYou can use curl to check weather info from your terminal\n\n\n!wttr.in/SanFrancisco # to check weather in San Francisco\nrate.sx: Crypto prices without leaving the terminal by running curl rate.sx\n\ncurl rate.sx\ndict.org: To look up definitions for a word, run curl ‘dict.org/d:word’\ncurl 'dict.org/d:word'"
  },
  {
    "objectID": "blog/obtaining data/obtaininda.html#compressing-and-decompressing-files",
    "href": "blog/obtaining data/obtaininda.html#compressing-and-decompressing-files",
    "title": "Using command line tool to obtain data",
    "section": "Compressing and Decompressing Files",
    "text": "Compressing and Decompressing Files\nLarge files can be compressed and decompressed.\n\nCompressing Files using tar\n    tar -cvf example.tar ~/desktop/example/\nThe options used in the above command to create a tar file are:\n\nc – Creates a new .tar archive file.\nv – Verbosely show the .tar file progress.\nf – File name type of the archive file.\n\n\n\nCompressing Files using gzip\ntar cvzf example.tar.gz /desktop/examples/\n\nor \n\ntar cvzf example.tgz /desktop/examples/"
  },
  {
    "objectID": "blog/obtaining data/obtaininda.html#list-content-of-an-archive-file",
    "href": "blog/obtaining data/obtaininda.html#list-content-of-an-archive-file",
    "title": "Using command line tool to obtain data",
    "section": "List Content of an Archive File",
    "text": "List Content of an Archive File\nTo list the contents of an archive file, you can use the following command (with t option):\n\ntar -tvf example.tar"
  },
  {
    "objectID": "blog/obtaining data/obtaininda.html#decompressing-archivedd-file",
    "href": "blog/obtaining data/obtaininda.html#decompressing-archivedd-file",
    "title": "Using command line tool to obtain data",
    "section": "Decompressing Archivedd File",
    "text": "Decompressing Archivedd File\nTo uncompress an archive file, you can use the following command (with x option):\n\ntar -xvf example.tar \nIf you want to untar in a different directory then use option as -C (specified directory).\n    tar -xvf example.tar -C /desktop/example/\nFinally, you can use unpack command also to decompress the file.\nunpack example.tar"
  },
  {
    "objectID": "blog/obtaining data/obtaininda.html#csvkit-converting-and-working-with-csv-at-command-line",
    "href": "blog/obtaining data/obtaininda.html#csvkit-converting-and-working-with-csv-at-command-line",
    "title": "Using command line tool to obtain data",
    "section": "Csvkit : converting and working with CSV at command Line",
    "text": "Csvkit : converting and working with CSV at command Line\ncsvkit is a suite of command-line tools for converting to and working with CSV, the king of tabular file formats. working with csvkit makes it easy to convert between different formats, and to work with the data in those formats.\nYou can install csvkit using the following command:\n    pip install csvkit\nWe can print the contents of the file using the following command:\nbat -A data.csv\nThe -A option show all non-printable characters such as space, tab, newline, etc.\nWe can also use another command called csvlook to print the contents of the file nicely.\ncsvlook data.csv\nBelow are some of the commands that you can use with csvkit:\n\nConvert Excel to CSV:\n\nin2csv data.xls &gt; data.csv  # Excel to csv\n\nin2csv data.json &gt; data.csv # json to csv\n\ncsvstat data.csv # give the statistics of the csv file\n\ncsvjson data.csv &gt; data.json # convert csv to json\n\ncsvcut  -n data.csv # print column names\n\ncsvcut -c column_a,column_c data.csv &gt; new.csv # select subset of columns"
  },
  {
    "objectID": "dailypython.html",
    "href": "dailypython.html",
    "title": "Daily Python Practice",
    "section": "",
    "text": "Title\n\n\nDate\n\n\n\n\n\n\n\n\n\nundefined\n\n\nundefined\n\n\n\n\n\n\n\nTips on Python Numbers\n\n\nOct 18, 2022\n\n\n\n\n\n\n\nPython Strings Tips\n\n\nOct 16, 2022\n\n\n\n\n\n\n\nGetting and Knowing your Data\n\n\nOct 15, 2022\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "PathLib: Yet Another Python file and directory managament\n\n\n\n\n\n\n\npython\n\n\n\n\nIt discussed how deep learning changes our approach to machine learning and why PyTorch is a good fit for deep learning\n\n\n\n\n\n\nDec 18, 2022\n\n\n\n\n\n\n  \n\n\n\n\nPytorch Tensor 101\n\n\n\n\n\n\n\nPytorch\n\n\npython\n\n\n\n\nA No-Nonsense Guides to Pytorch Tensor\n\n\n\n\n\n\nAug 1, 2022\n\n\n\n\n\n\n  \n\n\n\n\nUsing command line tool to obtain data\n\n\n\n\n\n\n\ndata science\n\n\nunix\n\n\n\n\nThis blog explain how to use command line tools to obtain data from the internet.\n\n\n\n\n\n\nJun 1, 2022\n\n\n\n\n\n\n  \n\n\n\n\nCopying files to and from a docker containers\n\n\n\n\n\n\n\ndata science\n\n\nunix\n\n\n\n\nThis blog explain how to copy files from host to docker container and vice versa.\n\n\n\n\n\n\nJun 1, 2022\n\n\n\n\n\n\n  \n\n\n\n\nWhy you should start a blog?\n\n\n\n\n\n\n\nBooks\n\n\n\n\nWelcome to my Quarto Blog\n\n\n\n\n\n\nMay 31, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "NLP.html",
    "href": "NLP.html",
    "title": "NLP Notes",
    "section": "",
    "text": "These are collections of stuff related to NLP that I came across and\n\n\n1.DocNow: Is a tool for appraising, collecting, and gathering consent for Twitter content.\n\nTwarc: Twarc is a command line tool and Python library for collecting tweet data from Twitter’s official API. It is designed for reliably collecting historical as well as realtime data, and can be used as a software library in your own tools and applications.\nHydrator is a desktop application for turning Tweet ID datasets back into tweet data to use in your research. It has been designed to be a reliable option for researchers who want to use their workstation for long running hydration jobs.\nThe Catalog is a community-sourced clearinghouse for tweet identifier datasets. Sharing tweet ids is a practice that is encouraged by Twitter as a way to share research data without negatively affecting users ability to have their data deleted or hidden from the web. We welcome your contributions!\n\n\n\n\n\nAwesome-NLP\nDeep Learning for NLP\nPyTorch Tutorial for Deep Learning Researchers\nPyTorch-Tutorial\n\n\n\n\n\nNatural Language Processing with Transformer\nTransformers for Machine Learning: A Deep Dive\nDeep Learning for NLP and Speech Recognition\n\n\n\n\n\nMultilingual Natural Language Processing - http://demo.clab.cs.cmu.edu/11737fa20/\nCSE 704 - Applied Natural Language Processing and Computational Social Science - https://kennyjoseph.github.io/cse702\nNLP Course | For You by Lena Voita - https://lena-voita.github.io/nlp_course.html\nDavid Bamman’s Course on Applied NLP - https://github.com/dbamman/anlp19 - Course Page\nCMU NLP - http://phontron.com/class/nn4nlp2017/schedule.html\nOxford Deep Learning NLP - https://github.com/oxford-cs-deepnlp-2017/lectures\nProbabilistic NLP course - https://uva-slpl.github.io/nlp2/syllabus.html\nMachine Learning: Linguistic & Sequence Modeling - https://seq2class.github.io/\nIntroduction to Natural Language Processing by Jacob Eisenstein - https://github.com/jacobeisenstein/gt-nlp-class\nA Course on word embeddings, variants, and applications: http://people.ds.cam.ac.uk/iv250/esslli2018.html"
  },
  {
    "objectID": "NLP.html#courses",
    "href": "NLP.html#courses",
    "title": "NLP Notes",
    "section": "",
    "text": "Multilingual Natural Language Processing - http://demo.clab.cs.cmu.edu/11737fa20/\nCSE 704 - Applied Natural Language Processing and Computational Social Science - https://kennyjoseph.github.io/cse702\nNLP Course | For You by Lena Voita - https://lena-voita.github.io/nlp_course.html\nDavid Bamman’s Course on Applied NLP - https://github.com/dbamman/anlp19 - Course Page\nCMU NLP - http://phontron.com/class/nn4nlp2017/schedule.html\nOxford Deep Learning NLP - https://github.com/oxford-cs-deepnlp-2017/lectures\nProbabilistic NLP course - https://uva-slpl.github.io/nlp2/syllabus.html\nMachine Learning: Linguistic & Sequence Modeling - https://seq2class.github.io/\nIntroduction to Natural Language Processing by Jacob Eisenstein - https://github.com/jacobeisenstein/gt-nlp-class\nA Course on word embeddings, variants, and applications: http://people.ds.cam.ac.uk/iv250/esslli2018.html"
  },
  {
    "objectID": "Quotes.html",
    "href": "Quotes.html",
    "title": "Quotes",
    "section": "",
    "text": "In this page, I documents Quotes that I found useful for my future reference.\n\n\n\n“One cannot think well, love well, sleep well, if one has not dined well.” ~Virginia Woolf “Keep learning, or risk becoming irrelevant.”"
  },
  {
    "objectID": "Quotes.html#keeping-well-as-a-phd-student",
    "href": "Quotes.html#keeping-well-as-a-phd-student",
    "title": "Quotes",
    "section": "",
    "text": "“One cannot think well, love well, sleep well, if one has not dined well.” ~Virginia Woolf “Keep learning, or risk becoming irrelevant.”"
  },
  {
    "objectID": "til.html",
    "href": "til.html",
    "title": "Today I Learned",
    "section": "",
    "text": "Title\n\n\nDate\n\n\n\n\n\n\n\n\n\nSkimpy: A light weight tool for creating summary statistics from dataframes\n\n\nDec 30, 2022\n\n\n\n\n\n\n\nKNN’s runtime\n\n\nNov 24, 2022\n\n\n\n\n\n\n\nModify Learning rate vs Early stopping\n\n\nNov 23, 2022\n\n\n\n\n\n\n\nEffect of Batch Size on Training Time\n\n\nNov 7, 2022\n\n\n\n\n\n\n\nEffect of Batch Size on Training Time\n\n\nNov 7, 2022\n\n\n\n\n\n\n\nSalary leak\n\n\nNov 7, 2022\n\n\n\n\n\n\n\nWeather predictions\n\n\nNov 4, 2022\n\n\n\n\n\n\n\nA decision tree for that?\n\n\nNov 3, 2022\n\n\n\n\n\n\n\nDropout and loss\n\n\nNov 2, 2022\n\n\n\n\n\n\n\nRegularizing a model\n\n\nNov 1, 2022\n\n\n\n\n\n\n\nDeep Learning\n\n\nOct 18, 2022\n\n\n\n\n\n\n\nTF_ID IF\n\n\nOct 16, 2022\n\n\n\n\n\n\n\nLogistic regression\n\n\nOct 16, 2022\n\n\n\n\n\n\n\nModel Recall\n\n\nOct 16, 2022\n\n\n\n\n\n\n\nLooping in Python Series\n\n\nOct 16, 2022\n\n\n\n\n\n\n\nR4DS Chapter 2 Exercises\n\n\nOct 15, 2022\n\n\n\n\n\n\n\nFormatting number with an underscore in Python\n\n\nOct 15, 2022\n\n\n\n\n\n\n\nActive Learning\n\n\nOct 15, 2022\n\n\n\n\n\n\n\nFatal: Not possible to fast-forward, aborting\n\n\nSep 17, 2022\n\n\n\n\n\n\n\nlabelled vs labeled\n\n\nMay 13, 2022\n\n\n\n\n\n\n\nConnecting two Indpendent Clauses with Comma and Semicolons\n\n\nMay 11, 2022\n\n\n\n\n\n\n\nBias and Varience in Machine Learning Algorithms\n\n\nMay 10, 2022\n\n\n\n\n\n\n\nBias and Varience in Machine Learning Algorithms\n\n\nMay 10, 2022\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "dailyml/dailyml0001/dailyml_0022.html",
    "href": "dailyml/dailyml0001/dailyml_0022.html",
    "title": "Active Learning",
    "section": "",
    "text": "Claire and Phil aren’t on the same page with their plan.\nThey want to train a machine learning model but want to minimize the number of samples they need to label. Labeling takes too long, and they want to avoid it as much as possible.\nClaire argues that they don’t need to train with the entire dataset. Instead, she believes they can maximize the model’s performance without using all the data.\nPhil disagrees. He argues that the only way to achieve the maximum possible performance is to train with the entire dataset. Since they aren’t willing to label all the data, they will need to settle for a mediocre model.\nWhat’s your opinion about this situation?\n\nAchieving the maximum possible performance without using the entire dataset is theoretically possible but very unlikely.\nThey can achieve the maximum possible performance without using the entire dataset by randomly sampling a portion of the data, labeling it, and training the model.\nThey can achieve the maximum possible performance without using the entire dataset, but they need a good strategy to sample the data they will label to train the model.\nThey will never achieve the maximum possible performance without using the entire dataset.\n\n\n\n\nWith active learning, we can build a model that will achieve better performance with fewer labeled samples by allowing the algorithm to choose the data that will provide the most information to its training process.\n\nClaire and Phil do not need to use the entire dataset to build a model that reaches its maximum possible performance. However, they will need a smart strategy to select the data they need to label.\nLet’s imagine a dataset with two classes that we can represent in two dimensions and a linear model that splits the data into two groups. Any samples around the lines’ boundaries that separate both classes are critical in our dataset. Those samples help the model decide how to split the data!\nBut what about samples far away from the split? They contribute much less to the model, and we don’t need them to find the separation between classes. The same happens with duplicate samples or samples that are too similar to existing ones.\nClaire and Phil, however, can’t depend on randomly sampling the dataset to decide which instances to label. They need a better strategy to determine which samples to pick.\nThis scenario is an example of Active learning. This learning technique allows us to build a better-performing machine learning model using fewer training labels by strategically choosing the samples to train the model.\n\n\n\n\nActive learning\nA Short Introduction to Active Learning\nTutorial on Active learning"
  },
  {
    "objectID": "dailyml/dailyml0001/dailyml_0022.html#answer",
    "href": "dailyml/dailyml0001/dailyml_0022.html#answer",
    "title": "Active Learning",
    "section": "",
    "text": "With active learning, we can build a model that will achieve better performance with fewer labeled samples by allowing the algorithm to choose the data that will provide the most information to its training process.\n\nClaire and Phil do not need to use the entire dataset to build a model that reaches its maximum possible performance. However, they will need a smart strategy to select the data they need to label.\nLet’s imagine a dataset with two classes that we can represent in two dimensions and a linear model that splits the data into two groups. Any samples around the lines’ boundaries that separate both classes are critical in our dataset. Those samples help the model decide how to split the data!\nBut what about samples far away from the split? They contribute much less to the model, and we don’t need them to find the separation between classes. The same happens with duplicate samples or samples that are too similar to existing ones.\nClaire and Phil, however, can’t depend on randomly sampling the dataset to decide which instances to label. They need a better strategy to determine which samples to pick.\nThis scenario is an example of Active learning. This learning technique allows us to build a better-performing machine learning model using fewer training labels by strategically choosing the samples to train the model."
  },
  {
    "objectID": "dailyml/dailyml0001/dailyml_0022.html#recommended-reading",
    "href": "dailyml/dailyml0001/dailyml_0022.html#recommended-reading",
    "title": "Active Learning",
    "section": "",
    "text": "Active learning\nA Short Introduction to Active Learning\nTutorial on Active learning"
  },
  {
    "objectID": "dailyml/dailyml0009/0003.html",
    "href": "dailyml/dailyml0009/0003.html",
    "title": "Effect of Batch Size on Training Time",
    "section": "",
    "text": "Lena ran two experiments training a neural network on a sample dataset. Her goal is to understand how different batch sizes affect the training process.\nOne of the experiments used a very small batch size, and the other used a batch size equal to all existing training data.\nAfter training and evaluating her models, Lena plotted the training and testing losses over 100 epochs of one of the experiments, and this is what she got:"
  },
  {
    "objectID": "dailyml/dailyml0009/0003.html#effect-of-batch-size",
    "href": "dailyml/dailyml0009/0003.html#effect-of-batch-size",
    "title": "Effect of Batch Size on Training Time",
    "section": "",
    "text": "Lena ran two experiments training a neural network on a sample dataset. Her goal is to understand how different batch sizes affect the training process.\nOne of the experiments used a very small batch size, and the other used a batch size equal to all existing training data.\nAfter training and evaluating her models, Lena plotted the training and testing losses over 100 epochs of one of the experiments, and this is what she got:"
  },
  {
    "objectID": "dailyml/dailyml0009/0003.html#attempt",
    "href": "dailyml/dailyml0009/0003.html#attempt",
    "title": "Effect of Batch Size on Training Time",
    "section": "Attempt",
    "text": "Attempt"
  },
  {
    "objectID": "dailyml/dailyml0009/0003.html#which-of-the-following-options-is-the-most-likely-to-be-true",
    "href": "dailyml/dailyml0009/0003.html#which-of-the-following-options-is-the-most-likely-to-be-true",
    "title": "Effect of Batch Size on Training Time",
    "section": "Which of the following options is the most likely to be true?",
    "text": "Which of the following options is the most likely to be true?\n\nThe model behind this plot corresponds to the experiment that used a very small batch size. This model was the one that took less time to train.\nThe model behind this plot corresponds to the experiment that used a very small batch size. This model was the one that took longer to train.\nThe model behind this plot corresponds to the experiment that used a batch size equal to all the available training data. This model was the one that took less time to train.\nThe model behind this plot corresponds to the experiment that used a batch size equal to all the available training data. This model was the one that took longer to train."
  },
  {
    "objectID": "dailyml/dailyml0009/0003.html#recommended-reading",
    "href": "dailyml/dailyml0009/0003.html#recommended-reading",
    "title": "Effect of Batch Size on Training Time",
    "section": "Recommended reading",
    "text": "Recommended reading\n\nCheck “What is the Curse of Dimensionality?” for an introduction to the Curse of Dimensionality.\nFor an explanation of data leakages, check “Data Leakage in Machine Learning”.\n“Introduction to Dimensionality Reduction for Machine Learning” covers different examples of dimensionality reduction."
  },
  {
    "objectID": "dailyml/dailyml0012/0003.html",
    "href": "dailyml/dailyml0012/0003.html",
    "title": "Modify Learning rate vs Early stopping",
    "section": "",
    "text": "“There’s something wrong with your network.”\nThat was the start of a message Camille received from her friend. She posted the plot of a neural network training loss online, and Camilla’s friend reached out to let her know.\nCamille is using gradient descent for the first time, so she appreciated the help.\n“Do you see what happened around the ninety epoch? The loss increases for a moment before coming back down again until the end. You don’t want that,”—concluded the message.\nWhat would you do if you were in Camille’s shoes?\n\nCamille should decrease the learning rate. That should stop the loss from increasing during training.\nCamille should increase the learning rate. That should stop the loss from increasing during training.\nCamille should use Early Stopping at around the ninety epoch.\nCamille shouldn’t do anything because her network has no problem."
  },
  {
    "objectID": "dailyml/dailyml0012/0003.html#something-wrong",
    "href": "dailyml/dailyml0012/0003.html#something-wrong",
    "title": "Modify Learning rate vs Early stopping",
    "section": "",
    "text": "“There’s something wrong with your network.”\nThat was the start of a message Camille received from her friend. She posted the plot of a neural network training loss online, and Camilla’s friend reached out to let her know.\nCamille is using gradient descent for the first time, so she appreciated the help.\n“Do you see what happened around the ninety epoch? The loss increases for a moment before coming back down again until the end. You don’t want that,”—concluded the message.\nWhat would you do if you were in Camille’s shoes?\n\nCamille should decrease the learning rate. That should stop the loss from increasing during training.\nCamille should increase the learning rate. That should stop the loss from increasing during training.\nCamille should use Early Stopping at around the ninety epoch.\nCamille shouldn’t do anything because her network has no problem."
  },
  {
    "objectID": "dailyml/dailyml0012/0003.html#attempt",
    "href": "dailyml/dailyml0012/0003.html#attempt",
    "title": "Modify Learning rate vs Early stopping",
    "section": "Attempt",
    "text": "Attempt\nGradient descent moves downhill on average, so a network that learns appropriately should see the loss decrease over the training session. However, individual updates can move in the opposite direction, causing the loss to fluctuate up and down.\nCamille’s plot shows the loss increasing momentarily, but it immediately starts decreasing. That’s normal, and Camille shouldn’t worry about it.\nSince the training process seems to be working correctly, modifying the learning loss might improve the results, but there’s nothing Camille needs to fix. If she uses Early Stopping, she will prevent the network from improving further.\nIn summary, Camille shouldn’t do anything at this point."
  },
  {
    "objectID": "dailyml/dailyml0012/0003.html#recommended-reading",
    "href": "dailyml/dailyml0012/0003.html#recommended-reading",
    "title": "Modify Learning rate vs Early stopping",
    "section": "Recommended reading",
    "text": "Recommended reading\n\nCheck “An overview of gradient descent optimization algorithms” for a deep dive into gradient descent and every one of its variants.\nCheck “Early Stopping” for an introduction to one of the most effective, easy-to-implement regularization techniques when training machine learning models"
  },
  {
    "objectID": "dailyml/dailyml0005/0003.html",
    "href": "dailyml/dailyml0005/0003.html",
    "title": "Dropout and loss",
    "section": "",
    "text": "It’s the final round of interviews, and Savannah has done wonderfully well.\nThe final round is more technical. Savannah needs to answer a series of questions to demonstrate her understanding of different fundamental concepts.\nWhile talking about deep learning, the interviewer focuses on the use of Dropout and asks Savannah a question about something she hasn’t thought about before:\nHow would you expect the training loss to behave if you train your model several times, each using an increasing Dropout rate?\n\nThe training loss will be higher as we increase the Dropout rate.\nThe training loss will be lower as we increase the Dropout rate.\nThe training loss will start oscillating as we increase the Dropout rate.\nThe training loss will stay the same independently of the Dropout rate."
  },
  {
    "objectID": "dailyml/dailyml0005/0003.html#dropout-and-loss",
    "href": "dailyml/dailyml0005/0003.html#dropout-and-loss",
    "title": "Dropout and loss",
    "section": "",
    "text": "It’s the final round of interviews, and Savannah has done wonderfully well.\nThe final round is more technical. Savannah needs to answer a series of questions to demonstrate her understanding of different fundamental concepts.\nWhile talking about deep learning, the interviewer focuses on the use of Dropout and asks Savannah a question about something she hasn’t thought about before:\nHow would you expect the training loss to behave if you train your model several times, each using an increasing Dropout rate?\n\nThe training loss will be higher as we increase the Dropout rate.\nThe training loss will be lower as we increase the Dropout rate.\nThe training loss will start oscillating as we increase the Dropout rate.\nThe training loss will stay the same independently of the Dropout rate."
  },
  {
    "objectID": "dailyml/dailyml0005/0003.html#attempt",
    "href": "dailyml/dailyml0005/0003.html#attempt",
    "title": "Dropout and loss",
    "section": "Attempt",
    "text": "Attempt\nHere is similar question asked on Stackoverflow : Validation loss when using Dropout\n\nDropout is a regularization method that works well and is vital for reducing overfitting.\nSometimes, the nodes in a neural network create strong dependencies on other nodes, which may lead to overfitting. An example is when a few nodes on a layer do most of the work, and the network ignores all the other nodes. Despite having many nodes on the layer, you only have a small percentage of those nodes contributing to predictions. We call this phenomenon “co-adaptation,” and we can tackle it using Dropout.\nDuring training, Dropout randomly removes a percentage of the nodes, forcing the network to learn in a balanced way. Now every node is on its own and can’t rely on other nodes to do their work. They have to work harder by themselves.\nOne crucial characteristic of Dropout will help Savanah answer the question correctly: Like most regularization methods, Dropout sacrifices training accuracy to improve generalization.\nIf we run a few training sessions, each using an increasing amount of Dropout, we should see the training loss trend higher. In other words, the more we regularize our model, the harder it will be to learn the training data."
  },
  {
    "objectID": "dailyml/dailyml0005/0003.html#recommended-reading",
    "href": "dailyml/dailyml0005/0003.html#recommended-reading",
    "title": "Dropout and loss",
    "section": "Recommended reading",
    "text": "Recommended reading\n\nFor more information about co-adaptation and how to use Dropout, check “Improving neural networks by preventing co-adaptation of feature detectors”.\n“A Gentle Introduction to Dropout for Regularizing Deep Neural Networks” is an excellent introduction to Dropout."
  },
  {
    "objectID": "dailyml/dailyml0003/0003.html",
    "href": "dailyml/dailyml0003/0003.html",
    "title": "Logistic regression",
    "section": "",
    "text": "There were four different exercises, but Jade only knew how to use logistic regression.\nLuckily, Jade only needed to choose one exercise and solve it. But she needs to ensure she can find a solution using logistic regression; she doesn’t have time to learn anything else.\nSelect every exercise that Jade could solve using logistic regression?\n\nThe first exercise is a regression problem.\nThe second exercise is a binary classification problem.\nThe third exercise is a 2-class classification problem.\nThe fourth exercise is a 3-class classification problem."
  },
  {
    "objectID": "dailyml/dailyml0003/0003.html#logistic-regression-exercises",
    "href": "dailyml/dailyml0003/0003.html#logistic-regression-exercises",
    "title": "Logistic regression",
    "section": "",
    "text": "There were four different exercises, but Jade only knew how to use logistic regression.\nLuckily, Jade only needed to choose one exercise and solve it. But she needs to ensure she can find a solution using logistic regression; she doesn’t have time to learn anything else.\nSelect every exercise that Jade could solve using logistic regression?\n\nThe first exercise is a regression problem.\nThe second exercise is a binary classification problem.\nThe third exercise is a 2-class classification problem.\nThe fourth exercise is a 3-class classification problem."
  },
  {
    "objectID": "dailyml/dailyml0003/0003.html#answer",
    "href": "dailyml/dailyml0003/0003.html#answer",
    "title": "Logistic regression",
    "section": "Answer",
    "text": "Answer\nFortunately for Jade, she can tackle the second, third, and fourth exercises.\nLogistic regression estimates the probability of an event occurring by outputting a single value bounded between 0 and 1. The closer the result is to zero, the less likely the event will occur, while the closer is to 1, the more likely it is.\nThis structure makes logistic regression ideal for tackling binary or 2-class classification problems. Notice that we can formulate a binary classification task as a 2-class classification task and vice versa.\nJade can also work on the 3-class classification exercise using the One-vs-All method (also called “One-vs-Rest.”) She can train three different logistic regression models:\nModel 1: Identifying Class 1 vs. [Class 2, Class 3] Model 2: Identifying Class 2 vs. [Class 1, Class 3] Model 3: Identifying Class 3 vs. [Class 1, Class 2]\nThe final result would be the prediction from the model with the highest confidence.\nJade should stay away from the first exercise. Logistic regression will not help her solve a regression problem."
  },
  {
    "objectID": "dailyml/dailyml0003/0003.html#recommended-reading",
    "href": "dailyml/dailyml0003/0003.html#recommended-reading",
    "title": "Logistic regression",
    "section": "Recommended reading",
    "text": "Recommended reading\n\nCheck out “Logistic Regression for Machine Learning” for an introduction to Logistic regression.\n“Essential Data Science Tips: How to Use One-Vs-Rest and One-Vs-One for Multi-Class Classification” is a great reference to understand how to use the One-vs-All method for multi-class classification."
  },
  {
    "objectID": "dailyml/dailyml0011/0003.html",
    "href": "dailyml/dailyml0011/0003.html",
    "title": "KNN’s runtime",
    "section": "",
    "text": "Evangeline is working with a dataset with a single column of data.\nShe wants to run k-Nearest Neighbors (KNN), but only if the algorithm is fast enough when making predictions.\nAssuming there are n samples in the dataset, which of the following will be the runtime of Evangeline’s KNN at prediction time?\n\nThe runtime that Evangeline should expect is O(1)\nThe runtime that Evangeline should expect is O(n)\nThe runtime that Evangeline should expect is O(log n)\nThe runtime that Evangeline should expect is O(n²)"
  },
  {
    "objectID": "dailyml/dailyml0011/0003.html#knns-runtime",
    "href": "dailyml/dailyml0011/0003.html#knns-runtime",
    "title": "KNN’s runtime",
    "section": "",
    "text": "Evangeline is working with a dataset with a single column of data.\nShe wants to run k-Nearest Neighbors (KNN), but only if the algorithm is fast enough when making predictions.\nAssuming there are n samples in the dataset, which of the following will be the runtime of Evangeline’s KNN at prediction time?\n\nThe runtime that Evangeline should expect is O(1)\nThe runtime that Evangeline should expect is O(n)\nThe runtime that Evangeline should expect is O(log n)\nThe runtime that Evangeline should expect is O(n²)"
  },
  {
    "objectID": "dailyml/dailyml0011/0003.html#attempt",
    "href": "dailyml/dailyml0011/0003.html#attempt",
    "title": "KNN’s runtime",
    "section": "Attempt",
    "text": "Attempt\nk-Nearest Neighbors’ runtime is O(nd) because we need to compute the distance to each feature of every sample. Here n represents the number of instances, and d the number of features.\nEvangeline is working with a single feature, so d = 1. Therefore, in this case, the runtime of KNN is O(n)."
  },
  {
    "objectID": "dailyml/dailyml0011/0003.html#recommended-reading",
    "href": "dailyml/dailyml0011/0003.html#recommended-reading",
    "title": "KNN’s runtime",
    "section": "Recommended reading",
    "text": "Recommended reading\nHere is a Stack Exchange answer that covers KNN’s runtime complexity in detail."
  },
  {
    "objectID": "dailynlp/nlp0002/nlp0002.html",
    "href": "dailynlp/nlp0002/nlp0002.html",
    "title": "Deep Learning",
    "section": "",
    "text": "Deep Lea"
  },
  {
    "objectID": "daily.html",
    "href": "daily.html",
    "title": "Today I Learned",
    "section": "",
    "text": "Title\n\n\nDate\n\n\n\n\n\n\n\n\n\nSkimpy: A light weight tool for creating summary statistics from dataframes\n\n\nDec 30, 2022\n\n\n\n\n\n\n\nKNN’s runtime\n\n\nNov 24, 2022\n\n\n\n\n\n\n\nModify Learning rate vs Early stopping\n\n\nNov 23, 2022\n\n\n\n\n\n\n\nEffect of Batch Size on Training Time\n\n\nNov 7, 2022\n\n\n\n\n\n\n\nEffect of Batch Size on Training Time\n\n\nNov 7, 2022\n\n\n\n\n\n\n\nSalary leak\n\n\nNov 7, 2022\n\n\n\n\n\n\n\nWeather predictions\n\n\nNov 4, 2022\n\n\n\n\n\n\n\nA decision tree for that?\n\n\nNov 3, 2022\n\n\n\n\n\n\n\nDropout and loss\n\n\nNov 2, 2022\n\n\n\n\n\n\n\nRegularizing a model\n\n\nNov 1, 2022\n\n\n\n\n\n\n\nDeep Learning\n\n\nOct 18, 2022\n\n\n\n\n\n\n\nTF_ID IF\n\n\nOct 16, 2022\n\n\n\n\n\n\n\nLogistic regression\n\n\nOct 16, 2022\n\n\n\n\n\n\n\nModel Recall\n\n\nOct 16, 2022\n\n\n\n\n\n\n\nLooping in Python Series\n\n\nOct 16, 2022\n\n\n\n\n\n\n\nR4DS Chapter 2 Exercises\n\n\nOct 15, 2022\n\n\n\n\n\n\n\nFormatting number with an underscore in Python\n\n\nOct 15, 2022\n\n\n\n\n\n\n\nActive Learning\n\n\nOct 15, 2022\n\n\n\n\n\n\n\nFatal: Not possible to fast-forward, aborting\n\n\nSep 17, 2022\n\n\n\n\n\n\n\nlabelled vs labeled\n\n\nMay 13, 2022\n\n\n\n\n\n\n\nConnecting two Indpendent Clauses with Comma and Semicolons\n\n\nMay 11, 2022\n\n\n\n\n\n\n\nBias and Varience in Machine Learning Algorithms\n\n\nMay 10, 2022\n\n\n\n\n\n\n\nBias and Varience in Machine Learning Algorithms\n\n\nMay 10, 2022\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "dailynlp.html",
    "href": "dailynlp.html",
    "title": "Daily NLP",
    "section": "",
    "text": "Title\n\n\nDate\n\n\n\n\n\n\n\n\n\nDeep Learning\n\n\nOct 18, 2022\n\n\n\n\n\n\n\nTF_ID IF\n\n\nOct 16, 2022\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "dailynlp/py0002/py0002.html",
    "href": "dailynlp/py0002/py0002.html",
    "title": "TF_ID IF",
    "section": "",
    "text": "from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom visual import show_tfidf  \n\n\ndocs = [\n    \"it is a good day, I like to stay here\",\n    \"I am happy to be here\",\n    \"I am bob\",\n    \"it is sunny today\",\n    \"I have a party today\",\n    \"it is a dog and that is a cat\",\n    \"there are dog and cat on the tree\",\n    \"I study hard this morning\",\n    \"today is a good day\",\n    \"tomorrow will be a good day\",\n    \"I like coffee, I like book and I like apple\",\n    \"I do not like it\",\n    \"I am kitty, I like bob\",\n    \"I do not care who like bob, but I like kitty\",\n    \"It is coffee time, bring your cup\",\n]\n\nvectorizer = TfidfVectorizer()\ntf_idf = vectorizer.fit_transform(docs)\nprint(\"idf: \", [(n, idf) for idf, n in zip(vectorizer.idf_, vectorizer.get_feature_names())])\nprint(\"v2i: \", vectorizer.vocabulary_)\n\n\nq = \"I get a coffee cup\"\nqtf_idf = vectorizer.transform([q])\nres = cosine_similarity(tf_idf, qtf_idf)\nres = res.ravel().argsort()[-3:]\nprint(\"\\ntop 3 docs for '{}':\\n{}\".format(q, [docs[i] for i in res[::-1]]))\n\n\ni2v = {i: v for v, i in vectorizer.vocabulary_.items()}\ndense_tfidf = tf_idf.todense()\nshow_tfidf(dense_tfidf, [i2v[i] for i in range(dense_tfidf.shape[1])], \"tfidf_sklearn_matrix\")\n\nidf:  [('am', 2.386294361119891), ('and', 2.386294361119891), ('apple', 3.0794415416798357), ('are', 3.0794415416798357), ('be', 2.6739764335716716), ('bob', 2.386294361119891), ('book', 3.0794415416798357), ('bring', 3.0794415416798357), ('but', 3.0794415416798357), ('care', 3.0794415416798357), ('cat', 2.6739764335716716), ('coffee', 2.6739764335716716), ('cup', 3.0794415416798357), ('day', 2.386294361119891), ('do', 2.6739764335716716), ('dog', 2.6739764335716716), ('good', 2.386294361119891), ('happy', 3.0794415416798357), ('hard', 3.0794415416798357), ('have', 3.0794415416798357), ('here', 2.6739764335716716), ('is', 1.9808292530117262), ('it', 1.9808292530117262), ('kitty', 2.6739764335716716), ('like', 1.9808292530117262), ('morning', 3.0794415416798357), ('not', 2.6739764335716716), ('on', 3.0794415416798357), ('party', 3.0794415416798357), ('stay', 3.0794415416798357), ('study', 3.0794415416798357), ('sunny', 3.0794415416798357), ('that', 3.0794415416798357), ('the', 3.0794415416798357), ('there', 3.0794415416798357), ('this', 3.0794415416798357), ('time', 3.0794415416798357), ('to', 2.6739764335716716), ('today', 2.386294361119891), ('tomorrow', 3.0794415416798357), ('tree', 3.0794415416798357), ('who', 3.0794415416798357), ('will', 3.0794415416798357), ('your', 3.0794415416798357)]\nv2i:  {'it': 22, 'is': 21, 'good': 16, 'day': 13, 'like': 24, 'to': 37, 'stay': 29, 'here': 20, 'am': 0, 'happy': 17, 'be': 4, 'bob': 5, 'sunny': 31, 'today': 38, 'have': 19, 'party': 28, 'dog': 15, 'and': 1, 'that': 32, 'cat': 10, 'there': 34, 'are': 3, 'on': 27, 'the': 33, 'tree': 40, 'study': 30, 'hard': 18, 'this': 35, 'morning': 25, 'tomorrow': 39, 'will': 42, 'coffee': 11, 'book': 6, 'apple': 2, 'do': 14, 'not': 26, 'kitty': 23, 'care': 9, 'who': 41, 'but': 8, 'time': 36, 'bring': 7, 'your': 43, 'cup': 12}\n\ntop 3 docs for 'I get a coffee cup':\n['It is coffee time, bring your cup', 'I like coffee, I like book and I like apple', 'I do not care who like bob, but I like kitty']\n\n\n\n\n\nSee you tomorrow !"
  },
  {
    "objectID": "bookreading.html",
    "href": "bookreading.html",
    "title": "Book I am reading",
    "section": "",
    "text": "Title\n\n\nDate\n\n\n\n\n\n\n\n\n\nDeep Work\n\n\nOct 15, 2022\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "dailyml/dailyml0010/0003.html",
    "href": "dailyml/dailyml0010/0003.html",
    "title": "Effect of Batch Size on Training Time",
    "section": "",
    "text": "Lena ran two experiments training a neural network on a sample dataset. Her goal is to understand how different batch sizes affect the training process.\nOne of the experiments used a very small batch size, and the other used a batch size equal to all existing training data.\nAfter training and evaluating her models, Lena plotted the training and testing losses over 100 epochs of one of the experiments, and this is what she got:"
  },
  {
    "objectID": "dailyml/dailyml0010/0003.html#effect-of-batch-size",
    "href": "dailyml/dailyml0010/0003.html#effect-of-batch-size",
    "title": "Effect of Batch Size on Training Time",
    "section": "",
    "text": "Lena ran two experiments training a neural network on a sample dataset. Her goal is to understand how different batch sizes affect the training process.\nOne of the experiments used a very small batch size, and the other used a batch size equal to all existing training data.\nAfter training and evaluating her models, Lena plotted the training and testing losses over 100 epochs of one of the experiments, and this is what she got:"
  },
  {
    "objectID": "dailyml/dailyml0010/0003.html#attempt",
    "href": "dailyml/dailyml0010/0003.html#attempt",
    "title": "Effect of Batch Size on Training Time",
    "section": "Attempt",
    "text": "Attempt"
  },
  {
    "objectID": "dailyml/dailyml0010/0003.html#which-of-the-following-options-is-the-most-likely-to-be-true",
    "href": "dailyml/dailyml0010/0003.html#which-of-the-following-options-is-the-most-likely-to-be-true",
    "title": "Effect of Batch Size on Training Time",
    "section": "Which of the following options is the most likely to be true?",
    "text": "Which of the following options is the most likely to be true?\n\nThe model behind this plot corresponds to the experiment that used a very small batch size. This model was the one that took less time to train.\nThe model behind this plot corresponds to the experiment that used a very small batch size. This model was the one that took longer to train.\nThe model behind this plot corresponds to the experiment that used a batch size equal to all the available training data. This model was the one that took less time to train.\nThe model behind this plot corresponds to the experiment that used a batch size equal to all the available training data. This model was the one that took longer to train."
  },
  {
    "objectID": "dailyml/dailyml0010/0003.html#recommended-reading",
    "href": "dailyml/dailyml0010/0003.html#recommended-reading",
    "title": "Effect of Batch Size on Training Time",
    "section": "Recommended reading",
    "text": "Recommended reading\n\nCheck “What is the Curse of Dimensionality?” for an introduction to the Curse of Dimensionality.\nFor an explanation of data leakages, check “Data Leakage in Machine Learning”.\n“Introduction to Dimensionality Reduction for Machine Learning” covers different examples of dimensionality reduction."
  },
  {
    "objectID": "dailyml/dailyml0004/0003.html",
    "href": "dailyml/dailyml0004/0003.html",
    "title": "Regularizing a model",
    "section": "",
    "text": "Charlotte’s machine learning model is overfitting.\nShe needs to find a way to handle it, but before trying anything, she wants to understand her options.\nWhich of the following are regularization techniques that Charlotte could consider?\n\nValidation-based early stopping\nDropout\nData augmentation\nCross-validation"
  },
  {
    "objectID": "dailyml/dailyml0004/0003.html#regularizing-a-model",
    "href": "dailyml/dailyml0004/0003.html#regularizing-a-model",
    "title": "Regularizing a model",
    "section": "",
    "text": "Charlotte’s machine learning model is overfitting.\nShe needs to find a way to handle it, but before trying anything, she wants to understand her options.\nWhich of the following are regularization techniques that Charlotte could consider?\n\nValidation-based early stopping\nDropout\nData augmentation\nCross-validation"
  },
  {
    "objectID": "dailyml/dailyml0004/0003.html#attempt",
    "href": "dailyml/dailyml0004/0003.html#attempt",
    "title": "Regularizing a model",
    "section": "Attempt",
    "text": "Attempt\nValidation-based early stopping is a regularization technique that stops the training process as soon as the generalization error of the model increases. In other words, if the model’s performance on the validation set starts degrading, the training process stops.\nDropout is a regularization method that works well and is vital for reducing overfitting. Dropout randomly removes a percentage of the nodes, forcing the network to learn in a balanced way, and tackling a phenomenon that we call “co-adaptation.”\nData augmentation has a regularization effect. Increasing the training data through data augmentation decreases the model’s variance and, in turn, increases the model’s generalization ability.\nFinally, cross-validation is a validation scheme and not a regularization method."
  },
  {
    "objectID": "dailyml/dailyml0004/0003.html#recommended-reading",
    "href": "dailyml/dailyml0004/0003.html#recommended-reading",
    "title": "Regularizing a model",
    "section": "Recommended reading",
    "text": "Recommended reading\n\nCheck “Early Stopping” for an introduction to one of the most effective, easy-to-implement regularization techniques when training machine learning models.\n“A Gentle Introduction to Dropout for Regularizing Deep Neural Networks” is an excellent introduction to Dropout.\n“The Essential Guide to Data Augmentation in Deep Learning” is an excellent article discussing data augmentation in detail."
  },
  {
    "objectID": "dailyml/dailyml0002/dailyml_0022.html",
    "href": "dailyml/dailyml0002/dailyml_0022.html",
    "title": "Model Recall",
    "section": "",
    "text": "The model’s recall\nA team built a binary classification model. They named the classes A and B.\nAfter finishing training, they evaluated the model on a validation set, and here is the confusion matrix with the results:\n\nGiven the above confusion matrix, what of the following correctly represents the model’s recall predicting classes A and B, respectively?\n\nThe model’s recall predicting class A is 80%, and class B is 80%.\nThe model’s recall predicting class A is 88%, and class B is 68%.\nThe model’s recall predicting class A is 88%, and class B is 88%.\nThe model’s recall predicting class A is 68%, and class B is 88%.\n\n\n\nAnswer\n\n\nRecommended Reading\n\nHow to Calculate Precision, Recall, and F-Measure for Imbalanced Classification"
  },
  {
    "objectID": "dailyml/dailyml0007/0003.html",
    "href": "dailyml/dailyml0007/0003.html",
    "title": "Weather predictions",
    "section": "",
    "text": "For the first project, Cam wants to work with the weather dataset he found online.\nCam’s Machine Learning class was a ton of fun. Their professor let them choose a problem to solve to allow them to showcase what they’ve learned so far.\nCam wants to predict the probability of snowing based on four factors: the date, the air temperature, the location, and the air pressure.\nWhich of the following is the most appropriate algorithm that Cam should use to solve this problem?\n\nCam should use linear regression.\nCam should use logistic regression.\nCam should use K-means.\nCam should use DBSCAN."
  },
  {
    "objectID": "dailyml/dailyml0007/0003.html#weather-predictions",
    "href": "dailyml/dailyml0007/0003.html#weather-predictions",
    "title": "Weather predictions",
    "section": "",
    "text": "For the first project, Cam wants to work with the weather dataset he found online.\nCam’s Machine Learning class was a ton of fun. Their professor let them choose a problem to solve to allow them to showcase what they’ve learned so far.\nCam wants to predict the probability of snowing based on four factors: the date, the air temperature, the location, and the air pressure.\nWhich of the following is the most appropriate algorithm that Cam should use to solve this problem?\n\nCam should use linear regression.\nCam should use logistic regression.\nCam should use K-means.\nCam should use DBSCAN."
  },
  {
    "objectID": "dailyml/dailyml0007/0003.html#attempt",
    "href": "dailyml/dailyml0007/0003.html#attempt",
    "title": "Weather predictions",
    "section": "Attempt",
    "text": "Attempt\nLogistic Regression is used when the dependent variable(target) is categorical.For example,\n\nTo predict whether an email is spam (1) or (0)\nWhether the tumor is malignant (1) or not (0)\n\nConsider a scenario where we need to classify whether an email is spam or not. If we use linear regression for this problem, there is a need for setting up a threshold based on which classification can be done. Say if the actual class is malignant, predicted continuous value 0.4 and the threshold value is 0.5, the data point will be classified as not malignant which can lead to serious consequence in real time.\nFrom this example, it can be inferred that linear regression is not suitable for classification problem. Linear regression is unbounded, and this brings logistic regression into picture.\nWhen to use logistic regression?\nLogistic regression is applied to predict the categorical dependent variable. In other words, it’s used when the prediction is categorical, for example, yes or no, true or false, 0 or 1. The predicted probability or output of logistic regression can be either one of them, and there’s no middle ground.\nIn essence, logistic regression helps solve probability and classification problems. In other words, you can expect only classification and probability outcomes from logistic regression.\nWhile logistic regression predicts the categorical variable for one or more independent variables, linear regression predicts the continuous variable. In other words, logistic regression provides a constant output, whereas linear regression offers a continuous output.\nSince the outcome is continuous in linear regression, there are infinite possible values for the outcome. But for logistic regression, the number of possible outcome values is limited.\nCam’s problem has two possible outcomes: it will snow or it won’t, and Cam wants his model to return the probability of it. Logistic regression is an excellent fit for any problem with a binary outcome.\nLogistic regression estimates the probability of an event occurring based on a dataset of independent variables. Linear regression, on the other hand, predicts the continuous dependent variable using a dataset of independent variables.\nK-means and DBSCAN are clustering algorithms and are not a good approach for this problem."
  },
  {
    "objectID": "dailyml/dailyml0007/0003.html#recommended-reading",
    "href": "dailyml/dailyml0007/0003.html#recommended-reading",
    "title": "Weather predictions",
    "section": "Recommended reading",
    "text": "Recommended reading\n\n“Linear Regression vs Logistic Regression” does a full comparison between linear and logistic regression.\nCheck “8 Clustering Algorithms in Machine Learning that All Data Scientists Should Know” for an explanation of K-Means and DBSCAN."
  },
  {
    "objectID": "dailyml/dailyml0008/0003.html",
    "href": "dailyml/dailyml0008/0003.html",
    "title": "Salary leak",
    "section": "",
    "text": "There was a massive data leak, and for some mysterious reason, you came across a dataset full of compensation data from top tech companies in Silicon Valley.\nYou thought it wouldn’t hurt to play around with the data for a little bit. You could finally build a model to predict future compensation based on the different attributes of each employee.\nBut one thing becomes apparent from the start: You need to cut down useless features to build something useful.\nDimensionality reduction to the rescue. You haven’t done it before and want to ensure you are doing it correctly.\nHow should you apply dimensionality reduction to your data?\n\nReduce the dimensions of the training dataset. It’s unnecessary to use dimensionality reduction on the test dataset.\nReduce the dimensions of the entire dataset. Split your data into training and test right after.\nReduce the dimensions of the training dataset, then reduce the dimensions of the test dataset. We can use different dimensionality reduction techniques as long as both splits end up with the same features.\nReduce the dimensions of the training dataset, then apply the same transformations to the test dataset."
  },
  {
    "objectID": "dailyml/dailyml0008/0003.html#salary-leak",
    "href": "dailyml/dailyml0008/0003.html#salary-leak",
    "title": "Salary leak",
    "section": "",
    "text": "There was a massive data leak, and for some mysterious reason, you came across a dataset full of compensation data from top tech companies in Silicon Valley.\nYou thought it wouldn’t hurt to play around with the data for a little bit. You could finally build a model to predict future compensation based on the different attributes of each employee.\nBut one thing becomes apparent from the start: You need to cut down useless features to build something useful.\nDimensionality reduction to the rescue. You haven’t done it before and want to ensure you are doing it correctly.\nHow should you apply dimensionality reduction to your data?\n\nReduce the dimensions of the training dataset. It’s unnecessary to use dimensionality reduction on the test dataset.\nReduce the dimensions of the entire dataset. Split your data into training and test right after.\nReduce the dimensions of the training dataset, then reduce the dimensions of the test dataset. We can use different dimensionality reduction techniques as long as both splits end up with the same features.\nReduce the dimensions of the training dataset, then apply the same transformations to the test dataset."
  },
  {
    "objectID": "dailyml/dailyml0008/0003.html#attempt",
    "href": "dailyml/dailyml0008/0003.html#attempt",
    "title": "Salary leak",
    "section": "Attempt",
    "text": "Attempt\nDimensionality reduction is a technique that reduces the number of features in a dataset. It is used to reduce the complexity of a model and to reduce the training time.\nYou found there are too many features to create a useful model. The Curse of Dimensionality states that, as the dimensionality of the data increases, the amount of data needed to train a learning algorithm grows exponentially.\nThere are different techniques to reduce the dimensionality of a dataset. They all follow the same principle: you start with a dataset, reduce its dimensionality, and obtain a new dataset with fewer features. Depending on the technique, the final dataset may contain a subset of the initial features or even have entirely different columns not present in the initial dataset.\nThe first choice argues that you only need to worry about reducing the dimension of the training dataset. That’s incorrect. How can you test a model trained with a dataset containing different features?\nThe second choice argues for reducing the dimensionality of the entire dataset and splitting the data right after that. Dimensionality reduction algorithms like PCA will use information about the whole dataset to produce new features. If we apply this algorithm to all of our data—including the test data, which we aren’t supposed to know about— we’ll leak details from the test data into the training set.\nThe third choice is also incorrect. You need to apply dimensionality reduction separately to the training and test datasets and make sure you use the same transformations from the training data on the test data.\nFor example, imagine your dimensionality reduction technique creates a new feature based on the mean of another two columns. If you compute this mean separately on the train and test data, the resulting columns will come from different mean values. You need to avoid this problem by using what the fourth choice suggests: apply the same transformations and use the same information from the training and testing sets."
  },
  {
    "objectID": "dailyml/dailyml0008/0003.html#recommended-reading",
    "href": "dailyml/dailyml0008/0003.html#recommended-reading",
    "title": "Salary leak",
    "section": "Recommended reading",
    "text": "Recommended reading\n\nCheck “What is the Curse of Dimensionality?” for an introduction to the Curse of Dimensionality.\nFor an explanation of data leakages, check “Data Leakage in Machine Learning”.\n“Introduction to Dimensionality Reduction for Machine Learning” covers different examples of dimensionality reduction."
  },
  {
    "objectID": "dailyml/dailyml0006/0003.html",
    "href": "dailyml/dailyml0006/0003.html",
    "title": "A decision tree for that?",
    "section": "",
    "text": "Gianna had the opportunity to meet some of her technical heroes at a conference, and of course, she peppered them with questions!\nAt some point, the conversation centered around a machine learning solution they implemented. Gianna was delighted to hear that they used a decision tree for their solution.\nThe next day, she was still thinking about the conversation. She didn’t have much machine learning experience but knew enough about decision trees to feel validated. It was fantastic to find out that decision trees are useful!\nWhich of the following machine learning problems can you solve with a decision tree?\n\nBinary classification problems where you need to decide the correct category for a sample among two possible choices.\nMulti-class classification problems where you need to decide the correct category for a sample among multiple choices.\nMulti-label classification problems where you need to decide the correct categories for a sample among multiple choices.\nRegression problems where you need to predict a continuous output."
  },
  {
    "objectID": "dailyml/dailyml0006/0003.html#a-decision-tree-for-that",
    "href": "dailyml/dailyml0006/0003.html#a-decision-tree-for-that",
    "title": "A decision tree for that?",
    "section": "",
    "text": "Gianna had the opportunity to meet some of her technical heroes at a conference, and of course, she peppered them with questions!\nAt some point, the conversation centered around a machine learning solution they implemented. Gianna was delighted to hear that they used a decision tree for their solution.\nThe next day, she was still thinking about the conversation. She didn’t have much machine learning experience but knew enough about decision trees to feel validated. It was fantastic to find out that decision trees are useful!\nWhich of the following machine learning problems can you solve with a decision tree?\n\nBinary classification problems where you need to decide the correct category for a sample among two possible choices.\nMulti-class classification problems where you need to decide the correct category for a sample among multiple choices.\nMulti-label classification problems where you need to decide the correct categories for a sample among multiple choices.\nRegression problems where you need to predict a continuous output."
  },
  {
    "objectID": "dailyml/dailyml0006/0003.html#attempt",
    "href": "dailyml/dailyml0006/0003.html#attempt",
    "title": "A decision tree for that?",
    "section": "Attempt",
    "text": "Attempt\nEvery one of the choices is a correct answer: decision trees are really powerful!\nWe usually discuss two types of decision trees in machine learning: classification and regression trees. The former covers the first three choices, while the latter covers the fourth choice.\nBinary classification problems aim to classify one sample into two different categories. Multi-class classification problems are similar, but they classify samples into more than two categories. Decision trees are a perfect fit for these problems.\nMulti-label classification is somewhat different. Here we want to classify a sample into one or more categories. Decision trees can also solve these problems. Check out Scikit-Learn’s implementation to see how they tackle multi-label classification.\nFinally, decision trees can also solve regression problems where we want to predict a continuous target variable. “How can Regression Trees be used for Solving Regression Problems?” is an excellent introduction."
  },
  {
    "objectID": "dailyml/dailyml0006/0003.html#recommended-reading",
    "href": "dailyml/dailyml0006/0003.html#recommended-reading",
    "title": "A decision tree for that?",
    "section": "Recommended reading",
    "text": "Recommended reading\n\nScikit-Learn’s “Multiclass and multioutput algorithms” will introduce you to solve these problems using Scikit-Learn’s decision tree implementation.\n“How can Regression Trees be used for Solving Regression Problems?” is an excellent article about using decision trees for regression tasks."
  },
  {
    "objectID": "dailyR.html",
    "href": "dailyR.html",
    "title": "Daily R Practice",
    "section": "",
    "text": "Title\n\n\nDate\n\n\n\n\n\n\n\n\n\nR4DS Chapter 2 Exercises\n\n\nOct 15, 2022\n\n\n\n\n\n\n\nR4DS Chapter 2 Exercises\n\n\nOct 15, 2022\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "dailyml.html",
    "href": "dailyml.html",
    "title": "Daily Machine Learning",
    "section": "",
    "text": "Title\n\n\nDate\n\n\n\n\n\n\n\n\n\nKNN’s runtime\n\n\nNov 24, 2022\n\n\n\n\n\n\n\nModify Learning rate vs Early stopping\n\n\nNov 23, 2022\n\n\n\n\n\n\n\nEffect of Batch Size on Training Time\n\n\nNov 7, 2022\n\n\n\n\n\n\n\nEffect of Batch Size on Training Time\n\n\nNov 7, 2022\n\n\n\n\n\n\n\nSalary leak\n\n\nNov 7, 2022\n\n\n\n\n\n\n\nWeather predictions\n\n\nNov 4, 2022\n\n\n\n\n\n\n\nA decision tree for that?\n\n\nNov 3, 2022\n\n\n\n\n\n\n\nDropout and loss\n\n\nNov 2, 2022\n\n\n\n\n\n\n\nRegularizing a model\n\n\nNov 1, 2022\n\n\n\n\n\n\n\nLogistic regression\n\n\nOct 16, 2022\n\n\n\n\n\n\n\nModel Recall\n\n\nOct 16, 2022\n\n\n\n\n\n\n\nActive Learning\n\n\nOct 15, 2022\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "todo.html",
    "href": "todo.html",
    "title": "TodO",
    "section": "",
    "text": "TodO\n\ntext : “Daily Paper Summary” file: dailypaper.qmd - text: “The Big Book of My PhD” file: https://shmuhammad2004.github.io/bigbookofphd/intro.html\n\n\nproject: type: website output-dir: docs resources: - “_redirects.txt” freeze: true\nwebsite: comments: giscus: repo: “shmuhammad2004/shmuhammadblog” repo-id: “R_kgDOHb5q2A” category: “Announcements” category-id: “DIC_kwDOHb5q2M4CPbHo” mapping: “pathname” reactions-enabled : true input-position: “top” theme: “light” language: “en” google-analytics: “UA-124900795-1” title: “Shamsuddeen Hassan Muhammad’s Blog” site-url: https://shmuhammadblog.github.io description: “Shamsuddeen Muhammad Blog” twitter-card: site : “@shmuhammadd” creator : “Shamsuddeen Hassan Muhammad” navbar: title: “Shamsuddeen Hassan Muhammad” background: “#000000” foreground: “#FFFFFF” pinned: true left: - text: “NLP Researcher and Data Scientist” right: - text: “Blog” file: index.qmd - file: til.qmd - text: “Atomic Habit” menu: - text : “Book I am reading and Summary” file: bookreading.qmd - text : “Daily Machine Learning” file: dailyml.qmd - text : “Daily NLP” file: dailynlp.qmd - text : “Daily Python” file: dailypython.qmd - text : “Daily R” file: dailyR.qmd - text: “About” file: about.qmd - icon: twitter href: https://twitter.com/Shmuhammadd - icon: github href: https://github.com/shmuhammad2004 - icon: linkedin href: https://www.linkedin.com/in/shmuhammad/ - icon: rss href: index.xml page-footer: left: - href: license.qmd text: License format: html: theme: [litera, styles.scss] include-in-header: plausible.html highlight-style: nord"
  },
  {
    "objectID": "datascience.html",
    "href": "datascience.html",
    "title": "Data Science Resources",
    "section": "",
    "text": "DevOps for Data Science"
  },
  {
    "objectID": "datascience.html#books",
    "href": "datascience.html#books",
    "title": "Data Science Resources",
    "section": "",
    "text": "DevOps for Data Science"
  },
  {
    "objectID": "dailypaper.html",
    "href": "dailypaper.html",
    "title": "Daily paper summary",
    "section": "",
    "text": "Title\n\n\nDate\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/whyblog/whyblog.html",
    "href": "blog/whyblog/whyblog.html",
    "title": "Why you should start a blog?",
    "section": "",
    "text": "My mentor, Sebastian Ruder advises PhD students to write blogs and has this to say: “Having a blog is the single thing that has led to the most positive interactions throughout my PhD.”1\nI was inspired, but I’ve been holding off writing blog posts consistently. I started in 2020 and stopped after two blog posts. Recently, I read a blog post by the co-founder of fast.ai, Rachel Thomas, “Why you should write blogpost”2 and watched an RStudio talk by David Robinson, “The unreasonable effectiveness of public work.”3 Both Rachel and David convinced me about the benefits of writing a blog post. Therefore, I braced up to start blogging consistently at least once fortnightly. As a Ph.D. student, writing habits will strongly build my writing and creativity muscles to the max.\nThere are many reasons one writes a blog. For me, below are only five reasons I think a blog will help me along the way of building my career."
  },
  {
    "objectID": "blog/whyblog/whyblog.html#deliberate-practice",
    "href": "blog/whyblog/whyblog.html#deliberate-practice",
    "title": "Why you should start a blog?",
    "section": "Deliberate practice",
    "text": "Deliberate practice\nDeliberate practice is a systematic, focused, consistent, goal-oriented training that builds expertise or improves performance.45 Building expertise in any field is not a marathon; it is a series of Sprints. Evidence has shown that experts or geniuses are always made, not born6 7. For example, bodybuilders, musicians, and footballers consistently practice to achieve mastery. No one becomes an expert from day one. The same is also true for writing and any other skills. Consistent writing, even small content but engaging and informative, will improve your writing skills. As we fondly say, “practice makes perfect.” Consistent practice allows one to do a task while thinking about other things. For example, a professional orator can deliver an excellent speech without reading from any single note. Stopping to think about the task can sometimes result in a flawless performance. People refer to this performance as being “in the zone. Aristotle said:”We are what we repeatedly do. Excellence, then, is not an act, but a habit.” \nDeliberate practice does not make what we learn easier; it changes the brain (Myelination). This concept is notably expressed as “cells that fire together, wire together.” Sometimes, we reach an “aha!” moment when learning difficult stuff - that is when someone has been struggling to understand a concept, and it suddenly becomes apparent - the clarity does not come out of nowhere.\n\nRather, it results from a steady accumulation of information. That’s because adding additional information opens up memories associated with the task. Once those memory neurons are active, they can form new connections. They also can form stronger connections within an existing network. Over time, your level of understanding increases until you suddenly “get” it 8.\n\nTherefore, this blog will serve as a way for me to do deliberate practices of many skills (writing, machine learning, visualization, python, r and, many more)"
  },
  {
    "objectID": "blog/whyblog/whyblog.html#repository-for-my-future-self",
    "href": "blog/whyblog/whyblog.html#repository-for-my-future-self",
    "title": "Why you should start a blog?",
    "section": "Repository for my future self",
    "text": "Repository for my future self\nI am absent-minded. I write code and forget how I did it or google the same thing many times So, anything that I often google or write a complex program, I will write a blog post on it. That way, I will refer to it. Hadley Wickham inspired me in his book R for Data Science; he said, if you write the same code three times, then, you write a function for that code. Hadley’s idea was adapted from code refactoring rule of thumb (Rule of three), which states that “two instances of similar code don’t require refactoring, but when similar code is used three times, it should be extracted into a new procedure.”9"
  },
  {
    "objectID": "blog/whyblog/whyblog.html#build-public-profile-and-network",
    "href": "blog/whyblog/whyblog.html#build-public-profile-and-network",
    "title": "Why you should start a blog?",
    "section": "Build public profile and network",
    "text": "Build public profile and network\nPutting your work or your skills to the public is a way to put your best foot forwards. Public work can be anything like Tweets, Blog post, GitHub Repo, or Book. Like-minded people with related interests may find your blog post, network with you, and give you feedback. Many opportunities may come in your future career from the network you build. An example of this was when David Robinson answered a question on Stack Overflow10, Stack Overflow engineer saw the brilliant answer and hired him (his first job at Stack Overflow). So, a blog allows one to showcase his skills, and other people can benefit from it.\n\n\n\nTweet transformed to a blog and book."
  },
  {
    "objectID": "blog/whyblog/whyblog.html#learning-by-teaching-protégé-effect",
    "href": "blog/whyblog/whyblog.html#learning-by-teaching-protégé-effect",
    "title": "Why you should start a blog?",
    "section": "Learning by teaching (protégé effect)",
    "text": "Learning by teaching (protégé effect)\nRobert Heinlein said, when one teaches, two learn. It means whenever you teach or explain a concept to someone, you will learn something from it or get a better insight and ultimately reach the “aha” moment. Therefore, writing a good tutorial about a brain-bending concept without dumbing it down is a great way to learn and increase visibility. As Einstein says, “If you can’t explain it simply, you don’t understand it well enough.” A study11 found that when students teach the lesson’s content (active learning), they develop a more in-depth and longer-lasting understanding of the material than students who do not teach it( passive learning). Therefore, this blog will allow me to write my research and summary of papers and man more. The approach of learning by teaching was widely known as Feynman learning technique12"
  },
  {
    "objectID": "blog/whyblog/whyblog.html#share-my-experience-and-opportunites",
    "href": "blog/whyblog/whyblog.html#share-my-experience-and-opportunites",
    "title": "Why you should start a blog?",
    "section": "Share my experience and opportunites:",
    "text": "Share my experience and opportunites:\nI naturally love to share my experience and other opportunities with people I know. Therefore, a blog post will serve as a way to share important resources that I come across and find useful. This will benefit a wider audience."
  },
  {
    "objectID": "blog/tensor/tensor.html",
    "href": "blog/tensor/tensor.html",
    "title": "Pytorch Tensor 101",
    "section": "",
    "text": "PyTorch is a Python-based open source and scientific computing package for building neural networks. It is dynamic graph-based framework that allows you to define your neural network in a way that is easy to understand and debug. Today, PyTorch is the most used deep learning framework and mostly use by researchers and engineers.\n\nPyTorch support GPU acceleration (making your code run faster) behind the scenes, better than NumPy. PyTorch also provides Autograd for automatic differentiation, which means that your code is automatically differentiated and you can use it to do backpropagation"
  },
  {
    "objectID": "blog/tensor/tensor.html#pytorch",
    "href": "blog/tensor/tensor.html#pytorch",
    "title": "Pytorch Tensor 101",
    "section": "",
    "text": "PyTorch is a Python-based open source and scientific computing package for building neural networks. It is dynamic graph-based framework that allows you to define your neural network in a way that is easy to understand and debug. Today, PyTorch is the most used deep learning framework and mostly use by researchers and engineers.\n\nPyTorch support GPU acceleration (making your code run faster) behind the scenes, better than NumPy. PyTorch also provides Autograd for automatic differentiation, which means that your code is automatically differentiated and you can use it to do backpropagation"
  },
  {
    "objectID": "blog/tensor/tensor.html#pytoch-installation",
    "href": "blog/tensor/tensor.html#pytoch-installation",
    "title": "Pytorch Tensor 101",
    "section": "Pytoch Installation",
    "text": "Pytoch Installation\nBefore you installed Pytorch, you need to install the following dependencies: Package Manager (e.g. pip, conda), Python, Numpy. For more information, please refer to the Pytorch documentation.\nFor me, I am using mac and conda as package manager, I therefore run the following command"
  },
  {
    "objectID": "blog/tensor/tensor.html#verification",
    "href": "blog/tensor/tensor.html#verification",
    "title": "Pytorch Tensor 101",
    "section": "VERIFICATION",
    "text": "VERIFICATION\nTo verify your installation works,\n\nimport torch\ntorch.manual_seed(1234)\ntorch.__version__\n\n'1.13.0.dev20220611'"
  },
  {
    "objectID": "blog/tensor/tensor.html#what-is-tensor",
    "href": "blog/tensor/tensor.html#what-is-tensor",
    "title": "Pytorch Tensor 101",
    "section": "What is Tensor",
    "text": "What is Tensor\n\nAssume we have 3 bedrooms, 1 carpark and 2 bathrooms. We can represent this data numerically in a form of vector [3, 1,2] to describe bedrooms, carpark and bathrooms\nTensor are the standard way of representing data in Pytorch, such as text, images, and audio. Their job is to represent data in a numerical way."
  },
  {
    "objectID": "blog/tensor/tensor.html#is-tensor-all-you-need",
    "href": "blog/tensor/tensor.html#is-tensor-all-you-need",
    "title": "Pytorch Tensor 101",
    "section": "is Tensor all you need?",
    "text": "is Tensor all you need?\n\nThere are many Python Data Structure for holding data including Python List and Numpy Array. List and Numpy Array operations are similar to Pytorch Tensor.\nLet us remember the basic of data structures in Python (List and Numpy Array) before we start using Pytorch Tensor"
  },
  {
    "objectID": "blog/tensor/tensor.html#from-python-lists-to-numpy-array",
    "href": "blog/tensor/tensor.html#from-python-lists-to-numpy-array",
    "title": "Pytorch Tensor 101",
    "section": "From Python lists to Numpy Array",
    "text": "From Python lists to Numpy Array\n\nPython does not have built-in support for Arrays, but Python Lists can be used instead.\nUsing our previous example, we can create a list of Python lists below.\n\n\na_list = [3, 1,2] #A list is the Python equivalent of an array\n\nprint(a_list) # print the list\nprint((type(a_list))) # print the type\nprint(a_list[0]) # subset the list\n\n[3, 1, 2]\n&lt;class 'list'&gt;\n3\n\n\n\nHowever, Python lists has the following limitations: It takes large memory size and slow.\n\n\nNumpy solved the problems with List:\n\nSize - Numpy data structures take up less space\nPerformance - they have a need for speed and are faster than lists\nFunctionality - SciPy and NumPy have optimized functions such as linear algebra operations built in.\n\n\n\nimport numpy as np\na_numpy = np.array([1,3,4]) # creating a numpy array\na_numpy\n\narray([1, 3, 4])\n\n\n\ntype(a_numpy) # nd arrays\n\nnumpy.ndarray\n\n\n\na_numpy[0] # we can subset similar to Python list\n\n1\n\n\n\na_numpy.shape # shape of the nd array\n\n(3,)\n\n\n\na_numpy.dtype # dtype of the nd array\n\ndtype('int64')\n\n\n\na_numpy.size # size of the nd array\n\n3\n\n\n\nPerformance comparison between Python lists and Numpy Arrays\n\nimport numpy as np\nimport time\n\n\nsize_of_vec = 1000\n\ndef pure_python_version():\n    t1 = time.time()\n    X = range(size_of_vec)\n    Y = range(size_of_vec)\n    Z = [X[i] + Y[i] for i in range(len(X)) ]\n    return time.time() - t1\n\ndef numpy_version():\n    t1 = time.time()\n    X = np.arange(size_of_vec)\n    Y = np.arange(size_of_vec)\n    Z = X + Y\n    return time.time() - t1\n\n\nt1 = pure_python_version()\nt2 = numpy_version()\nprint(t1, t2)\nprint(\"Numpy is in this example \" + str(t1/t2) + \" faster!\")\n\n0.00019288063049316406 0.0005578994750976562\nNumpy is in this example 0.3457264957264957 faster!"
  },
  {
    "objectID": "blog/tensor/tensor.html#from-numpy-arrays-to-torch-tensor",
    "href": "blog/tensor/tensor.html#from-numpy-arrays-to-torch-tensor",
    "title": "Pytorch Tensor 101",
    "section": "From Numpy Arrays to Torch Tensor",
    "text": "From Numpy Arrays to Torch Tensor\n\nTensors are like arrays, both are data structures that are used to store data. Tensor and Numpy arrays share common operations such as shape and size.\n\n\nTensors are generalization of vectors and matrices to an arbitrary number of dimensions.\n\n\n\nSimilar to how Numpy provides additional support not available in the Python list, so also Tensors provides support not available in Numpy array such as:\n\nGPU acceleration , which is a great advantage for deep learning,\ndistribute operations on multiple devices or machines,and\nkeep track of the graph of computations that created them ( usefull for backpropagation)."
  },
  {
    "objectID": "blog/tensor/tensor.html#let-us-learn-tensor",
    "href": "blog/tensor/tensor.html#let-us-learn-tensor",
    "title": "Pytorch Tensor 101",
    "section": "Let us Learn Tensor",
    "text": "Let us Learn Tensor\nVarious operations are available on tensors. In the next sections, we will discuss the following operations:\n\nCreating tensors.\nOperations with tensors.\nIndexing, slicing, and joining with tensors Computing gradients with tensors.\nUsing CUDA/MPS tensors with GPUs."
  },
  {
    "objectID": "blog/tensor/tensor.html#creating-tensors",
    "href": "blog/tensor/tensor.html#creating-tensors",
    "title": "Pytorch Tensor 101",
    "section": "Creating tensors",
    "text": "Creating tensors\n\nPyTorch allows us to create tensors in many different ways using the torch package. We will discuss some of these ways.\n\n\nCreating Random Tensor with a specific size\n\ntorch.tensor is a general Tensor constructor that infer the data type automatically.\n\n\nimport torch\n\na_random = torch.tensor((3,4)) # Create a random tensor\nprint(a_random)\n\ntensor([3, 4])\n\n\n\nprint(a_random.shape) # print the shape of the random tensor\nprint(a_random.size()) # print the size of the random tensor\nprint(type(a_random)) # print the type of the random tensor\nprint(a_random.type()) # print the type of the random tens\n\ntorch.Size([2])\ntorch.Size([2])\n&lt;class 'torch.Tensor'&gt;\ntorch.LongTensor\n\n\n\nNote: .shape is an alias for .size(), and was added to closely match numpy !\n\n\nIntead of allowing the torch.tensor to automatically determine the data type, you can explicitly specify the type of the data type by using the torch.type parameter\n\n\nimport torch\n\na_random = torch.tensor((3,4), dtype= torch.float) # Create a random tensor\nprint(a_random)\n\ntensor([3., 4.])\n\n\n\nprint(a_random.shape) # print the shape of the random tensor\nprint(a_random.size()) # print the size of the random tensor\nprint(type(a_random)) # print the type of the random tensor\nprint(a_random.type())\n\ntorch.Size([2])\ntorch.Size([2])\n&lt;class 'torch.Tensor'&gt;\ntorch.FloatTensor\n\n\n\nYou can also change an existing tensor type by using the\n\n\na_torch = torch.tensor([1, 2, 3]) \n\nprint(a_torch.type()) # Tensor type\n\ntorch.LongTensor\n\n\nWe can change from LongTensor t:\n\na_short =  a_torch.short() # Convert to short,  \na_float =  a_torch.float() # Convert to float()\n\nprint(a_short.type()) # Tensor type\nprint(a_float.type()) # Tensor type\n\ntorch.ShortTensor\ntorch.FloatTensor\n\n\n\nNote: A variant of torch.tensor constructor is torch.FloatTensorconstructor. When use, the default tensor type is FloatTensor. Infact, torch.Tensor is an alias for the torch.FloatTensor constructor.\n\n\nThe following two examples are equivalent:\n\n\na_random = torch.Tensor((3,4)) # Create a random tensor\nb_random = torch.FloatTensor((3,4)) # Create a random tensor\n\nprint(a_random.type())\nprint(b_random.type())\n\ntorch.FloatTensor\ntorch.FloatTensor\n\n\n\nI would recommend to stick to torch.tensor, if you would like to change the type, you can change\n\nTorch defines 10 tensor types with CPU and GPU variants: See different Pytorch Data Types:\n\nThe most common type (and generally the default) is torch.float32 or torch.float. This is referred to as “32-bit floating point”.\nBut there’s also 16-bit floating point (torch.float16 or torch.half) and 64-bit floating point (torch.float64 or torch.double).\nThe reason for all of these is to do with precision in computing. Precision is the amount of detail used to describe a number.\nThe higher the precision value (8, 16, 32), the more detail and hence data used to express a number.\nThis matters in deep learning and numerical computing because you’re making so many operations, the more detail you have to calculate on, the more compute you have to use.\n\n\nSo, lower precision datatypes are generally faster to compute on but sacrifice some performance on evaluation metrics like accuracy (faster to compute but less accurate).\n\n\n\n2: Creating Tensors from Random Numbers\nSimilar to the numpy, we can create a tensor from a random number.\n\na_random_torch = torch.randn(2, 3) # uniform random distribution numbers between 0 and 1\n# a_numpy_rand = np.random.randn(2,3) #numpy random normal distribution\n\nprint(a_random_torch)\n# print(a_numpy_rand)\n\ntensor([[ 0.0461,  0.4024, -1.0115],\n        [ 0.2167, -0.6123,  0.5036]])\n\n\n\na_random_torch = torch.rand(2, 3) # random normal distribution\n# a_numpy_rand = np.random.rand(2,3) \n\nprint(a_random_torch)\n# print(a_numpy_rand)\n\ntensor([[0.7749, 0.8208, 0.2793],\n        [0.6817, 0.2837, 0.6567]])\n\n\n\n\n3: Creating a filled tensor\n\na_same_scalar = torch.zeros(3,3)\nprint(a_same_scalar)\nprint(a_same_scalar.size())\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\ntorch.Size([3, 3])\n\n\n\ntorch.ones(3, 3) # torch.ones(size=(3, 3)) \n\ntensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]])\n\n\n\nAny PyTorch method with an underscore (_) refers to an in­place operation;\n\n\na_zero = torch.zeros(2, 3)\nprint(a_zero)\nprint(a_zero.fill_(5)) # inplace operation\nprint(a_zero)  # a_zero is now filled with 5\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\ntensor([[5., 5., 5.],\n        [5., 5., 5.]])\ntensor([[5., 5., 5.],\n        [5., 5., 5.]])\n\n\n###4: Creating and initializing a tensor from lists\n\na_list = torch.tensor([1, 2, 3])\na_list\n\ntensor([1, 2, 3])\n\n\n\n\n5: Creating and initializing a tensor from numpy arrays\n\nWe use torch.from_numpy to create a tensor from a numpy array.\n\n\nimport numpy as np\nnumpy_array = np.random.rand(2, 3) \nnumpy_array\n\ntorch_tensor = torch.from_numpy(numpy_array) # tensor from numpy array\ntorch_tensor\n\ntensor([[0.3487, 0.9072, 0.8480],\n        [0.7245, 0.6970, 0.4976]], dtype=torch.float64)\n\n\n\ntorch_tensor.type()\n\n'torch.DoubleTensor'\n\n\n\nThe datatype after creating of tensor from numpy array is DoubleTensor instead of the default FloatTensor. This corresponds with the data type of the NumPy random matrix, a float64,\n\n\nYou can always convert from PyTorch tensors to Numpy arrays using the numpy function torch.numpy().\n\n\ntorch_tensor.numpy()\n\narray([[0.3487288 , 0.90720583, 0.84795941],\n       [0.72447844, 0.69699952, 0.49759155]])\n\n\n\n\n6: Creating a range and tensors like\n\n# Use torch.arange(), torch.range() is deprecated \nzero_to_ten = torch.arange(0, 10) \nzero_to_ten\n\ntensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\n\nCreating tensor of type with the same shape as another tensor.\n\n# Can also create a tensor of zeros similar to another tensor\nten_zeros = torch.zeros_like(input=zero_to_ten) # will have same shape\nten_zeros\n\ntensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\n\n\nCreating Named Tensors\n\nNamed Tensors allow users to give explicit names to tensor dimensions.\nIn most cases, operations that take dimension parameters will accept dimension names, avoiding the need to track dimensions by position.\n\n\ntorch.zeros(2, 3, names=('N', 'C'))\n\n/var/folders/1h/b7ng0kgj3w78mg7n8k7q7rch0000gn/T/ipykernel_11570/697701580.py:1: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1654931446436/work/c10/core/TensorImpl.h:1489.)\n  torch.zeros(2, 3, names=('N', 'C'))\n\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.]], names=('N', 'C'))\n\n\n\nWe can use names to access tensor dimensions.\n\n\nimgs = torch.randn(1, 2, 2, 3 , names=('N', 'C', 'H', 'W')) \nimgs.names\n\n('N', 'C', 'H', 'W')\n\n\n\nimgs.names[0]\n\n'N'"
  },
  {
    "objectID": "blog/tensor/tensor.html#tensor-properties",
    "href": "blog/tensor/tensor.html#tensor-properties",
    "title": "Pytorch Tensor 101",
    "section": "Tensor properties",
    "text": "Tensor properties\nTensor has many properties including the following properties: the number of dimensions, the size, the type:\n\nTensor Dimensions\nWe can find the tensor dimensions using:ndim\n\n# Scalar\nscalar = torch.tensor(7)\nscalar\n\nscalar.ndim\n\n0\n\n\n\nMATRIX = torch.tensor([[1,2,3,4],\n                       [5,6,7,8]])\n\nMATRIX.ndim\n\n2\n\n\n\nYou can tell the number of dimensions a tensor in PyTorch has by the number of square brackets on the outside ([) and you only need to count one side of the brackets.\n\nIn practice, you’ll often see scalars and vectors denoted as lowercase letters such as y or a. And matrices and tensors denoted as uppercase letters such as X or W"
  },
  {
    "objectID": "blog/tensor/tensor.html#manipulating-tensors-tensor-operations",
    "href": "blog/tensor/tensor.html#manipulating-tensors-tensor-operations",
    "title": "Pytorch Tensor 101",
    "section": "Manipulating tensors (tensor operations)",
    "text": "Manipulating tensors (tensor operations)\n\nIn deep learning, data (images, text, video, audio, protein structures, etc) gets represented as tensors.\nA model learns by investigating those tensors and performing a series of operations (could be 1,000,000s+) on tensors to create a representation of the patterns in the input data.\nAfter you have created your tensors, you can operate on them like you would do with traditional programming language types, like +, ­, *, /.\n\n\nIndexing tensors\nIndexing and subsetting a tensor is similar to indexing a list.\n\nsome_list = list(range(6))\ntorch_list = torch.tensor(some_list)\ntorch_list\n\ntensor([0, 1, 2, 3, 4, 5])\n\n\n\nprint(torch_list[0]) # first element of the tensor\nprint(torch_list[1]) # second element of the tensor\n\ntensor(0)\ntensor(1)\n\n\n\ntorch_list[1:4] # subsetting a tensor\n\ntensor([1, 2, 3])\n\n\n\n\nTransposing Tensors\nTransposing 2D tensors is a simple operation using t\n\npoints = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\npoints\n\ntensor([[4., 1.],\n        [5., 3.],\n        [2., 1.]])\n\n\n\npoints_t = points.t()\npoints_t\n\ntensor([[4., 5., 2.],\n        [1., 3., 1.]])\n\n\nYou can also transpose 3D and higher tensors using the transpose method by specifying the two dimensions along which transposing (flipping shape and stride) should occur:\n\nsome_t = torch.ones(3, 4, 5)\ntranspose_t = some_t.transpose(0, 2)\nprint(some_t.shape)\nprint(transpose_t.shape)\n\ntorch.Size([3, 4, 5])\ntorch.Size([5, 4, 3])\n\n\n\n\nTensor View Operation\nTensor view operations returns a new tensor with the same data as the self tensor but of a different shape.\n\nx = torch.randn(2, 2)\nprint(x)\nprint(x.size())\n\ntensor([[-0.4790,  0.8539],\n        [-0.2285,  0.3081]])\ntorch.Size([2, 2])\n\n\n\ny = x.view(4)\nprint(y)\nprint(y.size())\n\ntensor([-0.4790,  0.8539, -0.2285,  0.3081])\ntorch.Size([4])\n\n\n\nUsing -1 in the shape argument will automatically infer the correct size of the dimension.\n\n\nz = x.view(-1, 2)  # the size -1 is inferred from other dimensions\n\nprint(z)\nprint(z.size())\n\ntensor([[-0.4790,  0.8539],\n        [-0.2285,  0.3081]])\ntorch.Size([2, 2])\n\n\n\nView Does not change tensor layout in memory, Transpose() operation change the tensor layout in memory.\n\n\n\nTensor Mathematical Basic Operations\nTensor addition is achive using torch.add as shown in the following example:\n\n# Create a tensor of values and add a number to it\ntensor = torch.tensor([1, 2, 3])\ntensor + 10\n\ntensor([11, 12, 13])\n\n\n\n# Multiply it by 10\ntensor * 10\n\ntensor([10, 20, 30])\n\n\n\n# Subtract and reassign\ntensor = tensor - 10\ntensor\n\ntensor([-9, -8, -7])\n\n\n\nPyTorch also has a bunch of built-in functions like torch.mul() (short for multiplcation) and torch.add() to perform basic operations.\n\n\n# Can also use torch functions\ntensor = torch.tensor([1, 2, 3])\ntorch.multiply(tensor, 10)  # multiply by 10\n\ntensor([10, 20, 30])\n\n\n\ntensor = torch.tensor([1, 2, 3])\n\ntorch.add(tensor, 20) # add by 20\n\ntensor([21, 22, 23])\n\n\n\ntorch.div(tensor, 20, rounding_mode='trunc') # divide by 20, with truncation as a rounding_mode\n\ntensor([0, 0, 0])\n\n\n\ntorch.div(tensor, 20, rounding_mode='floor') # divide by 20, with floor as a rounding_mode\n\ntensor([0, 0, 0])\n\n\n\ntorch.sum(tensor) # sum tensor entries  [1, 2, 3]\n\ntensor(6)"
  },
  {
    "objectID": "blog/tensor/tensor.html#matrix-multiplication-is-all-you-need",
    "href": "blog/tensor/tensor.html#matrix-multiplication-is-all-you-need",
    "title": "Pytorch Tensor 101",
    "section": "Matrix multiplication is all you need",
    "text": "Matrix multiplication is all you need\n\nIn deep learning algorithms (like neural networks), one of the most common operations is matrix multiplication.\nPyTorch implements matrix multiplication functionality in the torch.matmul() method.\nThe main two rules for matrix multiplication to remember are:\n\nThe inner dimensions must match:\n\n(3, 2) @ (3, 2) won’t work\n(2, 3) @ (3, 2) will work\n(3, 2) @ (2, 3) will work\n\nThe resulting matrix has the shape of the outer dimensions:\n\n(2, 3) @ (3, 2) -&gt; (2, 2)\n(3, 2) @ (2, 3) -&gt; (3, 3)\n\nNote: “@” in Python is the symbol for matrix multiplication.\nMore information about matrix multiplication can be found in the Matrix Multiplication section.\n\ntensor1 = torch.randn(3, 4)\ntensor2 = torch.randn(4)\n\nprint(tensor1.shape)\nprint(tensor2.shape)\n\ntorch.Size([3, 4])\ntorch.Size([4])\n\n\n\nresult = torch.matmul(tensor1, tensor2)\nresult.shape\n\ntorch.Size([3])\n\n\nNote: The difference between element-wise multiplication (multiply) and matrix multiplication (matmul) is the addition of values.\n\nmatmul: matrix multiplication\nmultiply: element-wise multiplication\n\n\ntensor = torch.tensor([1, 2, 3])\ntensor.shape\n\ntorch.Size([3])\n\n\nElement-wise matrix mutlication\n\ntensor * tensor\n\ntensor([1, 4, 9])\n\n\nMatrix multiplication\n\ntorch.matmul(tensor, tensor)\n\ntensor(14)\n\n\nCan also use the “@” symbol or torch.mm() for matrix multiplication, though not recommended\n\nprint(tensor @ tensor)\nprint(tensor.matmul(tensor))\n\ntensor(14)\ntensor(14)\n\n\n\nA matrix multiplication like this is also referred to as the dot product of two matrices. Neural networks are full of matrix multiplications and dot products.\n\nFor example, torch.nn.Linear() module (we’ll see this in action later on), also known as a feed-forward layer or fully connected layer, implements a matrix multiplication between an input x and a weights matrix A.\n\\[\ny = x\\cdot{A^T} + b\n\\]\nThank you for reading !"
  },
  {
    "objectID": "dailyR/dailyR0003/daily0001.html",
    "href": "dailyR/dailyR0003/daily0001.html",
    "title": "R4DS Chapter 2 Exercises",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6          ✔ purrr   0.3.5     \n✔ tibble  3.1.8          ✔ dplyr   1.0.10    \n✔ tidyr   1.2.1          ✔ stringr 1.4.1.9000\n✔ readr   2.1.3          ✔ forcats 0.5.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\nRun ggplot(data = mpg). What do you see?\n\n\nggplot(data = mpg)\n\n\n\n\n\nThis creates the background of the plot, but since no layers were specified with geom function, nothing is drawn.\n\nI see empty.\n\nHow many rows are in mpg? How many columns?\n\n\nnrow(mpg)\n\n[1] 234\n\n\nThere are 234 rows.\nWe can also use glimpes() to find number of rows:\n\nglimpse(mpg)\n\nRows: 234\nColumns: 11\n$ manufacturer &lt;chr&gt; \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"…\n$ model        &lt;chr&gt; \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 quattro\", \"…\n$ displ        &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0, 2.…\n$ year         &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, 200…\n$ cyl          &lt;int&gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8, …\n$ trans        &lt;chr&gt; \"auto(l5)\", \"manual(m5)\", \"manual(m6)\", \"auto(av)\", \"auto…\n$ drv          &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"4\", \"4\", \"4\", \"4\", \"4…\n$ cty          &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 1…\n$ hwy          &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25, 2…\n$ fl           &lt;chr&gt; \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p…\n$ class        &lt;chr&gt; \"compact\", \"compact\", \"compact\", \"compact\", \"compact\", \"c…\n\n\n\nWhat does the drv variable describe? Read the help for ?mpg to find out.\n\n\n?mpg\n\n\ndv means: “the type of drive train, where f = front-wheel drive, r = rear wheel drive, 4 = 4wd”\n\n\nMake a scatterplot of hwy vs cyl.\n\n\nggplot(data = mpg)+\ngeom_point(aes(hwy,cyl))\n\n\n\n\n\nWhat happens if you make a scatterplot of class vs drv? Why is the plot not useful?\n\n\nggplot(data = mpg)+\ngeom_point(aes(class,drv))\n\n\n\n\n\nA scatter plot is not a useful display of these variables since both drv and class are categorical variables. A scatterplots work best for plotting a continuous x and a continuous y variable, and when all (x, y) values are unique.\n\nHowever, there are other methods for plotting categorical variables: geom_count() and geom_tile()\n\nggplot(mpg, aes(x = class, y = drv)) +\n  geom_count()\n\n\n\n\n\nmpg %&gt;%\n  count(class, drv) %&gt;%\n  ggplot(aes(x = class, y = drv)) +\n    geom_tile(mapping = aes(fill = n))\n\n\n\n\n\nIn the previous plot, there are many missing tiles. These missing tiles represent unobserved combinations of class and drv values. These missing values are not unknown, but represent values of (class, drv) where n = 0. The complete() function in the tidyr package adds new rows to a data frame for missing combinations of columns.\n\n\nmpg %&gt;%\n  count(class, drv) %&gt;%\n  complete(class, drv, fill = list(n = 0)) %&gt;%\n  ggplot(aes(x = class, y = drv)) +\n    geom_tile(mapping = aes(fill = n))"
  },
  {
    "objectID": "dailyR/dailyR0003/daily0001.html#exercises",
    "href": "dailyR/dailyR0003/daily0001.html#exercises",
    "title": "R4DS Chapter 2 Exercises",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6          ✔ purrr   0.3.5     \n✔ tibble  3.1.8          ✔ dplyr   1.0.10    \n✔ tidyr   1.2.1          ✔ stringr 1.4.1.9000\n✔ readr   2.1.3          ✔ forcats 0.5.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\nRun ggplot(data = mpg). What do you see?\n\n\nggplot(data = mpg)\n\n\n\n\n\nThis creates the background of the plot, but since no layers were specified with geom function, nothing is drawn.\n\nI see empty.\n\nHow many rows are in mpg? How many columns?\n\n\nnrow(mpg)\n\n[1] 234\n\n\nThere are 234 rows.\nWe can also use glimpes() to find number of rows:\n\nglimpse(mpg)\n\nRows: 234\nColumns: 11\n$ manufacturer &lt;chr&gt; \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"…\n$ model        &lt;chr&gt; \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 quattro\", \"…\n$ displ        &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0, 2.…\n$ year         &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, 200…\n$ cyl          &lt;int&gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8, …\n$ trans        &lt;chr&gt; \"auto(l5)\", \"manual(m5)\", \"manual(m6)\", \"auto(av)\", \"auto…\n$ drv          &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"4\", \"4\", \"4\", \"4\", \"4…\n$ cty          &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 1…\n$ hwy          &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25, 2…\n$ fl           &lt;chr&gt; \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p…\n$ class        &lt;chr&gt; \"compact\", \"compact\", \"compact\", \"compact\", \"compact\", \"c…\n\n\n\nWhat does the drv variable describe? Read the help for ?mpg to find out.\n\n\n?mpg\n\n\ndv means: “the type of drive train, where f = front-wheel drive, r = rear wheel drive, 4 = 4wd”\n\n\nMake a scatterplot of hwy vs cyl.\n\n\nggplot(data = mpg)+\ngeom_point(aes(hwy,cyl))\n\n\n\n\n\nWhat happens if you make a scatterplot of class vs drv? Why is the plot not useful?\n\n\nggplot(data = mpg)+\ngeom_point(aes(class,drv))\n\n\n\n\n\nA scatter plot is not a useful display of these variables since both drv and class are categorical variables. A scatterplots work best for plotting a continuous x and a continuous y variable, and when all (x, y) values are unique.\n\nHowever, there are other methods for plotting categorical variables: geom_count() and geom_tile()\n\nggplot(mpg, aes(x = class, y = drv)) +\n  geom_count()\n\n\n\n\n\nmpg %&gt;%\n  count(class, drv) %&gt;%\n  ggplot(aes(x = class, y = drv)) +\n    geom_tile(mapping = aes(fill = n))\n\n\n\n\n\nIn the previous plot, there are many missing tiles. These missing tiles represent unobserved combinations of class and drv values. These missing values are not unknown, but represent values of (class, drv) where n = 0. The complete() function in the tidyr package adds new rows to a data frame for missing combinations of columns.\n\n\nmpg %&gt;%\n  count(class, drv) %&gt;%\n  complete(class, drv, fill = list(n = 0)) %&gt;%\n  ggplot(aes(x = class, y = drv)) +\n    geom_tile(mapping = aes(fill = n))"
  },
  {
    "objectID": "dailyR/dailyR0003/daily0001.html#exercises-1",
    "href": "dailyR/dailyR0003/daily0001.html#exercises-1",
    "title": "R4DS Chapter 2 Exercises",
    "section": "2.3.1 Exercises",
    "text": "2.3.1 Exercises\n\nWhat’s gone wrong with this code? Why are the points not blue?\n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, color = \"blue\"))\n\n\n\n\n\nThe argument colour = “blue” is included within the mapping argument, and as such, it is treated as an aesthetic, which is a mapping between a variable and a value\n\nBelow is the correct format:\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy), color = \"blue\")\n\n\n\n\nand it is blow."
  },
  {
    "objectID": "til/dailyml0011/0003.html",
    "href": "til/dailyml0011/0003.html",
    "title": "KNN’s runtime",
    "section": "",
    "text": "Evangeline is working with a dataset with a single column of data.\nShe wants to run k-Nearest Neighbors (KNN), but only if the algorithm is fast enough when making predictions.\nAssuming there are n samples in the dataset, which of the following will be the runtime of Evangeline’s KNN at prediction time?\n\nThe runtime that Evangeline should expect is O(1)\nThe runtime that Evangeline should expect is O(n)\nThe runtime that Evangeline should expect is O(log n)\nThe runtime that Evangeline should expect is O(n²)"
  },
  {
    "objectID": "til/dailyml0011/0003.html#knns-runtime",
    "href": "til/dailyml0011/0003.html#knns-runtime",
    "title": "KNN’s runtime",
    "section": "",
    "text": "Evangeline is working with a dataset with a single column of data.\nShe wants to run k-Nearest Neighbors (KNN), but only if the algorithm is fast enough when making predictions.\nAssuming there are n samples in the dataset, which of the following will be the runtime of Evangeline’s KNN at prediction time?\n\nThe runtime that Evangeline should expect is O(1)\nThe runtime that Evangeline should expect is O(n)\nThe runtime that Evangeline should expect is O(log n)\nThe runtime that Evangeline should expect is O(n²)"
  },
  {
    "objectID": "til/dailyml0011/0003.html#attempt",
    "href": "til/dailyml0011/0003.html#attempt",
    "title": "KNN’s runtime",
    "section": "Attempt",
    "text": "Attempt\nk-Nearest Neighbors’ runtime is O(nd) because we need to compute the distance to each feature of every sample. Here n represents the number of instances, and d the number of features.\nEvangeline is working with a single feature, so d = 1. Therefore, in this case, the runtime of KNN is O(n)."
  },
  {
    "objectID": "til/dailyml0011/0003.html#recommended-reading",
    "href": "til/dailyml0011/0003.html#recommended-reading",
    "title": "KNN’s runtime",
    "section": "Recommended reading",
    "text": "Recommended reading\nHere is a Stack Exchange answer that covers KNN’s runtime complexity in detail."
  },
  {
    "objectID": "til/dailyml0010/0003.html",
    "href": "til/dailyml0010/0003.html",
    "title": "Effect of Batch Size on Training Time",
    "section": "",
    "text": "Lena ran two experiments training a neural network on a sample dataset. Her goal is to understand how different batch sizes affect the training process.\nOne of the experiments used a very small batch size, and the other used a batch size equal to all existing training data.\nAfter training and evaluating her models, Lena plotted the training and testing losses over 100 epochs of one of the experiments, and this is what she got:"
  },
  {
    "objectID": "til/dailyml0010/0003.html#effect-of-batch-size",
    "href": "til/dailyml0010/0003.html#effect-of-batch-size",
    "title": "Effect of Batch Size on Training Time",
    "section": "",
    "text": "Lena ran two experiments training a neural network on a sample dataset. Her goal is to understand how different batch sizes affect the training process.\nOne of the experiments used a very small batch size, and the other used a batch size equal to all existing training data.\nAfter training and evaluating her models, Lena plotted the training and testing losses over 100 epochs of one of the experiments, and this is what she got:"
  },
  {
    "objectID": "til/dailyml0010/0003.html#attempt",
    "href": "til/dailyml0010/0003.html#attempt",
    "title": "Effect of Batch Size on Training Time",
    "section": "Attempt",
    "text": "Attempt"
  },
  {
    "objectID": "til/dailyml0010/0003.html#which-of-the-following-options-is-the-most-likely-to-be-true",
    "href": "til/dailyml0010/0003.html#which-of-the-following-options-is-the-most-likely-to-be-true",
    "title": "Effect of Batch Size on Training Time",
    "section": "Which of the following options is the most likely to be true?",
    "text": "Which of the following options is the most likely to be true?\n\nThe model behind this plot corresponds to the experiment that used a very small batch size. This model was the one that took less time to train.\nThe model behind this plot corresponds to the experiment that used a very small batch size. This model was the one that took longer to train.\nThe model behind this plot corresponds to the experiment that used a batch size equal to all the available training data. This model was the one that took less time to train.\nThe model behind this plot corresponds to the experiment that used a batch size equal to all the available training data. This model was the one that took longer to train."
  },
  {
    "objectID": "til/dailyml0010/0003.html#recommended-reading",
    "href": "til/dailyml0010/0003.html#recommended-reading",
    "title": "Effect of Batch Size on Training Time",
    "section": "Recommended reading",
    "text": "Recommended reading\n\nCheck “What is the Curse of Dimensionality?” for an introduction to the Curse of Dimensionality.\nFor an explanation of data leakages, check “Data Leakage in Machine Learning”.\n“Introduction to Dimensionality Reduction for Machine Learning” covers different examples of dimensionality reduction."
  },
  {
    "objectID": "til/dailyml0004/0003.html",
    "href": "til/dailyml0004/0003.html",
    "title": "Regularizing a model",
    "section": "",
    "text": "Charlotte’s machine learning model is overfitting.\nShe needs to find a way to handle it, but before trying anything, she wants to understand her options.\nWhich of the following are regularization techniques that Charlotte could consider?\n\nValidation-based early stopping\nDropout\nData augmentation\nCross-validation"
  },
  {
    "objectID": "til/dailyml0004/0003.html#regularizing-a-model",
    "href": "til/dailyml0004/0003.html#regularizing-a-model",
    "title": "Regularizing a model",
    "section": "",
    "text": "Charlotte’s machine learning model is overfitting.\nShe needs to find a way to handle it, but before trying anything, she wants to understand her options.\nWhich of the following are regularization techniques that Charlotte could consider?\n\nValidation-based early stopping\nDropout\nData augmentation\nCross-validation"
  },
  {
    "objectID": "til/dailyml0004/0003.html#attempt",
    "href": "til/dailyml0004/0003.html#attempt",
    "title": "Regularizing a model",
    "section": "Attempt",
    "text": "Attempt\nValidation-based early stopping is a regularization technique that stops the training process as soon as the generalization error of the model increases. In other words, if the model’s performance on the validation set starts degrading, the training process stops.\nDropout is a regularization method that works well and is vital for reducing overfitting. Dropout randomly removes a percentage of the nodes, forcing the network to learn in a balanced way, and tackling a phenomenon that we call “co-adaptation.”\nData augmentation has a regularization effect. Increasing the training data through data augmentation decreases the model’s variance and, in turn, increases the model’s generalization ability.\nFinally, cross-validation is a validation scheme and not a regularization method."
  },
  {
    "objectID": "til/dailyml0004/0003.html#recommended-reading",
    "href": "til/dailyml0004/0003.html#recommended-reading",
    "title": "Regularizing a model",
    "section": "Recommended reading",
    "text": "Recommended reading\n\nCheck “Early Stopping” for an introduction to one of the most effective, easy-to-implement regularization techniques when training machine learning models.\n“A Gentle Introduction to Dropout for Regularizing Deep Neural Networks” is an excellent introduction to Dropout.\n“The Essential Guide to Data Augmentation in Deep Learning” is an excellent article discussing data augmentation in detail."
  },
  {
    "objectID": "til/dailyR0003/daily0001.html",
    "href": "til/dailyR0003/daily0001.html",
    "title": "R4DS Chapter 2 Exercises",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0          ✔ purrr   0.3.5     \n✔ tibble  3.1.8          ✔ dplyr   1.0.10    \n✔ tidyr   1.2.1.9001     ✔ stringr 1.5.0     \n✔ readr   2.1.3          ✔ forcats 0.5.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\nRun ggplot(data = mpg). What do you see?\n\n\nggplot(data = mpg)\n\n\n\n\n\nThis creates the background of the plot, but since no layers were specified with geom function, nothing is drawn.\n\nI see empty.\n\nHow many rows are in mpg? How many columns?\n\n\nnrow(mpg)\n\n[1] 234\n\n\nThere are 234 rows.\nWe can also use glimpes() to find number of rows:\n\nglimpse(mpg)\n\nRows: 234\nColumns: 11\n$ manufacturer &lt;chr&gt; \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"…\n$ model        &lt;chr&gt; \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 quattro\", \"…\n$ displ        &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0, 2.…\n$ year         &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, 200…\n$ cyl          &lt;int&gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8, …\n$ trans        &lt;chr&gt; \"auto(l5)\", \"manual(m5)\", \"manual(m6)\", \"auto(av)\", \"auto…\n$ drv          &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"4\", \"4\", \"4\", \"4\", \"4…\n$ cty          &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 1…\n$ hwy          &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25, 2…\n$ fl           &lt;chr&gt; \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p…\n$ class        &lt;chr&gt; \"compact\", \"compact\", \"compact\", \"compact\", \"compact\", \"c…\n\n\n\nWhat does the drv variable describe? Read the help for ?mpg to find out.\n\n\n?mpg\n\n\ndv means: “the type of drive train, where f = front-wheel drive, r = rear wheel drive, 4 = 4wd”\n\n\nMake a scatterplot of hwy vs cyl.\n\n\nggplot(data = mpg)+\ngeom_point(aes(hwy,cyl))\n\n\n\n\n\nWhat happens if you make a scatterplot of class vs drv? Why is the plot not useful?\n\n\nggplot(data = mpg)+\ngeom_point(aes(class,drv))\n\n\n\n\n\nA scatter plot is not a useful display of these variables since both drv and class are categorical variables. A scatterplots work best for plotting a continuous x and a continuous y variable, and when all (x, y) values are unique.\n\nHowever, there are other methods for plotting categorical variables: geom_count() and geom_tile()\n\nggplot(mpg, aes(x = class, y = drv)) +\n  geom_count()\n\n\n\n\n\nmpg %&gt;%\n  count(class, drv) %&gt;%\n  ggplot(aes(x = class, y = drv)) +\n    geom_tile(mapping = aes(fill = n))\n\n\n\n\n\nIn the previous plot, there are many missing tiles. These missing tiles represent unobserved combinations of class and drv values. These missing values are not unknown, but represent values of (class, drv) where n = 0. The complete() function in the tidyr package adds new rows to a data frame for missing combinations of columns.\n\n\nmpg %&gt;%\n  count(class, drv) %&gt;%\n  complete(class, drv, fill = list(n = 0)) %&gt;%\n  ggplot(aes(x = class, y = drv)) +\n    geom_tile(mapping = aes(fill = n))"
  },
  {
    "objectID": "til/dailyR0003/daily0001.html#exercises",
    "href": "til/dailyR0003/daily0001.html#exercises",
    "title": "R4DS Chapter 2 Exercises",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0          ✔ purrr   0.3.5     \n✔ tibble  3.1.8          ✔ dplyr   1.0.10    \n✔ tidyr   1.2.1.9001     ✔ stringr 1.5.0     \n✔ readr   2.1.3          ✔ forcats 0.5.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\nRun ggplot(data = mpg). What do you see?\n\n\nggplot(data = mpg)\n\n\n\n\n\nThis creates the background of the plot, but since no layers were specified with geom function, nothing is drawn.\n\nI see empty.\n\nHow many rows are in mpg? How many columns?\n\n\nnrow(mpg)\n\n[1] 234\n\n\nThere are 234 rows.\nWe can also use glimpes() to find number of rows:\n\nglimpse(mpg)\n\nRows: 234\nColumns: 11\n$ manufacturer &lt;chr&gt; \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"…\n$ model        &lt;chr&gt; \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 quattro\", \"…\n$ displ        &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0, 2.…\n$ year         &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, 200…\n$ cyl          &lt;int&gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8, …\n$ trans        &lt;chr&gt; \"auto(l5)\", \"manual(m5)\", \"manual(m6)\", \"auto(av)\", \"auto…\n$ drv          &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"4\", \"4\", \"4\", \"4\", \"4…\n$ cty          &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 1…\n$ hwy          &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25, 2…\n$ fl           &lt;chr&gt; \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p…\n$ class        &lt;chr&gt; \"compact\", \"compact\", \"compact\", \"compact\", \"compact\", \"c…\n\n\n\nWhat does the drv variable describe? Read the help for ?mpg to find out.\n\n\n?mpg\n\n\ndv means: “the type of drive train, where f = front-wheel drive, r = rear wheel drive, 4 = 4wd”\n\n\nMake a scatterplot of hwy vs cyl.\n\n\nggplot(data = mpg)+\ngeom_point(aes(hwy,cyl))\n\n\n\n\n\nWhat happens if you make a scatterplot of class vs drv? Why is the plot not useful?\n\n\nggplot(data = mpg)+\ngeom_point(aes(class,drv))\n\n\n\n\n\nA scatter plot is not a useful display of these variables since both drv and class are categorical variables. A scatterplots work best for plotting a continuous x and a continuous y variable, and when all (x, y) values are unique.\n\nHowever, there are other methods for plotting categorical variables: geom_count() and geom_tile()\n\nggplot(mpg, aes(x = class, y = drv)) +\n  geom_count()\n\n\n\n\n\nmpg %&gt;%\n  count(class, drv) %&gt;%\n  ggplot(aes(x = class, y = drv)) +\n    geom_tile(mapping = aes(fill = n))\n\n\n\n\n\nIn the previous plot, there are many missing tiles. These missing tiles represent unobserved combinations of class and drv values. These missing values are not unknown, but represent values of (class, drv) where n = 0. The complete() function in the tidyr package adds new rows to a data frame for missing combinations of columns.\n\n\nmpg %&gt;%\n  count(class, drv) %&gt;%\n  complete(class, drv, fill = list(n = 0)) %&gt;%\n  ggplot(aes(x = class, y = drv)) +\n    geom_tile(mapping = aes(fill = n))"
  },
  {
    "objectID": "til/dailyR0003/daily0001.html#exercises-1",
    "href": "til/dailyR0003/daily0001.html#exercises-1",
    "title": "R4DS Chapter 2 Exercises",
    "section": "2.3.1 Exercises",
    "text": "2.3.1 Exercises\n\nWhat’s gone wrong with this code? Why are the points not blue?\n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, color = \"blue\"))\n\n\n\n\n\nThe argument colour = “blue” is included within the mapping argument, and as such, it is treated as an aesthetic, which is a mapping between a variable and a value\n\nBelow is the correct format:\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy), color = \"blue\")\n\n\n\n\nand it is blow."
  },
  {
    "objectID": "til/dailyml0005/0003.html",
    "href": "til/dailyml0005/0003.html",
    "title": "Dropout and loss",
    "section": "",
    "text": "It’s the final round of interviews, and Savannah has done wonderfully well.\nThe final round is more technical. Savannah needs to answer a series of questions to demonstrate her understanding of different fundamental concepts.\nWhile talking about deep learning, the interviewer focuses on the use of Dropout and asks Savannah a question about something she hasn’t thought about before:\nHow would you expect the training loss to behave if you train your model several times, each using an increasing Dropout rate?\n\nThe training loss will be higher as we increase the Dropout rate.\nThe training loss will be lower as we increase the Dropout rate.\nThe training loss will start oscillating as we increase the Dropout rate.\nThe training loss will stay the same independently of the Dropout rate."
  },
  {
    "objectID": "til/dailyml0005/0003.html#dropout-and-loss",
    "href": "til/dailyml0005/0003.html#dropout-and-loss",
    "title": "Dropout and loss",
    "section": "",
    "text": "It’s the final round of interviews, and Savannah has done wonderfully well.\nThe final round is more technical. Savannah needs to answer a series of questions to demonstrate her understanding of different fundamental concepts.\nWhile talking about deep learning, the interviewer focuses on the use of Dropout and asks Savannah a question about something she hasn’t thought about before:\nHow would you expect the training loss to behave if you train your model several times, each using an increasing Dropout rate?\n\nThe training loss will be higher as we increase the Dropout rate.\nThe training loss will be lower as we increase the Dropout rate.\nThe training loss will start oscillating as we increase the Dropout rate.\nThe training loss will stay the same independently of the Dropout rate."
  },
  {
    "objectID": "til/dailyml0005/0003.html#attempt",
    "href": "til/dailyml0005/0003.html#attempt",
    "title": "Dropout and loss",
    "section": "Attempt",
    "text": "Attempt\nHere is similar question asked on Stackoverflow : Validation loss when using Dropout\n\nDropout is a regularization method that works well and is vital for reducing overfitting.\nSometimes, the nodes in a neural network create strong dependencies on other nodes, which may lead to overfitting. An example is when a few nodes on a layer do most of the work, and the network ignores all the other nodes. Despite having many nodes on the layer, you only have a small percentage of those nodes contributing to predictions. We call this phenomenon “co-adaptation,” and we can tackle it using Dropout.\nDuring training, Dropout randomly removes a percentage of the nodes, forcing the network to learn in a balanced way. Now every node is on its own and can’t rely on other nodes to do their work. They have to work harder by themselves.\nOne crucial characteristic of Dropout will help Savanah answer the question correctly: Like most regularization methods, Dropout sacrifices training accuracy to improve generalization.\nIf we run a few training sessions, each using an increasing amount of Dropout, we should see the training loss trend higher. In other words, the more we regularize our model, the harder it will be to learn the training data."
  },
  {
    "objectID": "til/dailyml0005/0003.html#recommended-reading",
    "href": "til/dailyml0005/0003.html#recommended-reading",
    "title": "Dropout and loss",
    "section": "Recommended reading",
    "text": "Recommended reading\n\nFor more information about co-adaptation and how to use Dropout, check “Improving neural networks by preventing co-adaptation of feature detectors”.\n“A Gentle Introduction to Dropout for Regularizing Deep Neural Networks” is an excellent introduction to Dropout."
  },
  {
    "objectID": "til/dailyml0002/dailyml_0022.html",
    "href": "til/dailyml0002/dailyml_0022.html",
    "title": "Model Recall",
    "section": "",
    "text": "The model’s recall\nA team built a binary classification model. They named the classes A and B.\nAfter finishing training, they evaluated the model on a validation set, and here is the confusion matrix with the results:\n\nGiven the above confusion matrix, what of the following correctly represents the model’s recall predicting classes A and B, respectively?\n\nThe model’s recall predicting class A is 80%, and class B is 80%.\nThe model’s recall predicting class A is 88%, and class B is 68%.\nThe model’s recall predicting class A is 88%, and class B is 88%.\nThe model’s recall predicting class A is 68%, and class B is 88%.\n\n\n\nAnswer\n\n\nRecommended Reading\n\nHow to Calculate Precision, Recall, and F-Measure for Imbalanced Classification"
  },
  {
    "objectID": "til/dailyml0012/0003.html",
    "href": "til/dailyml0012/0003.html",
    "title": "Modify Learning rate vs Early stopping",
    "section": "",
    "text": "“There’s something wrong with your network.”\nThat was the start of a message Camille received from her friend. She posted the plot of a neural network training loss online, and Camilla’s friend reached out to let her know.\nCamille is using gradient descent for the first time, so she appreciated the help.\n“Do you see what happened around the ninety epoch? The loss increases for a moment before coming back down again until the end. You don’t want that,”—concluded the message.\nWhat would you do if you were in Camille’s shoes?\n\nCamille should decrease the learning rate. That should stop the loss from increasing during training.\nCamille should increase the learning rate. That should stop the loss from increasing during training.\nCamille should use Early Stopping at around the ninety epoch.\nCamille shouldn’t do anything because her network has no problem."
  },
  {
    "objectID": "til/dailyml0012/0003.html#something-wrong",
    "href": "til/dailyml0012/0003.html#something-wrong",
    "title": "Modify Learning rate vs Early stopping",
    "section": "",
    "text": "“There’s something wrong with your network.”\nThat was the start of a message Camille received from her friend. She posted the plot of a neural network training loss online, and Camilla’s friend reached out to let her know.\nCamille is using gradient descent for the first time, so she appreciated the help.\n“Do you see what happened around the ninety epoch? The loss increases for a moment before coming back down again until the end. You don’t want that,”—concluded the message.\nWhat would you do if you were in Camille’s shoes?\n\nCamille should decrease the learning rate. That should stop the loss from increasing during training.\nCamille should increase the learning rate. That should stop the loss from increasing during training.\nCamille should use Early Stopping at around the ninety epoch.\nCamille shouldn’t do anything because her network has no problem."
  },
  {
    "objectID": "til/dailyml0012/0003.html#attempt",
    "href": "til/dailyml0012/0003.html#attempt",
    "title": "Modify Learning rate vs Early stopping",
    "section": "Attempt",
    "text": "Attempt\nGradient descent moves downhill on average, so a network that learns appropriately should see the loss decrease over the training session. However, individual updates can move in the opposite direction, causing the loss to fluctuate up and down.\nCamille’s plot shows the loss increasing momentarily, but it immediately starts decreasing. That’s normal, and Camille shouldn’t worry about it.\nSince the training process seems to be working correctly, modifying the learning loss might improve the results, but there’s nothing Camille needs to fix. If she uses Early Stopping, she will prevent the network from improving further.\nIn summary, Camille shouldn’t do anything at this point."
  },
  {
    "objectID": "til/dailyml0012/0003.html#recommended-reading",
    "href": "til/dailyml0012/0003.html#recommended-reading",
    "title": "Modify Learning rate vs Early stopping",
    "section": "Recommended reading",
    "text": "Recommended reading\n\nCheck “An overview of gradient descent optimization algorithms” for a deep dive into gradient descent and every one of its variants.\nCheck “Early Stopping” for an introduction to one of the most effective, easy-to-implement regularization techniques when training machine learning models"
  },
  {
    "objectID": "til/dailyml0009/0003.html",
    "href": "til/dailyml0009/0003.html",
    "title": "Effect of Batch Size on Training Time",
    "section": "",
    "text": "Lena ran two experiments training a neural network on a sample dataset. Her goal is to understand how different batch sizes affect the training process.\nOne of the experiments used a very small batch size, and the other used a batch size equal to all existing training data.\nAfter training and evaluating her models, Lena plotted the training and testing losses over 100 epochs of one of the experiments, and this is what she got:"
  },
  {
    "objectID": "til/dailyml0009/0003.html#effect-of-batch-size",
    "href": "til/dailyml0009/0003.html#effect-of-batch-size",
    "title": "Effect of Batch Size on Training Time",
    "section": "",
    "text": "Lena ran two experiments training a neural network on a sample dataset. Her goal is to understand how different batch sizes affect the training process.\nOne of the experiments used a very small batch size, and the other used a batch size equal to all existing training data.\nAfter training and evaluating her models, Lena plotted the training and testing losses over 100 epochs of one of the experiments, and this is what she got:"
  },
  {
    "objectID": "til/dailyml0009/0003.html#attempt",
    "href": "til/dailyml0009/0003.html#attempt",
    "title": "Effect of Batch Size on Training Time",
    "section": "Attempt",
    "text": "Attempt"
  },
  {
    "objectID": "til/dailyml0009/0003.html#which-of-the-following-options-is-the-most-likely-to-be-true",
    "href": "til/dailyml0009/0003.html#which-of-the-following-options-is-the-most-likely-to-be-true",
    "title": "Effect of Batch Size on Training Time",
    "section": "Which of the following options is the most likely to be true?",
    "text": "Which of the following options is the most likely to be true?\n\nThe model behind this plot corresponds to the experiment that used a very small batch size. This model was the one that took less time to train.\nThe model behind this plot corresponds to the experiment that used a very small batch size. This model was the one that took longer to train.\nThe model behind this plot corresponds to the experiment that used a batch size equal to all the available training data. This model was the one that took less time to train.\nThe model behind this plot corresponds to the experiment that used a batch size equal to all the available training data. This model was the one that took longer to train."
  },
  {
    "objectID": "til/dailyml0009/0003.html#recommended-reading",
    "href": "til/dailyml0009/0003.html#recommended-reading",
    "title": "Effect of Batch Size on Training Time",
    "section": "Recommended reading",
    "text": "Recommended reading\n\nCheck “What is the Curse of Dimensionality?” for an introduction to the Curse of Dimensionality.\nFor an explanation of data leakages, check “Data Leakage in Machine Learning”.\n“Introduction to Dimensionality Reduction for Machine Learning” covers different examples of dimensionality reduction."
  },
  {
    "objectID": "til/dailyml0001/dailyml_0022.html",
    "href": "til/dailyml0001/dailyml_0022.html",
    "title": "Active Learning",
    "section": "",
    "text": "Claire and Phil aren’t on the same page with their plan.\nThey want to train a machine learning model but want to minimize the number of samples they need to label. Labeling takes too long, and they want to avoid it as much as possible.\nClaire argues that they don’t need to train with the entire dataset. Instead, she believes they can maximize the model’s performance without using all the data.\nPhil disagrees. He argues that the only way to achieve the maximum possible performance is to train with the entire dataset. Since they aren’t willing to label all the data, they will need to settle for a mediocre model.\nWhat’s your opinion about this situation?\n\nAchieving the maximum possible performance without using the entire dataset is theoretically possible but very unlikely.\nThey can achieve the maximum possible performance without using the entire dataset by randomly sampling a portion of the data, labeling it, and training the model.\nThey can achieve the maximum possible performance without using the entire dataset, but they need a good strategy to sample the data they will label to train the model.\nThey will never achieve the maximum possible performance without using the entire dataset.\n\n\n\n\nWith active learning, we can build a model that will achieve better performance with fewer labeled samples by allowing the algorithm to choose the data that will provide the most information to its training process.\n\nClaire and Phil do not need to use the entire dataset to build a model that reaches its maximum possible performance. However, they will need a smart strategy to select the data they need to label.\nLet’s imagine a dataset with two classes that we can represent in two dimensions and a linear model that splits the data into two groups. Any samples around the lines’ boundaries that separate both classes are critical in our dataset. Those samples help the model decide how to split the data!\nBut what about samples far away from the split? They contribute much less to the model, and we don’t need them to find the separation between classes. The same happens with duplicate samples or samples that are too similar to existing ones.\nClaire and Phil, however, can’t depend on randomly sampling the dataset to decide which instances to label. They need a better strategy to determine which samples to pick.\nThis scenario is an example of Active learning. This learning technique allows us to build a better-performing machine learning model using fewer training labels by strategically choosing the samples to train the model.\n\n\n\n\nActive learning\nA Short Introduction to Active Learning\nTutorial on Active learning"
  },
  {
    "objectID": "til/dailyml0001/dailyml_0022.html#answer",
    "href": "til/dailyml0001/dailyml_0022.html#answer",
    "title": "Active Learning",
    "section": "",
    "text": "With active learning, we can build a model that will achieve better performance with fewer labeled samples by allowing the algorithm to choose the data that will provide the most information to its training process.\n\nClaire and Phil do not need to use the entire dataset to build a model that reaches its maximum possible performance. However, they will need a smart strategy to select the data they need to label.\nLet’s imagine a dataset with two classes that we can represent in two dimensions and a linear model that splits the data into two groups. Any samples around the lines’ boundaries that separate both classes are critical in our dataset. Those samples help the model decide how to split the data!\nBut what about samples far away from the split? They contribute much less to the model, and we don’t need them to find the separation between classes. The same happens with duplicate samples or samples that are too similar to existing ones.\nClaire and Phil, however, can’t depend on randomly sampling the dataset to decide which instances to label. They need a better strategy to determine which samples to pick.\nThis scenario is an example of Active learning. This learning technique allows us to build a better-performing machine learning model using fewer training labels by strategically choosing the samples to train the model."
  },
  {
    "objectID": "til/dailyml0001/dailyml_0022.html#recommended-reading",
    "href": "til/dailyml0001/dailyml_0022.html#recommended-reading",
    "title": "Active Learning",
    "section": "",
    "text": "Active learning\nA Short Introduction to Active Learning\nTutorial on Active learning"
  },
  {
    "objectID": "til/dailyML01 copy/daily0001.html",
    "href": "til/dailyML01 copy/daily0001.html",
    "title": "Connecting two Indpendent Clauses with Comma and Semicolons",
    "section": "",
    "text": "To use a comma or not before “but” always trips me up. For future reference, I am documenting the general rules for connecting two indepå`endent clauses using coordinating conjunctions (e.g., but and is) and Independent Marker Words (e.g., however and also).\nLet us start with the general definition of what is clause and the two types of clauses."
  },
  {
    "objectID": "til/dailyML01 copy/daily0001.html#connecting-independent-clauses",
    "href": "til/dailyML01 copy/daily0001.html#connecting-independent-clauses",
    "title": "Connecting two Indpendent Clauses with Comma and Semicolons",
    "section": "Connecting independent clauses",
    "text": "Connecting independent clauses\nThere are two ways to connect independent clauses: using coordinating conjunctions and using conjunctive adverbs (connecting words).\n\nUsing coordinating conjunctions\nThere are seven types of coordinating conjunctions: and, but, for, or, nor, so, and yet. When two independent clauses are connected by a coordinating conjunction, the comma is necessary before the coordinating conjunction.\n\nI went to school today, but I forgot my bag at the airport.\nShe invited me to the party, but they all gave her excuses for not coming.\nToday is a rainy day, but I will go to the park to see him.\nThe beach is a lot of fun, yet the mountains are better.\n\n\n\nUsing conjunctive adverb or a transitional expression/connecting words\nAn independent marker word is a word (e.g., also, consequently, furthermore, however, moreover, nevertheless, and therefore.) that can be used to connect two independent clauses. If the second independent clause has an independent marker word, a semicolon is needed before the independent marker word.\n\nUse a semicolon to join two related independent clauses in place of a comma and a coordinating conjunction.\n\n\nI am going out; however, I’ll be home by nine.\nI am going to the store; however, I’ll be back in an hour.\nKathleen worked for many hours on all her homework; nevertheless, she was unable to finish all of it. (Conjunctive Adverb is used to connect two independent clauses. e.g nevertheless)\nHarvey is a good driver; moreover, he is a friendly one.\nTony finished reading three novels this week; in contrast, Joan finished only one novel (Transitional Expression: e.g In contrast, on the other hand)."
  },
  {
    "objectID": "til/dailyML01 copy/daily0001.html#common-mistakes-when-connecting-independent-clauses",
    "href": "til/dailyML01 copy/daily0001.html#common-mistakes-when-connecting-independent-clauses",
    "title": "Connecting two Indpendent Clauses with Comma and Semicolons",
    "section": "Common mistakes when connecting independent clauses",
    "text": "Common mistakes when connecting independent clauses\nSemicolons join ideas that are related and equal in grammatical structure. For example, the following is a wrong sentence:\n\nThis assignment is extra credit only; but we still need to hand it in.\n\nThe problem is that though the two clauses are related, we are not accurately connecting them. We cannot use a semicolon there because the clauses are joined by the coordinating conjunction but. We can use either the semicolon there or the conjunction, but not both. The following two options are correct:\n\nThis assignment is extra credit only, but we still need to hand it in.\n\n\nThis assignment is extra credit only; however, we still need to hand it in.\n\nAnother issue is that semicolons should not be used between a dependent clause and an independent clause.\n\nAlthough Nate is a kind employee; that new guy is not (incorrect). Although Nate is a kind employee, that new guy is not (correct).\n\nSometimes, you can replace the comma and coordinating conjunction with a semicolon since the connection between the two independent clauses is clear without the coordinating conjunction.\n\nJohn finished all his homework, but Kathleen did not finish hers (correct using coordinating conjunction and comma).\n\n\nJohn finished all his homework; Kathleen did not finish hers."
  },
  {
    "objectID": "til/dailyML01 copy/daily0001.html#semicolons-can-replace-commas",
    "href": "til/dailyML01 copy/daily0001.html#semicolons-can-replace-commas",
    "title": "Connecting two Indpendent Clauses with Comma and Semicolons",
    "section": "Semicolons can replace commas",
    "text": "Semicolons can replace commas\nUse a semicolon to replace a comma when you use a coordinating conjunction to link independent clauses that already contain commas.\nThe comma in the example below makes these independent clauses difficult to read because of the other commas in the clauses:\n\nMy dog is sick. She won’t eat, run around, or jump, nor will she go for a walk with me (incorrect).\n\nUsing a semicolon makes it easier to read the two independent clauses on either side of the coordinating conjunction:\n\nMy dog is sick. She won’t eat, run around, or jump; nor will she go for a walk with me.\n\nThank you for reading my note! Please feel free to contact me if you have any questions."
  },
  {
    "objectID": "til/git-rebasd/daily0001.html",
    "href": "til/git-rebasd/daily0001.html",
    "title": "Fatal: Not possible to fast-forward, aborting",
    "section": "",
    "text": "This happens is your branch is no longer directly based off of the branch you’re trying to merge it into - e.g. another commit was added to the destination branch that isn’t in your branch. Thus, you can’t fast-forward into it (because fast-forward requires your branch to completely contain the destination branch).\nSolution that works for me is from StackOverflow\ngit pull --rebase"
  },
  {
    "objectID": "til/git-rebasd/daily0001.html#why-this-problem",
    "href": "til/git-rebasd/daily0001.html#why-this-problem",
    "title": "Fatal: Not possible to fast-forward, aborting",
    "section": "",
    "text": "This happens is your branch is no longer directly based off of the branch you’re trying to merge it into - e.g. another commit was added to the destination branch that isn’t in your branch. Thus, you can’t fast-forward into it (because fast-forward requires your branch to completely contain the destination branch).\nSolution that works for me is from StackOverflow\ngit pull --rebase"
  },
  {
    "objectID": "dailypython/py0002/py0002.html",
    "href": "dailypython/py0002/py0002.html",
    "title": "Python Strings Tips",
    "section": "",
    "text": "Today our tips is using strings in Python"
  },
  {
    "objectID": "dailypython/py0002/py0002.html#using-f-strings-to-limit-number-of-decimals",
    "href": "dailypython/py0002/py0002.html#using-f-strings-to-limit-number-of-decimals",
    "title": "Python Strings Tips",
    "section": "Using f Strings to limit number of decimals",
    "text": "Using f Strings to limit number of decimals\n\nnum = 5.45656\n\nprint(f'{num:.2f}') # Limit to 2 decimal\nprint(f'{num:.3f}') # Limit to 3 decimals\n\n5.46\n5.457"
  },
  {
    "objectID": "dailypython/py0002/py0002.html#using-f-string-for-calculation",
    "href": "dailypython/py0002/py0002.html#using-f-string-for-calculation",
    "title": "Python Strings Tips",
    "section": "Using f-String for Calculation",
    "text": "Using f-String for Calculation\n\napple = 3\nbanana = 2\nprint(f'The total price is {apple + banana}.')\n\nThe total price is 5."
  },
  {
    "objectID": "dailypython/py0002/py0002.html#debug-your-python-code-with-an-equal-sign-in-an-f-string",
    "href": "dailypython/py0002/py0002.html#debug-your-python-code-with-an-equal-sign-in-an-f-string",
    "title": "Python Strings Tips",
    "section": "Debug Your Python Code with an Equal Sign in an f-String",
    "text": "Debug Your Python Code with an Equal Sign in an f-String\n\nIt is common to use f”var={var}” to see which values are being printed.\n\n\nfrom itertools import permutations\n\nnums = [1, 2, 3]\n\nfor i, j in permutations(nums, 2):\n    print(f\"i={i}, j={j}\")\n\ni=1, j=2\ni=1, j=3\ni=2, j=1\ni=2, j=3\ni=3, j=1\ni=3, j=2\n\n\n\nIn Python 3.8 and above, you can get the same outputs using f”{var=}“.\n\n\nfor i, j in permutations(nums, 2):\n    print(f\"{i=}, {j=}\")\n\ni=1, j=2\ni=1, j=3\ni=2, j=1\ni=2, j=3\ni=3, j=1\ni=3, j=2"
  },
  {
    "objectID": "dailypython/py0002/py0002.html#string-find-find-the-index-of-a-substring-in-a-python-string",
    "href": "dailypython/py0002/py0002.html#string-find-find-the-index-of-a-substring-in-a-python-string",
    "title": "Python Strings Tips",
    "section": "String find: Find The Index of a Substring in a Python String",
    "text": "String find: Find The Index of a Substring in a Python String\n\nfind() method can be used to find an index of a substring. If the substring is not found, the method returns -1. Otherwise, the method returns the index of the first occurrence of the substring\n\n\nmylove = \"I love this car\"\n\n# Find the index of first occurrence of the substring\nmylove.find(\"car\")\n\n12\n\n\n\nThe index for the first occurrence of the word car is 12\n\n\n# returns -1 since \"taxi\" is not present in the original string\nmylove.find(\"taxi\")\n\n-1"
  },
  {
    "objectID": "dailypython/py0002/py0002.html#split-a-string-by-multiple-characters",
    "href": "dailypython/py0002/py0002.html#split-a-string-by-multiple-characters",
    "title": "Python Strings Tips",
    "section": "Split a String by Multiple Characters",
    "text": "Split a String by Multiple Characters\n\nstr.split allows us to split a string by only one character.\n\n\nsent = \"Today-is a nice_day\"\n\nsent.split('-')\n\n['Today', 'is a nice_day']\n\n\n\nWe can use re.split() to split string with multiple charaters\n\n\nimport re\n\n# split by space, -, or _\nre.split(\" |-|_\", sent)\n\n['Today', 'is', 'a', 'nice', 'day']"
  },
  {
    "objectID": "dailypython/py0002/py0002.html#multiline-strings",
    "href": "dailypython/py0002/py0002.html#multiline-strings",
    "title": "Python Strings Tips",
    "section": "Multiline Strings",
    "text": "Multiline Strings\n\nIf your Python string gets very long, you can break it up using parentheses or a backslash.\n\nparentheses\n\ntext = (\n    \"This is a very \"\n    \"long sentence \"\n    \"that is made up.\"\n)\ntext\n\n'This is a very long sentence that is made up.'\n\n\nbackslash\n\ntext = \"This is a very \"\\\n    \"long sentence \"\\\n    \"that is made up.\"\n\ntext\n\n'This is a very long sentence that is made up.'"
  },
  {
    "objectID": "dailypython/py0002/py0002.html#detect-almost-similar-articles-or-strings-using-difflib.sequencematcher",
    "href": "dailypython/py0002/py0002.html#detect-almost-similar-articles-or-strings-using-difflib.sequencematcher",
    "title": "Python Strings Tips",
    "section": "Detect “Almost Similar” Articles or Strings using difflib.SequenceMatcher:",
    "text": "Detect “Almost Similar” Articles or Strings using difflib.SequenceMatcher:\n\nWhen analyzing articles, different articles can be almost similar but not 100% identical, maybe because of the grammar, or because of the change in two or three words (such as cross-posting). How can we detect the “almost similar” articles and drop one of them? That is when difflib.SequenceMatcher comes in handy.\n\n\nfrom difflib import SequenceMatcher\n\ntext1 = 'I am shamsuddeen'\ntext2 = 'I am shamsu'\nprint(SequenceMatcher(a=text1, b=text2).ratio())\n\n0.8148148148148148\n\n\n\nYou can also find best matches using difflib.get_close_matches:\n\n\nfrom difflib import get_close_matches\n\ntools = ['pencil', 'pen', 'erasor', 'ink']\nget_close_matches('pencel', tools)\n\n['pencil', 'pen']\n\n\nTo get closer matches, increase the value of the argument cutoff (default 0.6).\n\nget_close_matches('pencel', tools, cutoff=0.8)\n\n['pencil']\n\n\nSee you tomorrow !"
  }
]