<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-99.9.9">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2022-04-18">
<meta name="description" content="It discussed how deep learning changes our approach to machine learning and why PyTorch is a good fit for deep learning">

<title>Shamsuddeen Hassan Muhammad’s Blog - Chapter one Summary</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

<script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-124900795-1', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script>
<script defer="" data-domain="ivelasq.rbind.io" src="https://plausible.io/js/plausible.js"></script>


<meta name="twitter:title" content="Shamsuddeen Hassan Muhammad’s Blog - Chapter one Summary">
<meta name="twitter:description" content="It discussed how deep learning changes our approach to machine learning and why PyTorch is a good fit for deep learning">
<meta name="twitter:image" content="https://shmuhammadblog.github.io/blog/dlwp01/book_pic.png">
<meta name="twitter:creator" content="Shamsuddeen Hassan Muhammad">
<meta name="twitter:site" content="@shmuhammadd">
<meta name="twitter:image-height" content="534">
<meta name="twitter:image-width" content="500">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Shamsuddeen Hassan Muhammad</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="dropdown-header">
 <span class="menu-text">NLP Researcher and Data Scientist</span></li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../til.html" rel="" target="">
 <span class="menu-text">Today I Learned</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/Shmuhammadd" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/shmuhammad2004" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/shmuhammad/" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml" rel="" target=""><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Chapter one Summary</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Pytorch</div>
    <div class="quarto-category">Books</div>
  </div>
  </div>

<div>
  <div class="description">
    It discussed how deep learning changes our approach to machine learning and why PyTorch is a good fit for deep learning
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 18, 2022</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<center>
<p><img src="./images/book_pic.png" class="img-fluid"></p>
</center>
<section id="book-organization" class="level1">
<h1>Book Organization</h1>
<p>PART 1CORE PYTORCH 1. Intro to deep learning and PYTORCH</p>
<ol start="2" type="1">
<li><p>Pretrained models</p></li>
<li><p>Tensors</p></li>
<li><p>Real word data representation using Tensors</p></li>
<li><p>The mechanics of learning</p></li>
<li><p>Using neural networks to fit data</p></li>
<li><p>Learning from Images</p></li>
<li><p>Using convolutional to generalize</p></li>
</ol>
<p>Part 2 Learning from images in the real world: Early detection of lung 1. Using Pytorch to fight Cancer 2.</p>
<p>Part 3 DEPLOYMENT</p>
</section>
<section id="chapter-01-introduction-to-deep-learning-and-pytorch" class="level1">
<h1>Chapter 01 : Introduction to deep learning and PYTORCH</h1>
<p>This chapter objectives</p>
<blockquote class="blockquote">
<p>How deep learning changes our approach to machine learning</p>
</blockquote>
<blockquote class="blockquote">
<p>Understanding why PyTorch is a good fit for deep learning</p>
</blockquote>
<blockquote class="blockquote">
<p>Examining a typical deep learning project</p>
</blockquote>
<blockquote class="blockquote">
<p>The hardware you’ll need to follow along with the examples</p>
</blockquote>
<blockquote class="blockquote">
<blockquote class="blockquote">
<p>This book focuses on practical PyTorch, with the aim of covering enough ground to allow you to solve real-world machine learning problems, such as in vision, with deep learning or explore new models as they pop up in research literature</p>
</blockquote>
</blockquote>
<section id="pre-requisites" class="level2">
<h2 class="anchored" data-anchor-id="pre-requisites">Pre-requisites:</h2>
<ul>
<li><p>Experience with some Python programming: datatypes,classes,functions, loops, lists, dictionaries, etc.</p></li>
<li><p>A willingness to learn</p></li>
</ul>
</section>
<section id="deep-learning-and-machine-learning-deep-learning-revolution" class="level2">
<h2 class="anchored" data-anchor-id="deep-learning-and-machine-learning-deep-learning-revolution">Deep learning and machine learning: deep learning revolution</h2>
<ul>
<li><p>Machine learning relied heavily on feature engineering. Features are transformations on input data that facilitate a downstream algorithm, like a classifier, to produce correct outcomes on new data.</p></li>
<li><p>Deep learning, on the other hand, deals with finding such representations automatically, from raw data, in order to successfully perform a task.</p></li>
<li><p>is is not to say that feature engineering has no place with deep learning; we often need to inject some form of prior knowledge in a learning system.</p></li>
</ul>
<blockquote class="blockquote">
<p>Often, these automatically created features are better than those that are handcrafted! As with many disruptive technologies, this fact has led to a change in perspective.</p>
</blockquote>
<center>
<p><img src="./images/mlvsdp.png" class="img-fluid"></p>
</center>
<section id="how-feautures-are-learned" class="level3">
<h3 class="anchored" data-anchor-id="how-feautures-are-learned">How feautures are learned?</h3>
<center>
<p><img src="./images/layeredlearning.png" class="img-fluid"></p>
</center>
</section>
<section id="deep-learning-steps" class="level3">
<h3 class="anchored" data-anchor-id="deep-learning-steps">Deep Learning steps</h3>
<ul>
<li><p>We need a way to ingest whatever data we have at hand.</p></li>
<li><p>We somehow need to define the deep learning machine.</p></li>
<li><p>We must have an automated way, training, to obtain useful representations and make the machine produce desired outputs” (</p></li>
</ul>
<center>
<p><img src="./images/deep_learning_training.png" class="img-fluid"></p>
</center>
<blockquote class="blockquote">
<p>Training consists of driving the criterion toward lower and lower scores by incrementally modifying our deep learning machine until it achieves low scores, even on data not seen during training</p>
</blockquote>
</section>
</section>
<section id="pytorch-for-deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="pytorch-for-deep-learning">PyTorch for deep learning</h2>
<ul>
<li><p>PyTorch is a library for Python programs that facilitates building deep learning projects.</p></li>
<li><p>As Python does for programming, PyTorch provides an excellent introduction to deep learning.</p></li>
<li><p>At its core, the deep learning machine in figure above is a rather complex mathematical function mapping inputs to an output. To facilitate expressing this function, PyTorch provides a core data structure, the tensor, which is a multidimensional array that shares many similarities with NumPy arrays.</p></li>
<li><p>Then, PyTorch comes with features to perform accelerated mathematical operations on dedicated hardware, which makes it convenient to design neural network architectures and train them on individual machines or parallel computing resources.</p></li>
</ul>
</section>
<section id="why-pytorch" class="level2">
<h2 class="anchored" data-anchor-id="why-pytorch">Why PyTorch?</h2>
<ul>
<li><p>It’s Pythonic and easy to learn, and using the library generally feels familiar to developers who have used Python previously.</p></li>
<li><p>PyTorch gives us a data type, the <strong>Tensor</strong>, to hold numbers, vectors, matrices, or arrays in general. In addition, it provides functions for operating on them</p></li>
<li><p>But PyTorch offers two things that make it particularly relevant for deep learning:</p>
<ul>
<li><p>it provides accelerated computation using graphical processing units (GPUs), often yielding speedups in the range of 50x over doing the same calculation on a CPU.</p></li>
<li><p>PyTorch provides facilities that support numerical optimization on generic mathematical expressions, which deep learning uses for training (we can safely characterize PyTorch as a high-performance library with optimization (e.g RMSProp and Adam) support for scientific computing in Python.)</p></li>
</ul></li>
<li><p>While it was initially focused on research workflows, PyTorch has been equipped with a high-performance C++ runtime that can be used to deploy models for inference without relying on Python, and can be used for designing and training models in C++.</p></li>
<li><p>It also has bindings to other languages and an interface for deploying to mobile devices.</p></li>
</ul>
<section id="the-deep-learning-learnscapes" class="level3">
<h3 class="anchored" data-anchor-id="the-deep-learning-learnscapes">The deep learning Learnscapes</h3>
<center>
<p><img src="./images/libraries.png" class="img-fluid"></p>
</center>
<ul>
<li><p>Theano and TensorFlow were the premiere low-level libraries, working with a model that had the user define a computational graph and then execute it.</p></li>
<li><p>Lasagne and Keras were high-level wrappers around Theano, with Keras wrapping TensorFlow and CNTK as well.</p></li>
<li><p>Caffe, Chainer, DyNet, Torch (the Lua-based precursor to PyTorch), MXNet, CNTK, DL4J, and others filled various niches in the ecosystem.</p></li>
</ul>
<blockquote class="blockquote">
<p>The community largely consolidated behind either <strong>PyTorch</strong> or <strong>TensorFlow</strong>, with the adoption of other libraries dwindling, except for those filling specific niches.</p>
</blockquote>
<ul>
<li><p>Theano, one of the first deep learning frameworks, has ceased active development.</p></li>
<li><p>TensorFlow:</p>
<ul>
<li><p>Consumed Keras entirely, promoting it to a first-class API (means you can operate on them in the usual manner)</p></li>
<li><p>Released TF 2.0 with <a href="https://towardsdatascience.com/eager-execution-vs-graph-execution-which-is-better-38162ea4dbf6">eager mode</a> by default (even though slower than graph mode/execution)</p></li>
</ul></li>
<li><p>JAX,</p>
<ul>
<li>a library by Google that was developed independently from TensorFlow, has started gaining traction as a NumPy equivalent with GPU, autograd and JIT capabilities.</li>
</ul></li>
<li><p>PyTorch:</p>
<ul>
<li><p>Consumed Caffe2 for its backend</p></li>
<li><p>Replaced most of the low-level code reused from the Lua-based Torch project</p></li>
<li><p>Added support for ONNX, a vendor-neutral model description and exchange format</p></li>
<li><p>Added a delayed-execution “graph mode” runtime called TorchScript</p></li>
<li><p>Released version 1.0</p></li>
<li><p>Replaced CNTK and Chainer as the framework of choice by their respective corporate sponsors</p></li>
</ul></li>
</ul>
<blockquote class="blockquote">
<p>TensorFlow has a robust pipeline to production, an extensive industry-wide community, and massive mindshare. PyTorch has made huge inroads with the research and teaching communities, thanks to its ease of use, and has picked up momentum since, as researchers and graduates train students and move to industry. It has also built up steam in terms of production solutions.</p>
</blockquote>
<blockquote class="blockquote">
<p>Interestingly, with the advent of TorchScript and eager mode, both PyTorch and TensorFlow have seen their feature sets start to converge with the other’s, though the presentation of these features and the overall experience is still quite different between the two</p>
</blockquote>
<center>
<p><img src="./images/TensorflowvsPytorch.png" class="img-fluid">)</p>
</center>
</section>
</section>
<section id="an-overview-of-how-pytorch-supports-deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="an-overview-of-how-pytorch-supports-deep-learning">1.4 An overview of how PyTorch supports deep learning</h2>
<ul>
<li><p>Pytorch is written in Python, but there’s a lot of non-Python code in it.</p></li>
<li><p>For performance reasons, most of PyTorch is written in C++ and <a href="www.geforce.com/hardware/technology/cuda">CUDA</a>, a C++-like language from NVIDIA that can be compiled to run with massive parallelism on GPUs.</p></li>
<li><p>However, the Python API is where PyTorch shines in term of usability and integration with the wider Python ecosystem</p></li>
</ul>
<section id="pytorchs-support-for-deep-learning-tensors-autograd-and-distributed-computing" class="level3">
<h3 class="anchored" data-anchor-id="pytorchs-support-for-deep-learning-tensors-autograd-and-distributed-computing">PyTorch’s support for deep learning : Tensors, autograd, and distributed computing</h3>
<ul>
<li>PyTorch is a library that provides multidimensional arrays, or tensors and an extensive library of operations on them, provided by the torch module.</li>
</ul>
<blockquote class="blockquote">
<p>Tensors is just an n-dimensional array in PyTorch. Tensors support some additional enhancements which make them unique: Apart from CPU, they can be loaded on the GPU for faster computations</p>
</blockquote>
<p><img src="./images/tensor.png" class="img-fluid"></p>
<blockquote class="blockquote">
<p>A tensor is a generalization of vectors and matrices and is easily understood as a multidimensional array. Tensors are the building blocks of deep learning, and they are the most fundamental data structures in PyTorch (Tensorflow named after Tensor).</p>
</blockquote>
<center>
<p><img src="./images/pytorch_pipeline.png" class="img-fluid"></p>
</center>
</section>
<section id="pytorchs-support-for-deep-learning-autograd-and-distributed-computing" class="level3">
<h3 class="anchored" data-anchor-id="pytorchs-support-for-deep-learning-autograd-and-distributed-computing">PyTorch’s support for deep learning : Autograd, and distributed computing</h3>
<ul>
<li><p>The second core thing that PyTorch provides is the ability of tensors to keep track of the operations performed on them and to analytically compute derivatives of an output of a computation with respect to any of its inputs.</p></li>
<li><p>This is used for numerical optimization, and it is provided natively by tensors by virtue of dispatching through <a href="https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95">PyTorch’s autograd</a> engine under the hood.</p></li>
</ul>
<blockquote class="blockquote">
<p><a href="https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95">PyTorch’s autograd</a> abstracts the complicated mathematics and helps us “magically” calculate gradients of high dimensional curves with only a few lines of code.</p>
</blockquote>
<center>
<p><img src="./images/deep_learning_training.png" class="img-fluid"></p>
</center>
<p>On setting <strong>.requires_grad = True</strong> they start forming a backward graph that tracks every operation applied on them to calculate the gradients using something called a dynamic computation graph (DCG).</p>
<section id="torch.nn" class="level4">
<h4 class="anchored" data-anchor-id="torch.nn">1 Torch.nn</h4>
<blockquote class="blockquote">
<p>The core PyTorch modules for building neural networks are located in torch.nn, which provides common neural network layers and other architectural components. Fully connected layers, convolutional layers, activation functions, and loss functions can all be found here</p>
</blockquote>
<center>
<p><img src="./images/torchnn.png" class="img-fluid"></p>
</center>
</section>
<section id="fetching-data" class="level4">
<h4 class="anchored" data-anchor-id="fetching-data">2. Fetching data</h4>
<blockquote class="blockquote">
<p>We need to convert each sample from our data into a something PyTorch can actually handle: tensors.</p>
</blockquote>
<blockquote class="blockquote">
<p>This bridge between our <strong>custom data</strong> (in whatever format it might be) and a** standardized PyTorch tensor** is the <strong>Dataset</strong> class PyTorch provides in torch.utils.data. (Discuss in chapter 4). For example, text data can (), image data (), vedio data ()</p>
</blockquote>
<p>As this process is wildly different from one problem to the next, we will have to implement this data sourcing ourselves</p>
<p>For exaple, Vector data—Rank-2 tensors of shape (samples, features), where each sample is a vector of numerical attributes (“features”)</p>
<p>Timeseries data or sequence data—Rank-3 tensors of shape (samples, timesteps, features), where each sample is a sequence (of length timesteps) of feature vectors</p>
<p>Images—Rank-4 tensors of shape (samples, height, width, channels), where each sample is a 2D grid of pixels, and each pixel is represented by a vector of values (“channels”)</p>
<p>Video—Rank-5 tensors of shape (samples, frames, height, width, channels), where each sample is a sequence (of length frames) of images</p>
</section>
<section id="data-batches" class="level4">
<h4 class="anchored" data-anchor-id="data-batches">2. Data Batches</h4>
<blockquote class="blockquote">
<p>As data storage is often slow, we want to parallelize data loading. However, Python does not provide easy, efficient, parallel processing, we will need multiple processes to load our data. In order to assemble them into batches (tensors that encompass several samples). PyTorch provides all that magic in the <strong>DataLoader</strong> class (Chapt 7).</p>
</blockquote>
<center>
<p><img src="./images/pytorch_pipeline.png" class="img-fluid"></p>
</center>
</section>
<section id="training-several-steps" class="level4">
<h4 class="anchored" data-anchor-id="training-several-steps">Training (several steps)</h4>
<blockquote class="blockquote">
<p>At each step in the training loop, we evaluate our model on the samples we got from the data loader. We then compare the outputs of our model to the desired output (the targets) using some criterion or loss function. PyTorch also has a variety of loss functions at our disposal (Torch.nn).</p>
</blockquote>
<center>
<p><img src="./images/deep_learning_training.png" class="img-fluid"></p>
</center>
<blockquote class="blockquote">
<p>After we have compared our actual outputs to the ideal with the loss functions, we need to push the model a little to move its outputs to better resemble the target. This is where the PyTorch autograd engine comes in; but we also need an optimizer doing the updates, and that is what PyTorch offers us in torch.optim. (Chapt 5,6,8)</p>
</blockquote>
<p>How you should change your weights or learning rates of your neural network to reduce the losses is defined by the <strong>optimizers</strong> you use !!! The right optimization algorithm can reduce training time exponentially. Optimization algorithms or strategies are responsible for reducing the losses and to provide the most accurate results possible.</p>
<blockquote class="blockquote">
<blockquote class="blockquote">
<p><a href="https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6">Stochastic Gradient Descent, Mini-Batch Gradient Descent, Momentum, Adagrad , AdaDelta , Adam</a></p>
</blockquote>
</blockquote>
<blockquote class="blockquote">
<p>Gradient Descent is the most basic but most used optimization algorithm. It’s used heavily in linear regression and classification algorithms. Backpropagation in neural networks also uses a gradient descent algorithm</p>
</blockquote>
<center>
<p><img src="./images/pytorch_pipeline.png" class="img-fluid"></p>
</center>
<section id="parallel-training" class="level5">
<h5 class="anchored" data-anchor-id="parallel-training">Parallel training</h5>
<blockquote class="blockquote">
<p>Sometimes, we need to train on multiple GPU : <strong>torch.nn.parallel.DistributedDataParallel</strong> and the <strong>torch.distributed</strong> submodule can be employed to use the additional hardware.</p>
</blockquote>
</section>
</section>
<section id="trained-model" class="level4">
<h4 class="anchored" data-anchor-id="trained-model">4. Trained Model</h4>
<blockquote class="blockquote">
<p>The training loop is the most time-consuming part of a deep learning project. At the end of it, we are rewarded with a model whose parameters have been <strong>optimized on our task</strong>. This is the <strong>Trained Model</strong>.</p>
</blockquote>
</section>
<section id="deploying-the-model" class="level4">
<h4 class="anchored" data-anchor-id="deploying-the-model">5. Deploying the model</h4>
<blockquote class="blockquote">
<p>Thi may involve putting the model on a server or exporting it to load it to a cloud engine, Or we might integrate it with a larger application, or run it on a phone.</p>
</blockquote>
<p>PyTorch defaults to an <a href="https://towardsdatascience.com/eager-execution-vs-graph-execution-which-is-better-38162ea4dbf6">immediate execution model (eager mode)</a>. Whenever an instruction involving PyTorch is executed by the Python interpreter, the corresponding operation is immediately carried out by the underlying C++ or CUDA implementation. As more instructions operate on tensors, more operations are executed by the backend implementation.</p>
</section>
</section>
</section>
<section id="hardware-and-software-requirements" class="level2">
<h2 class="anchored" data-anchor-id="hardware-and-software-requirements">1.5 Hardware and software requirements</h2>
<ul>
<li><p>First part can be use on CPU</p></li>
<li><p>Second part you may need CUDA-enabled GPU with atleast 8GB of memory or better</p></li>
<li><p>To be clear: GPU is not mandatory if you’re willing to wait, but running on a GPU cuts training time by at least an order of magnitude (and usually it’s 40–50x faster).</p></li>
<li><p>Colab is a Google-owned cloud environment that runs on Google Cloud Platform. It is a free and open-source platform for creating and sharing interactive and reproducible workflows.</p></li>
<li><p>Again, training reduced by using multiple GPUs on the same machine, and even further on clusters of machines equipped with multiple GPUs.</p></li>
<li><p>Part 2 has some nontrivial download bandwidth and disk space requirements as well. The raw data needed for the cancer-detection project in part 2 is about 60 GB to download, and when uncompressed it requires about 120 GB of space.</p></li>
<li><p>While it is possible to use network storage for this, there might be training speed penalties if the network access is slower than local disk. Preferably you will have space on a local SSD to store the data for fast retrieval</p></li>
</ul>
</section>
<section id="exercises" class="level2">
<h2 class="anchored" data-anchor-id="exercises">Exercises</h2>
<div id="cell-69" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.rand(<span class="dv">5</span>, <span class="dv">3</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[0.0515, 0.2562, 0.3839],
        [0.1516, 0.3848, 0.1570],
        [0.7761, 0.2426, 0.5270],
        [0.9191, 0.4228, 0.1838],
        [0.0467, 0.3779, 0.7167]])</code></pre>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    if (id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        for (let i = 0; i < 2; i++) {
          container.appendChild(note.children[i].cloneNode(true));
        }
        return container.innerHTML
      } else {
        return note.innerHTML;
      }
    } else {
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      try { hash = new URL(url).hash; } catch {}
      const id = hash.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note !== null) {
        try {
          const html = processXRef(id, note);
          instance.setContent(html);
        } finally {
          instance.enable();
          instance.show();
        }
      } else {
        // See if we can fetch this
        fetch(url.split('#')[0])
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.getElementById(id);
          console.log(htmlDoc.body.innerHTML);
          if (note !== null) {
            const html = processXRef(id, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="shmuhammad2004/shmuhammadblog" data-repo-id="R_kgDOHb5q2A" data-category="Announcements" data-category-id="DIC_kwDOHb5q2M4CPbHo" data-mapping="pathname" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="light">
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../license.html">
<p>License</p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>



</body></html>