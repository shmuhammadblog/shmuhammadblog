{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Chapter one Summary\"\n",
    "format: \n",
    "  html:\n",
    "    code-fold: false\n",
    "jupyter: python3\n",
    "categories:   \n",
    "  - Pytorch\n",
    "  - Books\n",
    "description: \"It discussed how deep learning changes our approach to machine learning and why PyTorch is a good fit for deep learning\"\n",
    "image: book_pic.png\n",
    "date: \"2022-04-18\"\n",
    "draft: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "![](./images/book_pic.png) \n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 1CORE PYTORCH\n",
    "   1. Intro to deep learning and PYTORCH\n",
    "   \n",
    "   2. Pretrained models\n",
    "   \n",
    "   3. Tensors\n",
    "   \n",
    "   4. Real word data representation using Tensors\n",
    "   \n",
    "   5. The mechanics of learning\n",
    "   \n",
    "   6. Using neural networks to fit data\n",
    "   \n",
    "   7. Learning from Images\n",
    "   \n",
    "   8. Using convolutional to generalize\n",
    "   \n",
    "Part 2 Learning from images in the real world: Early detection of lung \n",
    "   1.  Using Pytorch to fight Cancer\n",
    "   2.  \n",
    "   \n",
    "Part 3 DEPLOYMENT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 01 : Introduction to deep learning and PYTORCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter objectives \n",
    "\n",
    "> How deep learning changes our approach to machine learning \n",
    "\n",
    "> Understanding why PyTorch is a good fit for deep learning \n",
    "\n",
    "> Examining a typical deep learning project \n",
    "\n",
    "> The hardware you’ll need to follow along with the examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> This book focuses on practical PyTorch, with the aim of covering enough ground to allow you to solve real-world machine learning problems, such as in vision, with deep learning or explore new models as they pop up in research literature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites:\n",
    "\n",
    "- Experience with some Python programming: datatypes,classes,functions, loops, lists, dictionaries, etc.\n",
    "  \n",
    "- A willingness to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning and machine learning: deep learning revolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Machine learning relied heavily on feature engineering. Features are transformations on input data that facilitate a downstream algorithm, like a classifier, to produce correct outcomes on new data.\n",
    "  \n",
    "- Deep learning, on the other hand, deals with finding such representations automatically, from raw data, in order to successfully perform a task.\n",
    "  \n",
    "- is is not to say that feature engineering has no place with deep learning; we often need to inject some form of prior knowledge in a learning system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Often, these automatically created features are better than those that are handcrafted! As with many disruptive technologies, this fact has led to a change in perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "![](./images/mlvsdp.png) \n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How feautures are learned?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "\n",
    "![](./images/layeredlearning.png) \n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning steps\n",
    "\n",
    "- We need a way to ingest whatever data we have at hand. \n",
    "\n",
    "- We somehow need to define the deep learning machine. \n",
    "\n",
    "- We must have an automated way, training, to obtain useful representations and make the machine produce desired outputs” ("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "![](./images/deep_learning_training.png)\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Training consists of driving the criterion toward lower and lower scores by incrementally modifying our deep learning machine until it achieves low scores, even on data not seen during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch for deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch is a library for Python programs that facilitates building deep learning projects.\n",
    "  \n",
    "- As Python does for programming, PyTorch provides an excellent introduction to deep learning. \n",
    "    \n",
    "- At its core, the deep learning machine in figure above is a rather complex mathematical function mapping inputs to an output. To facilitate expressing this function, PyTorch provides a core data structure, the tensor, which is a multidimensional array that shares many similarities with NumPy arrays. \n",
    "\n",
    "- Then, PyTorch comes with features to perform accelerated mathematical operations on dedicated hardware, which makes it convenient to design neural network architectures and train them on individual machines or parallel computing resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why PyTorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  It’s Pythonic and easy to learn, and using the library generally feels familiar to developers who have used Python previously.\n",
    "\n",
    "-  PyTorch gives us a data type, the **Tensor**, to hold numbers, vectors, matrices, or arrays in general. In addition, it provides functions for operating on them\n",
    "  \n",
    "- But PyTorch offers two things that make it particularly relevant for deep learning: \n",
    "  - it provides accelerated computation using graphical processing units (GPUs), often yielding speedups in the range of 50x over doing the same calculation on a CPU. \n",
    "  \n",
    "  - PyTorch provides facilities that support numerical optimization on generic mathematical expressions, which deep learning uses for training (we can safely characterize PyTorch as a high-performance library with optimization (e.g RMSProp and Adam) support for scientific computing in Python.)\n",
    "  \n",
    "- While it was initially focused on research workflows, PyTorch has been equipped with a high-performance C++ runtime that can be used to deploy models for inference without relying on Python, and can be used for designing and training models in C++. \n",
    "  \n",
    "- It also has bindings to other languages and an interface for deploying to mobile devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The deep learning Learnscapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "![](./images/libraries.png) \n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Theano and TensorFlow were the premiere low-level libraries, working with a model that had the user define a computational graph and then execute it. \n",
    "\n",
    "- Lasagne and Keras were high-level wrappers around Theano, with Keras wrapping TensorFlow and CNTK as well. \n",
    "\n",
    "- Caffe, Chainer, DyNet, Torch (the Lua-based precursor to PyTorch), MXNet, CNTK, DL4J, and others filled various niches in the ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The community largely consolidated behind either **PyTorch** or **TensorFlow**, with the adoption of other libraries dwindling, except for those filling specific niches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Theano, one of the first deep learning frameworks, has ceased active development. \n",
    "\n",
    "- TensorFlow: \n",
    "  - Consumed Keras entirely, promoting it to a first-class API (means you can operate on them in the usual manner) \n",
    "  \n",
    "  - Released TF 2.0 with [eager mode](https://towardsdatascience.com/eager-execution-vs-graph-execution-which-is-better-38162ea4dbf6) by default (even though slower than graph mode/execution) \n",
    "\n",
    "- JAX, \n",
    "  - a library by Google that was developed independently from TensorFlow, has started gaining traction as a NumPy equivalent with GPU, autograd and JIT capabilities. \n",
    "\n",
    "\n",
    "- PyTorch:\n",
    "  - Consumed Caffe2 for its backend \n",
    "  \n",
    "  - Replaced most of the low-level code reused from the Lua-based Torch project \n",
    "  \n",
    "  - Added support for ONNX, a vendor-neutral model description and exchange format \n",
    "  \n",
    "  - Added a delayed-execution “graph mode” runtime called TorchScript \n",
    "  \n",
    "  - Released version 1.0\n",
    "  \n",
    "  - Replaced CNTK and Chainer as the framework of choice by their respective corporate sponsors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TensorFlow has a robust pipeline to production, an extensive industry-wide community, and massive mindshare. PyTorch has made huge inroads with the research and teaching communities, thanks to its ease of use, and has picked up momentum since, as researchers and graduates train students and move to industry. It has also built up steam in terms of production solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Interestingly, with the advent of TorchScript and eager mode, both PyTorch and TensorFlow have seen their feature sets start to converge with the other’s, though the presentation of these features and the overall experience is still quite different between the two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "![](./images/TensorflowvsPytorch.png)) \n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 An overview of how PyTorch supports deep learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pytorch is written in Python, but there’s a lot of non-Python code in it. \n",
    "\n",
    "- For performance reasons, most of PyTorch is written in C++ and [CUDA](www.geforce.com/hardware/technology/cuda), a C++-like language from NVIDIA that can be compiled to run with massive parallelism on GPUs.\n",
    "\n",
    "- However, the Python API is where PyTorch shines in term of usability and integration with the wider Python ecosystem\n",
    "\n",
    "\n",
    "###  PyTorch’s support for deep learning : Tensors, autograd, and distributed computing\n",
    "\n",
    "\n",
    "- PyTorch is a library that provides multidimensional arrays, or tensors and an extensive library of operations on them, provided by the torch module. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tensors is just an n-dimensional array in PyTorch. Tensors support some additional enhancements which make them unique: Apart from CPU, they can be loaded on the GPU for faster computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/tensor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A tensor is a generalization of vectors and matrices and is easily understood as a multidimensional array. Tensors are the building blocks of deep learning, and they are the most fundamental data structures in PyTorch (Tensorflow named after Tensor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "![](./images/pytorch_pipeline.png) \n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   PyTorch’s support for deep learning : Autograd, and distributed computing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The second core thing that PyTorch provides is the ability of tensors to keep track of the operations performed on them and to analytically compute derivatives of an output of a computation with respect to any of its inputs.\n",
    "\n",
    "- This is used for numerical optimization, and it is provided natively by tensors by virtue of dispatching through [PyTorch’s autograd](https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95) engine under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  [PyTorch’s autograd](https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95) abstracts the complicated mathematics and helps us “magically” calculate gradients of high dimensional curves with only a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "![](./images/deep_learning_training.png)\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On setting **.requires_grad = True** they start forming a backward graph that tracks every operation applied on them to calculate the gradients using something called a dynamic computation graph (DCG)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 Torch.nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The core PyTorch modules for building neural networks are located in torch.nn, which provides common neural network layers and other architectural components. Fully connected layers, convolutional layers, activation functions, and loss functions can all be found here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "![](./images/torchnn.png)\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Fetching data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  We need to convert each sample from our data into a something PyTorch can actually handle: tensors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This bridge between our **custom data** (in whatever format it might be) and a** standardized PyTorch tensor** is the **Dataset** class PyTorch provides in torch.utils.data. (Discuss in chapter 4). For example, text data can (), image data (), vedio data ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this process is wildly different from one problem to the next, we will have to implement this data sourcing ourselves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For exaple, Vector data—Rank-2 tensors of shape (samples, features), where each sample is a vector of numerical attributes (“features”) \n",
    "\n",
    "Timeseries data or sequence data—Rank-3 tensors of shape (samples, timesteps, features), where each sample is a sequence (of length timesteps) of feature vectors \n",
    "\n",
    "Images—Rank-4 tensors of shape (samples, height, width, channels), where each sample is a 2D grid of pixels, and each pixel is represented by a vector of values (“channels”) \n",
    "\n",
    "Video—Rank-5 tensors of shape (samples, frames, height, width, channels), where each sample is a sequence (of length frames) of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As data storage is often slow, we want to parallelize data loading. However, Python does not provide easy, efficient, parallel processing, we will need multiple processes to load our data. In order to assemble them into batches (tensors that encompass several samples). PyTorch provides all that magic in the **DataLoader** class (Chapt 7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "![](./images/pytorch_pipeline.png) \n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training (several steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> At each step in the training loop, we evaluate our model on the samples we got from the data loader. We then compare the outputs of our model to the desired output (the targets) using some criterion or loss function. PyTorch also has a variety of loss functions at our disposal (Torch.nn). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "![](./images/deep_learning_training.png)\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After we have compared our actual outputs to the ideal with the loss functions, we need to push the model a little to move its outputs to better resemble the target. This is where the PyTorch autograd engine comes in; but we also need an optimizer doing the updates, and that is what PyTorch offers us in torch.optim. (Chapt 5,6,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How you should change your weights or learning rates of your neural network to reduce the losses is defined by the **optimizers** you use !!! The right optimization algorithm can reduce training time exponentially. Optimization algorithms or strategies are responsible for reducing the losses and to provide the most accurate results possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> [Stochastic Gradient Descent, Mini-Batch Gradient Descent, Momentum, Adagrad , AdaDelta , Adam](https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Gradient Descent is the most basic but most used optimization algorithm. It’s used heavily in linear regression and classification algorithms. Backpropagation in neural networks also uses a gradient descent algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "![](./images/pytorch_pipeline.png) \n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parallel training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sometimes, we need to train on multiple GPU : **torch.nn.parallel.DistributedDataParallel** and the **torch.distributed** submodule can be employed to use the additional hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The training loop is the most time-consuming part of a deep learning project. At the end of it, we are rewarded with a model whose parameters have been **optimized on our task**. This is the **Trained Model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Deploying the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Thi may involve putting the model on a server or exporting it to load it to a cloud engine,  Or we might integrate it with a larger application, or run it on a phone. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " PyTorch defaults to an [immediate execution model (eager mode)](https://towardsdatascience.com/eager-execution-vs-graph-execution-which-is-better-38162ea4dbf6). Whenever an instruction involving PyTorch is executed by the Python interpreter, the corresponding operation is immediately carried out by the underlying C++ or CUDA implementation. As more instructions operate on tensors, more operations are executed by the backend implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Hardware and software requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First part can be use on CPU\n",
    "  \n",
    "- Second part you may need CUDA-enabled GPU with atleast 8GB of memory or better\n",
    "\n",
    "- To be clear: GPU is not mandatory if you’re willing to wait, but running on a GPU cuts training time by at least an order of magnitude (and usually it’s 40–50x faster). \n",
    "\n",
    "- Colab is a Google-owned cloud environment that runs on Google Cloud Platform. It is a free and open-source platform for creating and sharing interactive and reproducible workflows.\n",
    "  \n",
    "- Again, training reduced by using multiple GPUs on the same machine, and even further on clusters of machines equipped with multiple GPUs. \n",
    "\n",
    "- Part 2 has some nontrivial download bandwidth and disk space requirements as well. The raw data needed for the cancer-detection project in part 2 is about 60 GB to download, and when uncompressed it requires about 120 GB of space.\n",
    "\n",
    "- While it is possible to use network storage for this, there might be training speed penalties if the network access is slower than local disk. Preferably you will have space on a local SSD to store the data for fast retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0515, 0.2562, 0.3839],\n",
      "        [0.1516, 0.3848, 0.1570],\n",
      "        [0.7761, 0.2426, 0.5270],\n",
      "        [0.9191, 0.4228, 0.1838],\n",
      "        [0.0467, 0.3779, 0.7167]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "895aa688263384567493505d09875376e7685297a6922210da729f70c5caa3cf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('datascience')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
