{
  "cells": [
    {
      "cell_type": "raw",
      "id": "2ee37dbb",
      "metadata": {},
      "source": [
        "---\n",
        "title: Pytorch Tensor 101\n",
        "execute:\n",
        "  echo: true\n",
        "format:\n",
        "  html:\n",
        "    code-fold: false\n",
        "categories:   \n",
        "  - Pytorch\n",
        "  - python\n",
        "description: A No-Nonsense Guides to Pytorch Tensor\n",
        "draft: false\n",
        "date: 2022-08-01\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c79add80",
      "metadata": {},
      "source": [
        "## PyTorch\n",
        "\n",
        "\n",
        "PyTorch is a Python-based open source and scientific computing package for building neural networks. It is dynamic graph-based framework that allows you to define your neural network in a way that is easy to understand and debug. Today, PyTorch is the [most used deep learning framework](https://paperswithcode.com/trends) and mostly use by researchers and engineers.  \n",
        "\n",
        "\n",
        "![](./pytorch_most_used.png)\n",
        "\n",
        "\n",
        "PyTorch support GPU acceleration (making your code run faster) behind the scenes, better than NumPy. PyTorch also provides Autograd for automatic differentiation, which means that your code is automatically differentiated and you can use it to do backpropagation\n",
        "\n",
        "\n",
        "## Pytoch Installation\n",
        "\n",
        "Before you installed Pytorch, you need to install the following dependencies: Package Manager (e.g. pip, conda), Python, Numpy. For more information, please refer to the [Pytorch documentation](https://pytorch.org/).\n",
        "\n",
        "For me, I am using mac and conda as package manager, I therefore  run the following command\n",
        "\n",
        "\n",
        "## VERIFICATION\n",
        "\n",
        "To verify your installation works, \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2a86dcb4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'1.13.0.dev20220611'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.manual_seed(1234)\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ace42e8b",
      "metadata": {},
      "source": [
        "## What is Tensor \n",
        "\n",
        "- Assume we have 3 bedrooms, 1 carpark and 2 bathrooms. We can represent this data numerically in a form of vector [3, 1,2] to describe bedrooms, carpark and bathrooms\n",
        "\n",
        "- Tensor are the standard way of representing data in Pytorch, such as text, images, and audio. Their job is to represent data in a numerical way. \n",
        "\n",
        "![](./tensor_represent_data.png)\n",
        "\n",
        "\n",
        "![](./tensor_loop.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## is Tensor all you need? \n",
        "\n",
        "- There are many Python Data Structure for holding data including Python List and Numpy Array. List and Numpy Array operations are similar to Pytorch Tensor.\n",
        "  \n",
        "- Let us remember the basic of data structures in Python (List and Numpy Array) before we start using Pytorch Tensor\n",
        "\n",
        "\n",
        "## From Python lists to Numpy Array\n",
        "\n",
        " - Python does not have built-in support for Arrays, but Python Lists can be used instead.\n",
        "\n",
        " - Using our previous example, we can create a list of Python lists below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8fc5f7e9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3, 1, 2]\n",
            "<class 'list'>\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "a_list = [3, 1,2] #A list is the Python equivalent of an array\n",
        "\n",
        "print(a_list) # print the list\n",
        "print((type(a_list))) # print the type\n",
        "print(a_list[0]) # subset the list"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35d89773",
      "metadata": {},
      "source": [
        "> However, Python lists has the following limitations: It takes large memory size and slow.\n",
        "\n",
        "- Numpy solved the problems with List: \n",
        "\n",
        "  - Size - Numpy data structures take up less space\n",
        "\n",
        "  - Performance - they have a need for speed and are faster than lists\n",
        "\n",
        "  - Functionality - SciPy and NumPy have optimized functions such as linear algebra operations built in.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9b827ad4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 3, 4])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "a_numpy = np.array([1,3,4]) # creating a numpy array\n",
        "a_numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "57b93feb",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(a_numpy) # nd arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0ccf25c1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a_numpy[0] # we can subset similar to Python list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ca7f84cb",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3,)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a_numpy.shape # shape of the nd array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "dbb59e6d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dtype('int64')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a_numpy.dtype # dtype of the nd array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "de17db96",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a_numpy.size # size of the nd array"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9271590",
      "metadata": {},
      "source": [
        "### Performance comparison between Python lists and Numpy Arrays\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "6c25bf4b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.00019288063049316406 0.0005578994750976562\n",
            "Numpy is in this example 0.3457264957264957 faster!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\n",
        "size_of_vec = 1000\n",
        "\n",
        "def pure_python_version():\n",
        "    t1 = time.time()\n",
        "    X = range(size_of_vec)\n",
        "    Y = range(size_of_vec)\n",
        "    Z = [X[i] + Y[i] for i in range(len(X)) ]\n",
        "    return time.time() - t1\n",
        "\n",
        "def numpy_version():\n",
        "    t1 = time.time()\n",
        "    X = np.arange(size_of_vec)\n",
        "    Y = np.arange(size_of_vec)\n",
        "    Z = X + Y\n",
        "    return time.time() - t1\n",
        "\n",
        "\n",
        "t1 = pure_python_version()\n",
        "t2 = numpy_version()\n",
        "print(t1, t2)\n",
        "print(\"Numpy is in this example \" + str(t1/t2) + \" faster!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2c01c8c",
      "metadata": {},
      "source": [
        "## From Numpy Arrays to Torch Tensor\n",
        "\n",
        "- Tensors are like arrays, both are data structures that are used to store data. Tensor and Numpy arrays share common operations such as shape and size.\n",
        "\n",
        ">  Tensors are generalization of vectors and matrices to an arbitrary number of dimensions. \n",
        "\n",
        "![](./tensor_generalization.png)\n",
        "\n",
        "\n",
        "- Similar to how Numpy provides additional support not available in the Python list, so also Tensors provides support not available in Numpy array such as:\n",
        "\n",
        "  - GPU acceleration , which is a great advantage for deep learning,\n",
        "  \n",
        "  - distribute operations on multiple devices or machines,and\n",
        "\n",
        "  - keep track of the graph of computations that created them ( usefull for backpropagation).\n",
        " \n",
        "\n",
        "## Let us Learn Tensor\n",
        "\n",
        "Various operations are available on tensors. In the next sections, we will discuss the following operations:\n",
        "\n",
        "- Creating tensors.\n",
        "  \n",
        "- Operations with tensors.\n",
        "  \n",
        "- Indexing, slicing, and joining with tensors Computing gradients with tensors.\n",
        "  \n",
        "- Using CUDA/MPS tensors with GPUs.\n",
        "\n",
        "\n",
        "## Creating tensors\n",
        "\n",
        "- PyTorch allows us to create tensors in many different ways using the torch package. We will discuss some of these ways.\n",
        "\n",
        "### Creating Random Tensor with a specific size\n",
        " \n",
        "> `torch.tensor` is a general Tensor constructor that infer the data type automatically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "59357309",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([3, 4])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "a_random = torch.tensor((3,4)) # Create a random tensor\n",
        "print(a_random)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "9678196d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "<class 'torch.Tensor'>\n",
            "torch.LongTensor\n"
          ]
        }
      ],
      "source": [
        "print(a_random.shape) # print the shape of the random tensor\n",
        "print(a_random.size()) # print the size of the random tensor\n",
        "print(type(a_random)) # print the type of the random tensor\n",
        "print(a_random.type()) # print the type of the random tens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9577dcc1",
      "metadata": {},
      "source": [
        "> Note: .shape is an alias for .size(), and was added to closely match numpy !\n",
        "\n",
        "- Intead of allowing the `torch.tensor` to automatically determine the data type, you can explicitly specify the type of the data type by using the `torch.type` parameter \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "a72a2dce",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([3., 4.])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "a_random = torch.tensor((3,4), dtype= torch.float) # Create a random tensor\n",
        "print(a_random)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "42926488",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "<class 'torch.Tensor'>\n",
            "torch.FloatTensor\n"
          ]
        }
      ],
      "source": [
        "print(a_random.shape) # print the shape of the random tensor\n",
        "print(a_random.size()) # print the size of the random tensor\n",
        "print(type(a_random)) # print the type of the random tensor\n",
        "print(a_random.type())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26664a6c",
      "metadata": {},
      "source": [
        "- You can also change an existing tensor type by using the \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "6d45dc48",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.LongTensor\n"
          ]
        }
      ],
      "source": [
        "a_torch = torch.tensor([1, 2, 3]) \n",
        "\n",
        "print(a_torch.type()) # Tensor type"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6da3fb7",
      "metadata": {},
      "source": [
        "We can change from LongTensor t:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "96ba6a81",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.ShortTensor\n",
            "torch.FloatTensor\n"
          ]
        }
      ],
      "source": [
        "a_short =  a_torch.short() # Convert to short,  \n",
        "a_float =  a_torch.float() # Convert to float()\n",
        "\n",
        "print(a_short.type()) # Tensor type\n",
        "print(a_float.type()) # Tensor type"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "830939d1",
      "metadata": {},
      "source": [
        "> Note: A variant of `torch.tensor` constructor is `torch.FloatTensor`constructor. When use, the default tensor type is `FloatTensor`. Infact, torch.Tensor is an alias for the `torch.FloatTensor` constructor.\n",
        "\n",
        "- The following two examples are equivalent:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "55478c68",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.FloatTensor\n",
            "torch.FloatTensor\n"
          ]
        }
      ],
      "source": [
        "a_random = torch.Tensor((3,4)) # Create a random tensor\n",
        "b_random = torch.FloatTensor((3,4)) # Create a random tensor\n",
        "\n",
        "print(a_random.type())\n",
        "print(b_random.type())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdd96479",
      "metadata": {},
      "source": [
        "> I would recommend to stick to `torch.tensor`, if you would like to change the type, you can change\n",
        "\n",
        "\n",
        " Torch defines 10 tensor types with CPU and GPU variants:\n",
        "[See different Pytorch Data Types](https://pytorch.org/docs/stable/tensors.html#data-types):\n",
        "\n",
        "- The most common type (and generally the default) is torch.float32 or torch.float. This is referred to as \"32-bit floating point\".\n",
        "\n",
        "- But there's also 16-bit floating point (torch.float16 or torch.half) and 64-bit floating point (torch.float64 or torch.double).\n",
        "\n",
        "- The reason for all of these is to do with precision in computing. Precision is the amount of detail used to describe a number.\n",
        "\n",
        "- The higher the precision value (8, 16, 32), the more detail and hence data used to express a number.\n",
        "\n",
        "- This matters in deep learning and numerical computing because you're making so many operations, the more detail you have to calculate on, the more compute you have to use.\n",
        "\n",
        "> So, lower precision datatypes are generally faster to compute on but sacrifice some performance on evaluation metrics like accuracy (faster to compute but less accurate).\n",
        "\n",
        "\n",
        "### 2: Creating Tensors from Random Numbers\n",
        "\n",
        "Similar to the numpy, we can create a tensor from a random number.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "0756e85e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0461,  0.4024, -1.0115],\n",
            "        [ 0.2167, -0.6123,  0.5036]])\n"
          ]
        }
      ],
      "source": [
        "a_random_torch = torch.randn(2, 3) # uniform random distribution numbers between 0 and 1\n",
        "# a_numpy_rand = np.random.randn(2,3) #numpy random normal distribution\n",
        "\n",
        "print(a_random_torch)\n",
        "# print(a_numpy_rand)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "d202c87a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.7749, 0.8208, 0.2793],\n",
            "        [0.6817, 0.2837, 0.6567]])\n"
          ]
        }
      ],
      "source": [
        "a_random_torch = torch.rand(2, 3) # random normal distribution\n",
        "# a_numpy_rand = np.random.rand(2,3) \n",
        "\n",
        "print(a_random_torch)\n",
        "# print(a_numpy_rand)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c3bd25f",
      "metadata": {},
      "source": [
        "### 3: Creating a filled tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "b2c94999",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "torch.Size([3, 3])\n"
          ]
        }
      ],
      "source": [
        "a_same_scalar = torch.zeros(3,3)\n",
        "print(a_same_scalar)\n",
        "print(a_same_scalar.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "b5821f87",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.]])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.ones(3, 3) # torch.ones(size=(3, 3)) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef2dcd87",
      "metadata": {},
      "source": [
        "> Any PyTorch method with an underscore (_) refers to an in­place operation;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "8c68ff1f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([[5., 5., 5.],\n",
            "        [5., 5., 5.]])\n",
            "tensor([[5., 5., 5.],\n",
            "        [5., 5., 5.]])\n"
          ]
        }
      ],
      "source": [
        "a_zero = torch.zeros(2, 3)\n",
        "print(a_zero)\n",
        "print(a_zero.fill_(5)) # inplace operation\n",
        "print(a_zero)  # a_zero is now filled with 5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51d51228",
      "metadata": {},
      "source": [
        "###4: Creating and initializing a tensor from lists\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "a3f01c30",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1, 2, 3])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a_list = torch.tensor([1, 2, 3])\n",
        "a_list"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4582cc19",
      "metadata": {},
      "source": [
        "### 5: Creating and initializing a tensor from numpy arrays\n",
        "\n",
        "\n",
        "- We use `torch.from_numpy` to create a tensor from a numpy array.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "ddd87208",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.3487, 0.9072, 0.8480],\n",
              "        [0.7245, 0.6970, 0.4976]], dtype=torch.float64)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "numpy_array = np.random.rand(2, 3) \n",
        "numpy_array\n",
        "\n",
        "torch_tensor = torch.from_numpy(numpy_array) # tensor from numpy array\n",
        "torch_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "7710744e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'torch.DoubleTensor'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch_tensor.type()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a139021",
      "metadata": {},
      "source": [
        "- The datatype after creating of tensor from numpy array is DoubleTensor instead of the default FloatTensor. This corresponds with the data type of the NumPy random matrix, a `float64`,\n",
        "\n",
        "\n",
        "> You can always convert from PyTorch tensors to Numpy arrays using the numpy function torch.numpy().\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "1c13411a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.3487288 , 0.90720583, 0.84795941],\n",
              "       [0.72447844, 0.69699952, 0.49759155]])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch_tensor.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d81ac92c",
      "metadata": {},
      "source": [
        "### 6: Creating a range and tensors like\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "bb7b3a15",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Use torch.arange(), torch.range() is deprecated \n",
        "zero_to_ten = torch.arange(0, 10) \n",
        "zero_to_ten"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0452d9a",
      "metadata": {},
      "source": [
        "### Creating tensor of type with the same shape as another tensor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "7ae26464",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Can also create a tensor of zeros similar to another tensor\n",
        "ten_zeros = torch.zeros_like(input=zero_to_ten) # will have same shape\n",
        "ten_zeros"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48a99cd5",
      "metadata": {},
      "source": [
        "### Creating Named Tensors\n",
        "\n",
        "- Named Tensors allow users to give explicit names to tensor dimensions. \n",
        "\n",
        "- In most cases, operations that take dimension parameters will accept dimension names, avoiding the need to track dimensions by position.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "d470d86a",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/1h/b7ng0kgj3w78mg7n8k7q7rch0000gn/T/ipykernel_11570/697701580.py:1: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1654931446436/work/c10/core/TensorImpl.h:1489.)\n",
            "  torch.zeros(2, 3, names=('N', 'C'))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.],\n",
              "        [0., 0., 0.]], names=('N', 'C'))"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.zeros(2, 3, names=('N', 'C'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bb5bb18",
      "metadata": {},
      "source": [
        "- We can use `names` to access tensor dimensions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "3ee5340c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('N', 'C', 'H', 'W')"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "imgs = torch.randn(1, 2, 2, 3 , names=('N', 'C', 'H', 'W')) \n",
        "imgs.names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "00e083e0",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'N'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "imgs.names[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa962f78",
      "metadata": {},
      "source": [
        "## Tensor properties\n",
        "\n",
        "Tensor has many properties including the following properties: the number of dimensions, the size, the type: \n",
        "\n",
        "\n",
        "### Tensor Dimensions\n",
        "\n",
        "We can find the tensor dimensions using:`ndim`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "ec6d00c7",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Scalar\n",
        "scalar = torch.tensor(7)\n",
        "scalar\n",
        "\n",
        "scalar.ndim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "2d74b748",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MATRIX = torch.tensor([[1,2,3,4],\n",
        "                       [5,6,7,8]])\n",
        "\n",
        "MATRIX.ndim"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63e17d4a",
      "metadata": {},
      "source": [
        "> You can tell the number of dimensions a tensor in PyTorch has by the number of square brackets on the outside ([) and you only need to count one side of the brackets.\n",
        "\n",
        "In practice, you'll often see scalars and vectors denoted as lowercase letters such as y or a. And matrices and tensors denoted as uppercase letters such as X or W\n",
        "\n",
        "## Manipulating tensors (tensor operations)\n",
        "\n",
        "- In deep learning, data (images, text, video, audio, protein structures, etc) gets represented as tensors.\n",
        "\n",
        "- A model learns by investigating those tensors and performing a series of operations (could be 1,000,000s+) on tensors to create a representation of the patterns in the input data.\n",
        "\n",
        "- After you have created your tensors, you can operate on them like you would do with traditional programming language types, like +, ­, *, /. \n",
        "\n",
        "\n",
        "###  Indexing tensors\n",
        "\n",
        "Indexing and subsetting a tensor is similar to indexing a list.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "ffeeacc3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3, 4, 5])"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "some_list = list(range(6))\n",
        "torch_list = torch.tensor(some_list)\n",
        "torch_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "b122983e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0)\n",
            "tensor(1)\n"
          ]
        }
      ],
      "source": [
        "print(torch_list[0]) # first element of the tensor\n",
        "print(torch_list[1]) # second element of the tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "89113b9e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1, 2, 3])"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch_list[1:4] # subsetting a tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dc53a55",
      "metadata": {},
      "source": [
        "### Transposing Tensors\n",
        "\n",
        "Transposing 2D tensors is a simple operation using `t`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "d9f4ecee",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[4., 1.],\n",
              "        [5., 3.],\n",
              "        [2., 1.]])"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
        "points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "27f6955f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[4., 5., 2.],\n",
              "        [1., 3., 1.]])"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "points_t = points.t()\n",
        "points_t"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de1eabb4",
      "metadata": {},
      "source": [
        "You can also transpose 3D and higher tensors using the `transpose` method by specifying the two dimensions along which transposing (flipping shape and stride) should occur:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "3a4d021d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 4, 5])\n",
            "torch.Size([5, 4, 3])\n"
          ]
        }
      ],
      "source": [
        "some_t = torch.ones(3, 4, 5)\n",
        "transpose_t = some_t.transpose(0, 2)\n",
        "print(some_t.shape)\n",
        "print(transpose_t.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37a07475",
      "metadata": {},
      "source": [
        "###  Tensor View Operation\n",
        "\n",
        "\n",
        "Tensor view operations returns a new tensor with the same data as the self tensor but of a different shape.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "f528ee04",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.4790,  0.8539],\n",
            "        [-0.2285,  0.3081]])\n",
            "torch.Size([2, 2])\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn(2, 2)\n",
        "print(x)\n",
        "print(x.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "9bd6ec01",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-0.4790,  0.8539, -0.2285,  0.3081])\n",
            "torch.Size([4])\n"
          ]
        }
      ],
      "source": [
        "y = x.view(4)\n",
        "print(y)\n",
        "print(y.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0ab9328",
      "metadata": {},
      "source": [
        ">  Using `-1` in the shape argument will automatically infer the correct size of the dimension.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "769ff5f7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.4790,  0.8539],\n",
            "        [-0.2285,  0.3081]])\n",
            "torch.Size([2, 2])\n"
          ]
        }
      ],
      "source": [
        "z = x.view(-1, 2)  # the size -1 is inferred from other dimensions\n",
        "\n",
        "print(z)\n",
        "print(z.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11bb4202",
      "metadata": {},
      "source": [
        "> View Does not change tensor layout in memory, Transpose() operation change the tensor layout in memory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2eb4ae3f",
      "metadata": {},
      "source": [
        "### Tensor Mathematical Basic Operations\n",
        "\n",
        "Tensor addition is achive using `torch.add` as shown in the following example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "bcdbccc2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([11, 12, 13])"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a tensor of values and add a number to it\n",
        "tensor = torch.tensor([1, 2, 3])\n",
        "tensor + 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "86b43ae5",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([10, 20, 30])"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Multiply it by 10\n",
        "tensor * 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "60ec40e4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-9, -8, -7])"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Subtract and reassign\n",
        "tensor = tensor - 10\n",
        "tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17ce17b3",
      "metadata": {},
      "source": [
        "> PyTorch also has a bunch of built-in functions like torch.mul() (short for multiplcation) and torch.add() to perform basic operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "2d0cb4b9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([10, 20, 30])"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Can also use torch functions\n",
        "tensor = torch.tensor([1, 2, 3])\n",
        "torch.multiply(tensor, 10)  # multiply by 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "05870f7e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([21, 22, 23])"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tensor = torch.tensor([1, 2, 3])\n",
        "\n",
        "torch.add(tensor, 20) # add by 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "0f59296f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0, 0, 0])"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.div(tensor, 20, rounding_mode='trunc') # divide by 20, with truncation as a rounding_mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "38707fc5",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0, 0, 0])"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.div(tensor, 20, rounding_mode='floor') # divide by 20, with floor as a rounding_mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "32445c20",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(6)"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.sum(tensor) # sum tensor entries  [1, 2, 3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d40505b",
      "metadata": {},
      "source": [
        "## Matrix multiplication is all you need\n",
        "\n",
        "- In deep learning algorithms (like neural networks), one of the most common operations is matrix multiplication.\n",
        "\n",
        "- PyTorch implements matrix multiplication functionality in the torch.matmul() method.\n",
        "\n",
        "\n",
        "- The main two rules for matrix multiplication to remember are:\n",
        "\n",
        "The inner dimensions must match:\n",
        "\n",
        "  - (3, 2) @ (3, 2) won't work\n",
        "  - (2, 3) @ (3, 2) will work\n",
        "  - (3, 2) @ (2, 3) will work\n",
        "\n",
        "The resulting matrix has the shape of the outer dimensions:\n",
        "\n",
        "  - (2, 3) @ (3, 2) -> (2, 2)\n",
        "  - (3, 2) @ (2, 3) -> (3, 3)\n",
        "\n",
        "Note: \"@\" in Python is the symbol for matrix multiplication.\n",
        "\n",
        "More information about matrix multiplication can be found in the [Matrix Multiplication](https://pytorch.org/docs/stable/torch.html#torch-matmul) section.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "2345216a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 4])\n",
            "torch.Size([4])\n"
          ]
        }
      ],
      "source": [
        "tensor1 = torch.randn(3, 4)\n",
        "tensor2 = torch.randn(4)\n",
        "\n",
        "print(tensor1.shape)\n",
        "print(tensor2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "8a214913",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3])"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = torch.matmul(tensor1, tensor2)\n",
        "result.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f58d00e",
      "metadata": {},
      "source": [
        "Note: The difference between element-wise multiplication (multiply) and matrix multiplication (matmul) is the addition of values.\n",
        "\n",
        "\n",
        "- matmul: matrix multiplication\n",
        "    \n",
        "- multiply: element-wise multiplication \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "ac375e50",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3])"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tensor = torch.tensor([1, 2, 3])\n",
        "tensor.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7905bedf",
      "metadata": {},
      "source": [
        "Element-wise matrix mutlication\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "618711e9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1, 4, 9])"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tensor * tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7830ba12",
      "metadata": {},
      "source": [
        " Matrix multiplication\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "056c5038",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(14)"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.matmul(tensor, tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ded8b46",
      "metadata": {},
      "source": [
        " Can also use the \"@\" symbol or `torch.mm()` for matrix multiplication, though not recommended\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "4f32958d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(14)\n",
            "tensor(14)\n"
          ]
        }
      ],
      "source": [
        "print(tensor @ tensor)\n",
        "print(tensor.matmul(tensor))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03a3ce2a",
      "metadata": {},
      "source": [
        "> A matrix multiplication like this is also referred to as the dot product of two matrices. Neural networks are full of matrix multiplications and dot products.\n",
        "\n",
        "\n",
        "For example, [`torch.nn.Linear()`](https://pytorch.org/docs/1.9.1/generated/torch.nn.Linear.html) module (we'll see this in action later on), also known as a feed-forward layer or fully connected layer, implements a matrix multiplication between an input `x` and a weights matrix `A`.\n",
        "\n",
        "\n",
        "$$\n",
        "y = x\\cdot{A^T} + b\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "805eb78e",
      "metadata": {},
      "source": [
        "Thank you for reading ! "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "datascience",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 17:00:33) \n[Clang 13.0.1 ]"
    },
    "vscode": {
      "interpreter": {
        "hash": "f6f6bd4cc3c48269a815f65a204a5f5d311d52370c5b2dcbc9202c6267080e31"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
